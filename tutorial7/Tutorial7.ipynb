{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial7",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dgromann/SemComp_WS2018/blob/master/tutorial7/Tutorial7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "NqBj76rSaQyU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Lesson 0.0.0: Store this notebook! \n",
        "\n",
        "Go to \"File\" and make sure you store this file as a local copy to either GitHub or your Google Drive. If you do not have a Google account and also do not want to create one, please check Option C below. "
      ]
    },
    {
      "metadata": {
        "id": "Ddv-6ADkjtQ-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Option A) Google Drive WITH collaboration\n",
        "\n",
        "If you want to work in a collaborative manner where each of you in the group can see each other's contributions, one of you needs to store the notebook in Google Drive and share it with the others. You share it by clicking on the SHARE button on the top right of this page and share the link with the \"everyone who receives this link can edit\" option with the other team members per e-mail, skype, or any other way you prefer.\n",
        "\n",
        "If you work with others, keep in mind to always copy the code before you edit it and always indicate your name as a comment (e.g. #Dagmar ) in the cell that it is clear who wrote which part. I also recommend creating a new code cell for your contributions."
      ]
    },
    {
      "metadata": {
        "id": "MWvHoYoAjwtm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Option B) Github without collaboration\n",
        "\n",
        "Collaborative functions are not available when storing the notebook in GitHub; you will see your own work but not that of others.\n"
      ]
    },
    {
      "metadata": {
        "id": "FK33HnCZjyot",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Option C) Download this notebook as ipynb (Jupyter notebook) or py (Python file)\n",
        "\n",
        "To run either of these on your local machine requires the installation of the required programs, which for the first tutorial are Python and NLTK. This will become more as we continue on to machine learning (requiring sklearn) and deep learning (requiring tensorflow and/or pytorch). In Google Codelab all of these are provided and do not need to be installed locally.\n"
      ]
    },
    {
      "metadata": {
        "id": "Usqe3lwcCGVm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Lesson 0.0.1: Repository of PyTorch tutorials:\n",
        "\n",
        "Online free PyTorch tutorials:\n",
        "\n",
        "* Official PyTorch tutorials: https://pytorch.org/tutorials/\n",
        "* Official PyTorch documentation: https://pytorch.org/docs/0.4.1/\n",
        "* Basic nice pytorch tutorials: https://github.com/yunjey/pytorch-tutorial\n",
        "* Sequence Modeling Toolkit in Pytorch, for e.g. Neural Machine Translation: https://github.com/pytorch/fairseq\n"
      ]
    },
    {
      "metadata": {
        "id": "1janmZiuahJe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lesson 0.1: PyTorch tutorial - Brief refresher backprop and optimizer\n",
        "\n",
        "Let's look at our first neural network in Pytorch. \n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "4A8YbV5E8iWS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CW5mJfzM9olY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We have looked at how backpropagation works in PyTorch a little bit before. For a quick refresher, please look \n",
        "at the code below and do the small exercises. "
      ]
    },
    {
      "metadata": {
        "id": "4rfvpWur8jHa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = torch.randn(2, 2) # as oppsosed to torch.tensor([1, 2, 3], requires_grad=True)\n",
        "y = torch.randn(2, 2)\n",
        "\n",
        "# By default, user created Tensors have \"requires_grad=False\"\n",
        "print(x.requires_grad, y.requires_grad)\n",
        "z = x + y\n",
        "\n",
        "# So backpropagation on z is not possible\n",
        "print(\"z does not have enough info to compute gradients: \", z.grad_fn)\n",
        "\n",
        "# \".requires_grad_( ... )\" changes an existing tensor's \"requires_grad\"\n",
        "# flag in-place. The input flag defaults to \"True\" if not given.\n",
        "x = x.requires_grad_()\n",
        "y = y.requires_grad_()\n",
        "\n",
        "# now z contains enough information to compute the gradients\n",
        "z = x + y\n",
        "print(\"z now has enough info:\", z.grad_fn)\n",
        "\n",
        "# If the input to an operation has \"requires_grad=True\", also the output has the same flag\n",
        "print(\"z allows for backprop: \", z.requires_grad)\n",
        "\n",
        "# Now z has the computation history that relates itself to x and y\n",
        "# EXERCISE: How can we detach the values of z from this history and relation to x and y? \n",
        "\n",
        "# EXERCISE: does new_z have information to backpropagate to x and y? \n",
        "\n",
        "\n",
        "# And it also should not be able to backpropagate to x and y since detach \n",
        "# \"forgets\" the comutation history and only copies the values of z to new_z.\n",
        "# Thus, it does not know how it was computed. \n",
        "\n",
        "# We can also detach a tensor temporarily from its history by using torch.no_grad():\n",
        "\n",
        "print(\"x requires_grad == True: \", x.requires_grad)\n",
        "print(\"x ** 2 requires_grad == \", (x ** 2).requires_grad)\n",
        "\n",
        "with torch.no_grad():\n",
        "  print(\"x ** 2 requires_grad == \", (x ** 2).requires_grad)\n",
        "\n",
        "print(\"x ** 2 requires_grad == \", (x ** 2).requires_grad)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ua4GMYyo__O4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lesson 1: PyTorch Tutorial - First Model\n",
        "\n",
        "Neural networks obtain much of their power by combining linear and non-linear functions in clever ways. In this lesson, we will learn these core components, make up an objective function and see how to train a model. \n",
        "\n",
        "## Base Model - Linear\n",
        "\n",
        "The base class for all neural network modules is  ```torch.nn.Module``` and all of your models should be subclasses of \n",
        "this base class, e.g. ```torch.nn.Linear```generates a linear layer. Each module has a list of parameters (e.g. size of input features and size of output features) that are subclasses of the class ```torch.nn.Parameter```. \n",
        "\n",
        "Remember a simple linear NN connects the input and the hidden layer in the following way: \n",
        "\n",
        "> $F(x) = Wx + b$\n"
      ]
    },
    {
      "metadata": {
        "id": "il1_6HJ9CtDv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Initialize a linear model of shape 5 for input features and shape 3 \n",
        "# for output features with a bias set to True per default\n",
        "linear = nn.Linear(5, 3)\n",
        "print(linear)\n",
        "\n",
        "# Randomly initialize a tensor\n",
        "data = torch.randn(2, 5)\n",
        "print(data)\n",
        "\n",
        "# EXERCISE: Can we map data under A? That is, map from a five dimensional to a 3 dimensional space as defined above?\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "41DiP_2KTISF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Non-Linearity\n",
        "\n",
        "If we want to add a non-linear activation function to the above equation, we can for instance choose ReLU: \n",
        "\n",
        "> $ F(x) = ReLU (Wx + b)$"
      ]
    },
    {
      "metadata": {
        "id": "1Ab7-mp4THsJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# F is the library \"torch.nn.functional\" imported above\n",
        "# EXERCISE: What happens here? What does ReLU do again?\n",
        "print(F.relu(data))\n",
        "\n",
        "# EXERCISE: Put \"data\" through a softmax and explain the output\n",
        "# Hint: for softmax you need to define a dimension by saying (data, dim=0)\n",
        "# dimension specifies along which dunebsion softmax will be computed\n",
        "\n",
        "# EXERCISE: What happens if you change the dimensionality to 1? \n",
        "# What does the \"dim\" indiciation mean? \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AvzPQI5_YMHA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Loss function\n",
        "\n",
        "The loss funtion is the function that your network is trained to minimize. It computes how far off your network is from the correct answer. Thus, it shows how confident your network is with its prediction. If the loss is very high, the network is confidentn in its answer and the answer is wrong. Uf the answer is correct and the network is confident in its answer, the loss will be low. With a small loss, it will hopefully generalize well unless it overfitted to the training data. \n",
        "\n",
        "In backpropagation, we take the derivative of the loss function to start computing the gradient as we go back through the network. All network components inherit from the ```nn.Module``` function and overried the ```forward()``` function. "
      ]
    },
    {
      "metadata": {
        "id": "J9b8AOcT2Ak0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class FirstModel(nn.Module):\n",
        "  \n",
        "  def __init__(self, vocab_size, num_labels):\n",
        "    super(FirstModel, self).__init__()\n",
        "    self.linear = nn.Linear(vocab_size, num_labels)\n",
        "  \n",
        "  def forward(self, vec):\n",
        "    return F.log_softmax(self.linear(vec), dim=1)\n",
        "\n",
        "def generate_input(num):\n",
        "  return torch.rand(num, 10)\n",
        "\n",
        "def generate_target():\n",
        "  return torch.LongTensor([random.randint(0,2)])\n",
        "\n",
        "model = FirstModel(10, 3) \n",
        "# EXERCISE: use torch.randn to create a random vector of the correct input size for this model\n",
        "vector = torch.randn(4, 10)\n",
        "print(\"Output: \", vector.view(1,1,-1))\n",
        "\n",
        "      \n",
        "loss_function = nn.NLLLoss() # NLLLoss() Negative Log Likelihood, aka multi-class cross-entropy \n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1) # Stochastic Gradient Descent as optimizer \n",
        "\n",
        "\n",
        "for epoch in range(100): \n",
        "    for vec in generate_input(4):\n",
        "      # Let's clear the gradients before we start training since PyTorch accumulates them\n",
        "      model.zero_grad()\n",
        "      \n",
        "      # EXERCISE: What happens to the loss currently if we change the target to one of three classes?\n",
        "      # You can use the provided function generate_target for that purpose\n",
        "      # Binary classification but we always set the label to 1 currently\n",
        "      target = torch.LongTensor([1])\n",
        "\n",
        "      # This function gets the logit probabilities \n",
        "      log_probs = model(vec.view(1,-1))\n",
        "      \n",
        "      # This function calculates the loss \n",
        "      loss = loss_function(log_probs, target)\n",
        "      \n",
        "      print(\"Loss: \", loss)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hEs5jWJ_EWOU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lesson 2: Long-Short Term Memory (LSTM)\n",
        "\n",
        "LSTMs are our first non-linear model that we are going to build. Below is a very simple example on how to start building a very simple model."
      ]
    },
    {
      "metadata": {
        "id": "JVJMdEz_-KjC",
        "colab_type": "code",
        "outputId": "c653d766-f6ea-4356-bd5c-569ed79e74d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "lstm = nn.LSTM(3, 3)\n",
        "inputs = torch.rand(1,3)\n",
        "\n",
        "hidden = (torch.randn(1,1,3), torch.randn(1,1,3))\n",
        "\n",
        "for i in inputs: \n",
        "  out, hidden = lstm(i.view(1,1,-1), hidden)  \n",
        "print(\"Toy output: \", out, \"Hidden: \", hidden)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Toy output:  tensor([[[ 0.1219,  0.4065, -0.0905]]], grad_fn=<StackBackward>) Hidden:  (tensor([[[ 0.1219,  0.4065, -0.0905]]], grad_fn=<StackBackward>), tensor([[[ 0.5348,  0.8511, -0.2229]]], grad_fn=<StackBackward>))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YsTFMOIZH5CR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's make it a bit more complicated and build a POS tagger as an LSTM. Remember, LSTMs have the following shape and calculcates the following functions for each input sequence: \n",
        "\n",
        "\n",
        "> $f_t = \\sigma(W_f x_t + U_f h_{(t-1)} + b_f)$\n",
        "\n",
        "> $i_t = \\sigma(W_i x_t + U_i h_{(t-1)} + b_i)$\n",
        "\n",
        "> $\\tilde c_t = \\tanh(W_g x_t + U_g h_{(t-1)} + b_g)$\n",
        "\n",
        "> $o_t = \\sigma(W_0 x_t + U_0 h_{(t-1)} + b_o)$\n",
        "\n",
        "> $c_t = f_t * c_{(t-1)} + i_t * \\tilde c_t$\n",
        "\n",
        "> $h_t = o_t * \\tanh(c_t)$\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "AVeautIZGqlw",
        "colab_type": "code",
        "outputId": "6885d737-22cd-4786-eb36-c6ee644df230",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# Functions to prepare the input\n",
        "def prepare_sequence(seq, to_ix):\n",
        "    idxs = [to_ix[w] for w in seq]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "\n",
        "training_data = [\n",
        "    (\"The frog ate the fly\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
        "    (\"Paul likes the book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"]),\n",
        "    (\"All type the word\".split(), [\"NN\", \"V\", \"DET\", \"NN\"]),\n",
        "    (\"The car broke\".split(), [\"DET\", \"NN\", \"V\"]),\n",
        "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])]\n",
        "word_to_ix = {}\n",
        "\n",
        "for sent, tags in training_data:\n",
        "    for word in sent:\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "\n",
        "print(word_to_ix)\n",
        "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n",
        "\n",
        "# These will usually be more like 300 or 64 dimensional.\n",
        "# We will keep them small, so we can see how the weights change as we train.\n",
        "EMBEDDING_DIM = 6\n",
        "HIDDEN_DIM = 6"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'The': 0, 'frog': 1, 'ate': 2, 'the': 3, 'fly': 4, 'Paul': 5, 'likes': 6, 'book': 7, 'All': 8, 'type': 9, 'word': 10, 'car': 11, 'broke': 12, 'Everybody': 13, 'read': 14, 'that': 15}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nU-p835GITXP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Code cell to define the model \n",
        "class LSTMTagger(nn.Module):\n",
        "    \"\"\" LSTM model to tag words with their correct part-of-speeches\"\"\"\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
        "        super(LSTMTagger, self).__init__()\n",
        "       \n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # input dimensionality (embedding_dim) and output dimensionality (hidden_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
        "\n",
        "        # EXERCISE: initialize the following linear layer to connect the hidden and the output (tag space) layer\n",
        "        self.hidden2tag =  \n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        # The variables here are (num_lyers, minibatch_size, hidden_dim)\n",
        "        h0 = torch.zeros(1, 1, self.hidden_dim)\n",
        "        c0 = torch.zeros(1, 1, self.hidden_dim)\n",
        "        return (h0, c0)\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        lstm_out, self.hidden = self.lstm(embeds.view(len(sentence), 1, -1), self.hidden)\n",
        "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        return tag_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SSxyX23JP7aE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Auxiliary functions\n",
        "\n",
        "def get_accuracy(targets, prediction): \n",
        "  (max_vals, arg_maxs) = torch.max(prediction.data, dim=1) \n",
        "  num_correct = torch.sum(targets==arg_maxs)\n",
        "  acc = (num_correct * 100.0 / len(targets))\n",
        "  return acc.item()  # percentage based\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wrl0jrbMIchE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Code cell to train the model \n",
        "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
        "\n",
        "# EXERCISE: define the loss function and an optimizer\n",
        "\n",
        "\n",
        "# Here you can output some sample scores\n",
        "with torch.no_grad():\n",
        "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
        "    tag_scores = model(inputs)\n",
        "    print(tag_scores)\n",
        "    print(F.softmax(tag_scores))\n",
        "  \n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):  # again, normally you would NOT do 300 epochs, it is toy data\n",
        "    accuracy = 0 \n",
        "    for sentence, tags in training_data:\n",
        "        \n",
        "        # EXERCISE: fill in this part of the code - the required steps are provided\n",
        "        # Step 1: clear the accumulated gradients before we start training \n",
        "\n",
        "        # Step 2: since this is an LSTM we need to initialize the hidden states\n",
        "        # and clear the history from the last instance\n",
        "\n",
        "        # Step 3: prepare the input sequence for the network (words to indices) - see available functions\n",
        "        \n",
        "        # Step 4: prepare the target sequences to be able to calculate the loss - see available functions \n",
        "               \n",
        "        # Step 5: run a forward pass \n",
        "          \n",
        "        # Step 6: compute the loss, calculate the gradient and optimize the weights\n",
        "        \n",
        "        accuracy += get_accuracy(targets, prediction)\n",
        "        \n",
        "    print(\"Epoch %s of %s, Loss %s, Accuracy %s \" % (epoch, num_epochs, loss, accuracy/len(training_data)))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}