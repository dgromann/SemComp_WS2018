{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial9_model_solutions.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dgromann/SemComp_WS2018/blob/master/Tutorial9/Tutorial9_model_solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "NqBj76rSaQyU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Lesson 0.0.0: Store this notebook! \n",
        "\n",
        "Go to \"File\" and make sure you store this file as a local copy to either GitHub or your Google Drive. If you do not have a Google account and also do not want to create one, please check Option C below. "
      ]
    },
    {
      "metadata": {
        "id": "Ddv-6ADkjtQ-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Option A) Google Drive WITH collaboration\n",
        "\n",
        "If you want to work in a collaborative manner where each of you in the group can see each other's contributions, one of you needs to store the notebook in Google Drive and share it with the others. You share it by clicking on the SHARE button on the top right of this page and share the link with the \"everyone who receives this link can edit\" option with the other team members per e-mail, skype, or any other way you prefer.\n",
        "\n",
        "If you work with others, keep in mind to always copy the code before you edit it and always indicate your name as a comment (e.g. #Dagmar ) in the cell that it is clear who wrote which part. I also recommend creating a new code cell for your contributions."
      ]
    },
    {
      "metadata": {
        "id": "MWvHoYoAjwtm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Option B) Github without collaboration\n",
        "\n",
        "Collaborative functions are not available when storing the notebook in GitHub; you will see your own work but not that of others.\n"
      ]
    },
    {
      "metadata": {
        "id": "FK33HnCZjyot",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Option C) Download this notebook as ipynb (Jupyter notebook) or py (Python file)\n",
        "\n",
        "To run either of these on your local machine requires the installation of the required programs, which for the first tutorial are Python and NLTK. This will become more as we continue on to machine learning (requiring sklearn) and deep learning (requiring tensorflow and/or pytorch). In Google Codelab all of these are provided and do not need to be installed locally.\n",
        "\n",
        "This tutorial has been adapted from [this pytorch Seq2Seq tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)."
      ]
    },
    {
      "metadata": {
        "id": "ddEJOIz1NkYF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lesson 1.0: Encoder-Decoder Sequence to Sequence Learning with Attention\n",
        "\n",
        "Before we get into ontology learning we need to understand the architecture. So today we will build an encoder decoder sequence to sequence learning model with attention that we will use next week to build ontologies automatically from text. The problem will be restricted to certain type of sequences to translate to a specific case of ontological statement, since the overall problem of ontology learning is an open problem. "
      ]
    },
    {
      "metadata": {
        "id": "PjNNMifuXWYV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercises in this lesson\n",
        "\n",
        "**Before** you get started with coding, answer the following questions: \n",
        "1.   What is the problem we want to solve here? What is our prediction?\n",
        "2.   What are the preprocessing steps? (here you can run the cells under point 1.1.) \n",
        "3.   Which type of optimizer does this code use and with which batch configuration?\n",
        "4.   Which kind of other optimization is done below? \n",
        "\n",
        "Answers: \n",
        "1. NMT where we translate from English to German \n",
        "2. We insert a blank space to the punctuation so that it is not attached to words (e.g. dog! vs. dog !) and we remove everything but .!? and actual letters including some special German characters, such as ä\n",
        "3. SGD with full batch - update after each input and output sequence\n",
        "4. Attention and dropout \n",
        "\n",
        "**These are the programming steps of today: **\n",
        "1.   Write your own Encoder\n",
        "2.   Complete the Decoder \n",
        "3.   Start a training run (see # Exercise comments below)\n",
        "4.   Evaluate the model using the existing functions \n",
        "\n",
        "**After** you have finished training and evlauating, answer the following questions:\n",
        "1.   Where do we have to place the attention layer? What does it take as input? What is the output?\n",
        "2.   What is the difference between loss and perplexity (ppl)?\n",
        "3.   How could we improve the optimization in training?\n",
        "4.   How could we improve the evaluation? \n",
        "\n",
        "\n",
        "Answers: \n",
        "1. Right at the beginning of the decoder. It takes the embeddings (dropped out) of the input sequence (English) as input.\n",
        "2. Loss is the loss function output (negative log likelihood here) which tells us far we are from the real answer for each sequence (average over batch) whereas perplexity uses the accumulated loss to calculate how confident the model is in its predictions\n",
        "3. We could turn the approach into mini-batch and use beam search. A different optimizer with a decaying learning rate could obtain better results. We could consider doing batch normalization.\n",
        "4. We should do a complete evaluation using BLEU or a similar metric. "
      ]
    },
    {
      "metadata": {
        "id": "Q_PYsx-NMblB",
        "colab_type": "code",
        "outputId": "7123fc06-f999-4121-e044-5c1e47bb4707",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!pip3 install torch\n",
        "\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IzeuNvHPQtmN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.1 Datasets\n",
        "\n",
        "In Neural Machine Translation (NMT) research it is common to use the datasets provided by the Shared Task on Machine Translation at the Workshop on Statistical Machine Translation (WMT), especially [WMT'14](http://www.statmt.org/wmt14/translation-task.html) with a more polished version provided on this [Stanford project page](https://nlp.stanford.edu/projects/nmt/). However, those datasets have 4.5 M parallele sentences, which takes too long for us to compute today. \n",
        "\n",
        "Instead, we are going to use a smaller corpus provided at the [Tatoeba project](https://www.manythings.org/anki/). The version below is targeted at German to English but feel free to replace the languages by any pair you are interested in. Translation pairs are tab delimitied: \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "I'm up.\t  Ich bin wach.\n",
        "Go ahead!\tNur zu!\n",
        "I get by.\tIch komme klar.\n",
        "```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "O0gUALFSSEWU",
        "colab_type": "code",
        "outputId": "3782b119-a10f-45e6-b4c6-5900bbab8c3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "cell_type": "code",
      "source": [
        "# Loading the dataset\n",
        "!wget https://raw.githubusercontent.com/dgromann/SemComp_WS2018/master/Tutorial9/en_de.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-02-03 13:54:49--  https://raw.githubusercontent.com/dgromann/SemComp_WS2018/master/Tutorial9/en_de.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12816559 (12M) [text/plain]\n",
            "Saving to: ‘en_de.txt’\n",
            "\n",
            "en_de.txt           100%[===================>]  12.22M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2019-02-03 13:54:49 (98.7 MB/s) - ‘en_de.txt’ saved [12816559/12816559]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dy9EP8BlU_vp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Preparing the data\n",
        "\n",
        "As usual with natural language and neural networks, we need to find out the exact vocabulary of our dataset in both languages. This vocabulary is then matched with a quantifiable index. The index in turn is used to create our one-hot encodings. In contrast, to the toy example on character-level text generation last time, we will do a proper preparing of our data. \n",
        "\n",
        "This means that by convention we mark our sequences with a \"Start of Sentence\" (**SOS**) token to indicate the start and \"End of Sentence\" (**EOS**), which clearly demarcates the end of the input and has to be the end of the prediction when we finished training. "
      ]
    },
    {
      "metadata": {
        "id": "nd0Ut99VWm1S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "SOS_token = 0 \n",
        "EOS_token = 1\n",
        "\n",
        "MAX_LENGTH = 10\n",
        "\n",
        "class WordIndexer: \n",
        "  def __init__(self):\n",
        "    self.word2index = {}\n",
        "    self.word2count = {}\n",
        "    self.index2word = {SOS_token:\"<SOS>\", EOS_token:\"<EOS>\"} \n",
        "    \n",
        "  def addSequence(self, sequence):\n",
        "    for word in sequence.split():\n",
        "      self.addWord(word)\n",
        "  \n",
        "  def addWord(self, word): \n",
        "    if word not in self.word2index:\n",
        "      self.word2index[word] = len(self.index2word)\n",
        "      self.word2count[word] = 1\n",
        "      self.index2word[len(self.index2word)] = word \n",
        "    else: \n",
        "      self.word2count[word] += 1 \n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rp8M5rq8bYp4",
        "colab_type": "code",
        "outputId": "86d37ee2-0ce5-49bb-cae0-779691150dad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "# We need to read the text file and align the translation pairs\n",
        "# that are tab delimited in the original data set \n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "def normalize(sequence):\n",
        "    sequence = sequence.lower().strip()\n",
        "    sequence = re.sub(r\"([.!?])\", r\" \\1\", sequence)\n",
        "    sequence = re.sub(r\"[^a-zA-Z.!?äüöß]+\", r\" \", sequence)\n",
        "    return sequence\n",
        "\n",
        "#We read the data, split them into lines, and then separate them into pairs\n",
        "def readData(filename):\n",
        "    lines = open(filename, encoding='utf8').read().strip().split('\\n')\n",
        "    pairs = [[normalize(s) for s in l.split('\\t')] for l in lines]      \n",
        "    return pairs\n",
        "  \n",
        "def prepareData(filename): \n",
        "  pairs = readData(filename)\n",
        "  print(\"Length of sentence pairs: \", len(pairs))\n",
        "  pairs = filterPairs(pairs)\n",
        "  print(\"Length of filtered pairs trimmed is %s with Max_legnth %s \" % (len(pairs), MAX_LENGTH))\n",
        "  input_lang = WordIndexer()\n",
        "  output_lang = WordIndexer()\n",
        " \n",
        "  for pair in pairs:\n",
        "    input_lang.addSequence(pair[0])\n",
        "    output_lang.addSequence(pair[1])\n",
        "    \n",
        "  print(\"Number of words in input language: \", len(input_lang.index2word))\n",
        "  print(\"Number of words in output language: \", len(output_lang.index2word))\n",
        "  \n",
        "  return input_lang, output_lang, pairs\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData(\"en_de.txt\")\n",
        "print(random.choice(pairs))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of sentence pairs:  176692\n",
            "Length of filtered pairs trimmed is 132991 with Max_legnth 10 \n",
            "Number of words in input language:  12434\n",
            "Number of words in output language:  24789\n",
            "['he put down his thoughts in his notebook .', 'er schrieb seine gedanken in sein notizbuch nieder .']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4YWR_v9Nq168",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.2 Building the model\n",
        "\n",
        "The model today is an encoder-decoder sequence to sequence model with attention. So basically it is two models (encoder and decoder) combined into one bigger architecture."
      ]
    },
    {
      "metadata": {
        "id": "cybhrpr7rW1F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Encoder"
      ]
    },
    {
      "metadata": {
        "id": "pXZQuJaprV1b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        " \n",
        "        # Exercise: Initialize the Embedding layer and a Gated Recurrent Unit layer\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "  \n",
        "    def forward(self, input, hidden):\n",
        "        # Exercise: Complete the forward function (embedding requires view(1, 1, -1)) \n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G7O2vRQVr7NI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Decoder with Attention"
      ]
    },
    {
      "metadata": {
        "id": "IOge7MtYzNkj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "        \n",
        "        # Exercise: Write an Embedding layer \n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "\n",
        "        # Attention and dropout layer are already provided for you \n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        \n",
        "        \n",
        "        # Exercise: Write a hidden layer as GRU and a Linear layer to the output\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)  \n",
        "        \n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)  # Unsqueeze teturns a new tensor with a dimension of size one inserted at the specified position.\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4Q95Gank6A8T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training"
      ]
    },
    {
      "metadata": {
        "id": "72DVrHMq6Cn0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = [lang.word2index[word] for word in sentence.split(' ') if len(word) > 0]\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "  \n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oto3DCSsuNpj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "training_size = math.ceil(len(pairs)*0.8)\n",
        "val_size = int((len(pairs) - training_size) / 2)\n",
        "    \n",
        "training_data, validation_data, test_data = random_split(pairs, [training_size, val_size, val_size])\n",
        "    \n",
        "def training(encoder, decoder, n_iters=10000, print_every=100, plot_every=1000, learning_rate=0.01):\n",
        "    \n",
        "    start = time.time()\n",
        "    losses = [] # Stores the actual losses\n",
        "    plot_losses = [] # Stores the average losses\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    criterion = nn.NLLLoss()\n",
        "   \n",
        "    training_pairs = [tensorsFromPair(random.choice(training_data)) for i in range(n_iters)]                                      \n",
        "    \n",
        "    for iter in range(1, n_iters + 1):\n",
        "        #as always, initialize hidden state and reset gradients \n",
        "        encoder_hidden = encoder.initHidden()\n",
        "        encoder_optimizer.zero_grad()\n",
        "        decoder_optimizer.zero_grad()\n",
        "        \n",
        "        # get training pair \n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "        \n",
        "        input_length = input_tensor.size(0)\n",
        "        target_length = target_tensor.size(0)\n",
        "    \n",
        "        loss = 0\n",
        "         \n",
        "        encoder_outputs = torch.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\n",
        "        \n",
        "        # We run through the input sequence\n",
        "        for pos in range(input_length):\n",
        "          encoder_output, encoder_hidden = encoder(input_tensor[pos], encoder_hidden)\n",
        "          encoder_outputs[pos] = encoder_output[0, 0]\n",
        "          \n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        \n",
        "        # We keep on predicting until we hit EOS - in training we know how long the output sequence should be\n",
        "        for pos in range(target_length):\n",
        "          decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "          topv, topi = decoder_output.topk(1) # Returns the k largest elements of the given input tensor along a given dimension.\n",
        "          decoder_input = topi.squeeze().detach() # Squeeze returns a tensor with all the dimensions of input of size 1 removed.\n",
        "\n",
        "          loss += criterion(decoder_output, target_tensor[pos])\n",
        "          if decoder_input.item() == EOS_token:\n",
        "            break\n",
        "        \n",
        "        loss.backward()\n",
        "\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "    \n",
        "        loss = loss.item() / target_length \n",
        "        \n",
        "        losses.append(loss)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "       \n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) loss: %s ppl: %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg, np.exp(np.mean(losses))))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "    \n",
        "    showPlot(plot_losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e9aEjRwB9ZB4",
        "colab_type": "code",
        "outputId": "21043abd-a076-42dc-964c-fd5c8fc4e91f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 712
        }
      },
      "cell_type": "code",
      "source": [
        "hidden_size = 256\n",
        "# Exercise: initialize the encoder and attn_decoder and do a training run \n",
        "# for all of these the functions are provided - you need to set the right parameters\n",
        "# Run the utility cell below before you run the training function\n",
        "encoder = EncoderRNN(len(input_lang.index2word), hidden_size).to(device)\n",
        "attn_decoder = AttnDecoderRNN(hidden_size, len(output_lang.index2word), dropout_p=0.1).to(device)\n",
        "\n",
        "training(encoder, attn_decoder, n_iters=100000, print_every=5000, learning_rate=0.01)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3m 18s (- 62m 52s) (5000 5%) loss: 4.9594050609268425 ppl: 142.5090\n",
            "6m 33s (- 59m 3s) (10000 10%) loss: 4.4776039061869595 ppl: 112.0006\n",
            "9m 49s (- 55m 37s) (15000 15%) loss: 4.138372821153699 ppl: 92.3077\n",
            "13m 5s (- 52m 21s) (20000 20%) loss: 3.919619229145044 ppl: 79.3406\n",
            "16m 22s (- 49m 7s) (25000 25%) loss: 3.749235780639265 ppl: 70.0247\n",
            "19m 39s (- 45m 51s) (30000 30%) loss: 3.6334708031070053 ppl: 63.1988\n",
            "22m 57s (- 42m 38s) (35000 35%) loss: 3.542159901810521 ppl: 57.9732\n",
            "26m 15s (- 39m 23s) (40000 40%) loss: 3.448084967584976 ppl: 53.7044\n",
            "29m 33s (- 36m 7s) (45000 45%) loss: 3.3347461333999124 ppl: 49.9694\n",
            "32m 53s (- 32m 53s) (50000 50%) loss: 3.2405164576995236 ppl: 46.7270\n",
            "36m 11s (- 29m 36s) (55000 55%) loss: 3.2128319388869113 ppl: 44.1200\n",
            "39m 28s (- 26m 19s) (60000 60%) loss: 3.1513647457050906 ppl: 41.8441\n",
            "42m 45s (- 23m 1s) (65000 65%) loss: 3.090154997171533 ppl: 39.8223\n",
            "46m 3s (- 19m 44s) (70000 70%) loss: 3.0578570258578783 ppl: 38.0794\n",
            "49m 21s (- 16m 27s) (75000 75%) loss: 3.0095548020494896 ppl: 36.5129\n",
            "52m 38s (- 13m 9s) (80000 80%) loss: 2.9769759340642388 ppl: 35.1236\n",
            "55m 57s (- 9m 52s) (85000 85%) loss: 2.9055467373961448 ppl: 33.7993\n",
            "59m 16s (- 6m 35s) (90000 90%) loss: 2.894864232176144 ppl: 32.6448\n",
            "62m 35s (- 3m 17s) (95000 95%) loss: 2.8789546399288355 ppl: 31.6189\n",
            "65m 55s (- 0m 0s) (100000 100%) loss: 2.8436275194916667 ppl: 30.6688\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x396 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAFKCAYAAAAqkecjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3XlgVfWd///nufdm3/d9g7CEfZdF\nWRTEom3HKUpQwRktjmN1tK12qP6qdqYLOrVfvtp+xWldpmrRlmaQuuBKFFlkFwJhSSBkD9n3Pff3\nRyQYs8NJbpbX4y8859xz33mX8so553M+H8Nut9sRERGRAWdxdAEiIiIjlUJYRETEQRTCIiIiDqIQ\nFhERcRCFsIiIiIMohEVERBzENtBfWFhYaer5/PzcKS2tMfWcI5H6aA710RzqoznUR3OY0cegIK9O\ntw/5K2GbzeroEoYF9dEc6qM51EdzqI/m6M8+DvkQFhERGaoUwiIiIg6iEBYREXGQHgdmffHFFzz4\n4IOMGTMGgLFjx/Kzn/2sbf/evXv57W9/i8ViIS4ujl/+8pdYLMp2ERGRnvRqdPScOXN49tlnO933\n+OOP86c//YnQ0FD+7d/+jZ07d7Jo0SJTixQRERmOrvgVpaSkJDw9PQHw9/entLT0iosSEREZCXp1\n3zgtLY17772X1atXs2vXrnb7LgbwhQsX2LVrl66CRUREesnoaT3hgoICDh48yLe+9S2ysrJYu3Yt\nH3zwAc7Ozm3HFBcXs27dOn70ox9x9dVXd/uFTU3NendNRESEXtyODgkJYcWKFQBER0cTGBhIQUEB\nUVFRAFRVVbFu3ToeeuihHgMYMH32lqAgL9Nn4RqJ1EdzqI/mUB/NoT6aw4w+XvaMWdu2bePFF18E\noLCwkOLiYkJCQtr2b9iwgTvvvJOFCxdeUYEiIiIjTY+3o6uqqnj44YepqKigsbGR+++/n+LiYry8\nvLj66quZPXs206dPbzv+pptuYtWqVV2ez8zfyhoam0nNriAu2ANvD+eePyBd0m/M5lAfzaE+mkN9\nNEd/Xgn3eDva09OTTZs2dbk/JSXl8qu6Qqezyvi/f/kSNxcrN82LZemsSJz0vFlERIaIIT2rxsQ4\nf+69eTJWi4W/Jqfz2B++YP/JC/RwcS8iIjIoDOkQNgyDG68exa//ZS7L50RRWlnP81tT2Hu8wNGl\niYiI9GhIh/BFHq5OrLp2DI+umQnAl+lFDq5IRESkZ8MihC+KDfXC082Js7kVji5FRESkR8MqhA3D\nYFS4N0XldZRXNzi6HBERkW4NqxAGGB3uDcDZ3HIHVyIiItK9YRfCo8J9AHRLWkREBr1hF8JxYd4Y\nQHqOroRFRGRwG3Yh7O5qIyzQg3P5lbS06H1hEREZvIZdCAOMCvOmvqGZ3KJqR5ciIiLSpeEZwhGt\ng7PSNThLREQGseEZwmEXR0hrcJaIiAxewzKEI4I8cHGyKoRFRGRQG5YhbLVYiAvzIreomtr6JkeX\nIyIi0qkeQ/iLL75g7ty5rFmzhjVr1vCf//mf7fbv3r2blStXsmrVKn7/+9/3W6F9FRfujR04l6er\nYRERGZx6XE8YYM6cOTz77LOd7vvFL37Biy++SEhICHfccQfLly8nPj7e1CIvx+ivJu1Iz61gQqy/\ng6sRERHp6IpuR2dlZeHj40NYWBgWi4VFixaxZ88es2q7IqMuTl+pSTtERGSQ6lUIp6Wlce+997J6\n9Wp27drVtr2wsBB//0tXmf7+/hQWFppf5WXw9XQhwNuFs3kV2O2atENERAafHm9Hx8bGcv/99/Ot\nb32LrKws1q5dywcffICzs/NlfaGfnzs2m/WyPtuVoCCvTrcnxAXw+Ze5tFithAZ4mPqdw1FXfZS+\nUR/NoT6aQ300R3/1sccQDgkJYcWKFQBER0cTGBhIQUEBUVFRBAcHU1RU1HZsQUEBwcHB3Z6vtLTm\nCktuLyjIi8LCyk73RQS4A7D/WC5zJ4aa+r3DTXd9lN5TH82hPppDfTSHGX3sKsR7vB29bds2Xnzx\nRaD19nNxcTEhISEAREZGUlVVRXZ2Nk1NTezYsYMFCxZcUaFmahuclaMR0iIiMvj0eCV87bXX8vDD\nD/Pxxx/T2NjIk08+ydtvv42XlxfLli3jySef5Mc//jEAK1asIC4urt+L7q2YUC+cbBZOZZU5uhQR\nEZEOegxhT09PNm3a1OX+2bNn8+abb5palFmcbBZGh3tzKrOMqtpGPN2cHF2SiIhIm2E5Y9bXjY3y\nxQ6cydbVsIiIDC7DPoTHRfsBcCpTISwiIoPLsA/h0eHeWC2GnguLiMigM+xD2NnJSly4N5kFlVrM\nQUREBpVhH8IA46J8sdvhTLamsBQRkcFjZIRwtC8Ap7JKHVyJiIjIJSMihOMjfLAYBqf1XFhERAaR\nERHCrs42YkK9yMirpL6h2dHliIiIACMkhKH1lnRzi520XD0XFhGRwWHkhHBU63Ph03pfWEREBokR\nE8JjIn0wQO8Li4jIoDFiQtjd1YmoEE/O5lbQ2KTnwiIi4ngjJoQBxkX50dTcwtlcLW0oIiKON7JC\nuO19Yd2SFhERx+t1CNfV1bF06VKSkpLabX/99ddZtWoVq1ev5pe//KXpBZpp7FeDs06e16QdIiLi\neL0O4eeffx4fH59226qqqnjxxRd5/fXX2bx5M+np6Rw5csT0Is3i6eZEVLAnaTl6LiwiIo7XqxBO\nT08nLS2NxYsXt9vu5OSEk5MTNTU1NDU1UVtb2yGoB5uEmNbnwmk5ei4sIiKO1asQfuqpp1i/fn2H\n7S4uLvzgBz9g6dKlLFmyhKlTpxIXF2d6kWYaH9O6vrBuSYuIiKPZejpg69atTJs2jaioqA77qqqq\neOGFF9i+fTuenp7ceeednDx5kvHjx3d5Pj8/d2w265VV/Q1BQV69PnaBlyu/SzpGWm5Fnz43Eqgf\n5lAfzaE+mkN9NEd/9bHHEE5OTiYrK4vk5GTy8/NxdnYmNDSU+fPnk56eTlRUFP7+/gDMmjWLlJSU\nbkO4tLTGvOppbUxhYWWfPhMb6sXpzFKyckpxde6xBSPC5fRROlIfzaE+mkN9NIcZfewqxHtMoI0b\nN7b9+bnnniMiIoL58+cDEBERQXp6OnV1dbi6upKSksKiRYuuqNCBkBDjx9ncCs5klzN5VICjyxER\nkRHqst4TTkpK4sMPPyQwMJC7776btWvXsnr1ahISEpg1a5bZNZpufHTrc+FUPRcWEREH6tO92Ace\neKDDtsTERBITE00raCDER/pgtRgKYRERcagRNWPWRS5OVkZH+JCZX0l1XaOjyxERkRFqRIYwtD4X\ntqOlDUVExHFGdAiDnguLiIjjjNgQjgvzxtlmITVTISwiIo4xYkPYyWZhTKQPOYXVlFc3OLocEREZ\ngUZsCMOlKSxP6WpYREQcYESHcEJM60xfKedKHFyJiIiMRCM6hGNCPfH1dGbX0Tw+P5rn6HJERGSE\nGdEhbLVYeOiWqbi72nj53VR2pyiIRURk4IzoEAaIDvHi4cTpuLvaePGdVPYez3d0SSIiMkKM+BAG\niAn14seJ03BztvGHt0+wL7XA0SWJiMgIoBD+SmyoNz9OnIars40/vp1K1oUqR5ckIiLDnEL4a+LC\nvFn37Qk0Nbew6a0U6huaHV2SiIgMYwrhb5gWH8iyWVHkFdfw549OO7ocEREZxnoVwnV1dSxdupSk\npKR22/Py8li9ejUrV67k8ccf75cCHWHl4tHEhHix82gee09ooJaIiPSPXoXw888/j4+PT4ftGzZs\n4K677mLLli1YrVZyc3NNL9ARnGwW7v3uRFycrfxp+ykulNY4uiQRERmGegzh9PR00tLSWLx4cbvt\nLS0tHDx4kGuvvRaAJ554gvDw8H4p0hFC/N1Zu3wcdQ3N/PHtVEeXIyIiw1CPIfzUU0+xfv36DttL\nSkrw8PDg17/+NatXr+aZZ57plwIdad7EUMZH+5KWU051XaOjyxERkWHG1t3OrVu3Mm3aNKKiojrs\ns9vtFBQUsHbtWiIiIrjnnntITk7ucMX8TX5+7ths1isq+puCgrxMPd/XTRwdyMnMMqoaWoiN6r/v\nGQz6s48jifpoDvXRHOqjOfqrj92GcHJyMllZWSQnJ5Ofn4+zszOhoaHMnz8fPz8/wsPDiY6OBmDe\nvHmcOXOmxxAuNfn5alCQF4WFlaae8+v8PZwBSDlTSIi3S799j6P1dx9HCvXRHOqjOdRHc5jRx65C\nvNsQ3rhxY9ufn3vuOSIiIpg/f37rB202oqKiyMjIIDY2luPHj3PjjTdeUZGDUWSwJwDZhZq8Q0RE\nzNVtCHcmKSkJLy8vli1bxqOPPsr69eux2+2MHTu2bZDWcBIW4I7VYpCtGbRERMRkvQ7hBx54oMO2\nmJgYNm/ebGpBg43NaiHU353sompa7HYshuHokkREZJjQjFm9EBnsSX1DM8XldY4uRUREhhGFcC9E\nBnkA6Ja0iIiYSiHcC5FBGpwlIiLmUwj3wqUQrnZwJSIiMpwohHvB39sFNxebroRFRMRUCuFeMAyD\nyCAP8ktqaGzSGsMiImIOhXAvRQZ5YrdDbpFWVBIREXMohHtJM2eJiIjZFMK91PaakkJYRERMohDu\npYjAr66E9a6wiIiYRCHcS+6uNgK8XfWakoiImEYh3AeRQR6UVzdQUdPg6FJERGQYUAj3wcXBWTm6\nJS0iIiZQCPeBZs4SEREzKYT74OII6SyNkBYRERP0KoTr6upYunQpSUlJne5/5plnWLNmjamFDUYh\n/u7YrAY5CmERETFBr0L4+eefx8fHp9N9aWlp7N+/39SiBiub1UJYgAc5hdU0Nbc4uhwRERniegzh\n9PR00tLSWLx4caf7N2zYwA9/+EOz6xq0EmL8aGhq4d295x1dioiIDHG2ng546qmn+NnPfsbWrVs7\n7EtKSmLOnDlERET0+gv9/Nyx2ax9q7IHQUFepp6vO//83ckcOFXI27szWDY3luhQ7wH77v42kH0c\nztRHc6iP5lAfzdFffew2hLdu3cq0adOIiorqsK+srIykpCRefvllCgoKev2FpaXmLoAQFORFYWGl\nqefsye3LxvDc347x29cP8tM7ZmKxGAP6/f3BEX0cjtRHc6iP5lAfzWFGH7sK8W5DODk5maysLJKT\nk8nPz8fZ2ZnQ0FDmz5/P3r17KSkp4fbbb6ehoYHMzEx+9atf8eijj15RoUPB9DFBzEkIZl/qBT4+\nmM2y2R1/SREREelJtyG8cePGtj8/99xzREREMH/+fABuuOEGbrjhBgCys7P56U9/OiIC+KLblo7l\nREYpf/ssnWljAgnydXN0SSIiMsT0+Ez4m5KSkvDy8mLZsmX9Uc+Q4e3hzOqlY/jD30/w338/zuzx\nIVgtBlargbe7M9PHBGIYQ/82tYiI9J9eh/ADDzzQ5b7IyEheffVVUwoaSuZOCGHfiQK+TC8mPaei\n3b6HE6cxIdbfQZWJiMhQ0OcrYbnEMAx+8I+TST1fSkNjC80tLWQXVvH27vOczipTCIuISLcUwlfI\nZrUweVRA239PiPXn7d3nScspd2BVIiIyFGjuaJN5ujkR6u/O2dwKWlrsji5HREQGMYVwP4iP8KGu\noZmcIq22JCIiXVMI94PREa2zaKXrlrSIiHRDIdwPRke0LnahEBYRke4ohPtBeKAHbi5WDc4SEZFu\nKYT7gcUwGBXuQ0FpLZU1DY4uR0REBimFcD+Jb7slXdHDkSIiMlIphPtJ2+CsXN2SFhGRzimE+8mo\nMB8MIC1bISwiIp1TCPcTd1cb4UEenMuvoKm5xdHliIjIIKQQ7kfxET40NLbOJy0iIvJNCuF+NDpc\ng7NERKRrvQrhuro6li5dSlJSUrvte/fu5dZbbyUxMZGf/vSntLTotuvXaeYsERHpTq9C+Pnnn8fH\nx6fD9scff5xnn32WN954g+rqanbu3Gl6gUNZqL87Hq42TdohIiKd6jGE09PTSUtLY/HixR32JSUl\nERoaCoC/vz+lpaWmFziUGYbB6AgfisrrKKuqd3Q5IiIyyPS4nvBTTz3Fz372M7Zu3dphn6enJwAX\nLlxg165dPPjggz1+oZ+fOzab9TJK7VpQkJep5zPT1LHBHE0v5s8fp3H3dyYSGTx4ax3MfRxK1Edz\nqI/mUB/N0V997DaEt27dyrRp04iKiurymOLiYu69916eeOIJ/Pz8evzC0tKavlfZjaAgLwoLK009\np5lmjQlg91FvDqQWcOjkBRZOC+e7C2Lx8XRxdGntDPY+DhXqoznUR3Ooj+Ywo49dhXi3IZycnExW\nVhbJycnk5+fj7OxMaGgo8+fPB6Cqqop169bx0EMPcfXVV19RgcOVl7szj94xk0Oni9jyaTrJh3PY\nk5LPv62cQkJMz7+0iIjI8NVtCG/cuLHtz8899xwRERFtAQywYcMG7rzzThYuXNh/FQ4DhmEwc1wQ\nU+MD2PllLq99eJo/f3ian981B4vFcHR5IiLiID0+E/6mpKQkvLy8uPrqq9m6dSvnz59ny5YtANx0\n002sWrXK9CKHC5vVwpIZkZzLr+Tzo3nsTsnn6ilhji5LREQcpNch/MADD3TYlpKSYmoxI8U/XB3H\n3uMFbP38LFdNCMbJ5IFqIiIyNGjGLAfw93Zl6cxISirq+eRQTrt9OYVV/Mcr+/noQJaDqhMRkYGi\nEHaQFfNicHOx8fbuDGrqmoDWFZc2vH6IjPxKtu48R31Ds4OrFBGR/qQQdhBPNydWzI2muq6J7fvO\n82VaEb954zC19c2Mi/Klpr6JPSfyHV2miIj0I4WwAy2dFYWPpzPv78viub8dA+CB703mnu9MxGox\n+PhgNna73cFViohIf1EIO5CLk5XvLoijsakFNxcrDydOZ2p8IH5eLswcF0ROYTWnMsscXaaIiPST\nPr+iJOZaODUci8VgbJQvof7ubduvmxnJvtQLfHwwm/Ga1ENEZFhSCDuYxWKwcGp4h+3xET5Eh3hy\n6EwhxeV1BPi4OqA6ERHpT7odPUgZhsF1MyKx2yH5SE7PHxARkSFHITyIXTUhBE83Jz49kktjU+vr\nSna7nZKKOmrrmxxcnYiIXCndjh7EnJ2sXDM1jPf2ZvLKe6eob2wmPbec8qoG4sK8+dmdsxxdooiI\nXAFdCQ9yS6ZHYDEM9hzP59DpQgC83Z04l1dBVW2jg6sTEZEroSvhQS7Qx40frppKdW0jo8N98Pd2\nYduuDN76/Bxp2eVMGxPo6BJFROQyKYSHgImx/u3+e0ykDwBnsssUwiIiQ5huRw9Bo8K9sRgGZ7LL\nHV2KiIhcgV6FcF1dHUuXLiUpKand9t27d7Ny5UpWrVrF73//+34pUDpydbYRFeJJRn5F26hpEREZ\nenoVws8//zw+Pj4dtv/iF7/gueeeY/PmzezatYu0tDTTC5TOjYn0oanZzrm8SkeXIiIil6nHEE5P\nTyctLY3Fixe3256VlYWPjw9hYWFYLBYWLVrEnj17+qtO+Yaxkb5A63NhEREZmnoM4aeeeor169d3\n2F5YWIi//6UBQ/7+/hQWFppbnXQpvm1wlp4Li4gMVd2Ojt66dSvTpk0jKirKtC/083PHZrOadj6A\noCAvU883FAQFeREa4M7Z3AoCAjyxWAxTzilXTn00h/poDvXRHP3Vx25DODk5maysLJKTk8nPz8fZ\n2ZnQ0FDmz59PcHAwRUVFbccWFBQQHBzc4xeWltZcedVfExTkRWHhyHwuOirMm90p+Rw9mU9EkOcV\nnWsk99FM6qM51EdzqI/mMKOPXYV4tyG8cePGtj8/99xzREREMH/+fAAiIyOpqqoiOzub0NBQduzY\nwW9+85srKlL6ZkykD7tT8jmTXX7FISwiIgOvz5N1JCUl4eXlxbJly3jyySf58Y9/DMCKFSuIi4sz\nvUDp2pivDc5aPD3CwdWIiEhf9TqEH3jggQ7bZs+ezZtvvmlqQdJ7oQHueLjaNDhLRGSI0oxZQ5jF\nMBgT6UtReR2llfWOLkdERPpIITzEfX0eaRERGVoUwkPcxfeF03RLWkRkyFEID3Gxod7YrBZOZ+lK\nWERkqFEID3FONgvjo33JvFDFzqO5ji5HRET6QCE8DKxZPg53FxuvfXCazAK9mC8iMlQohIeBIF83\nvn/TBBqbWvh/W1OoqWtydEkiItILCuFhYtqYQFbMjeFCaS0vv5uK3W53dEkiItKDPs+YJYPXzQvj\nSM8p5+DpQjZ/dAZfLxeKymopKq+jobGZ0ZE+jI/2Iz7CBzcX/U8vIuJo+pd4GLFaLNz73Yk8+fJ+\nPjqY3W6fAZzOLue9vZlYDIPxMb7c+91JeLo5OaZYERFRCA83Pp4uPLx6OifPlxLg7UqgjysBPq4Y\nBqTllHMqs4xjZ4s5kVHKjkPZfHuB5vsWEXEUhfAwFBHoQUSgR4ftk+ICmBQXwIq5Mfzo97v49Mtc\nbpwXa8paxCIi0ncamDUCubnYmDchhJKKeo6eLXZ0OSIiI5ZCeIRaNK116cPkwzkd9jW3tHAkrYiG\nxuaBLktEZETp8XZ0bW0t69evp7i4mPr6eu677z6WLFnStv/1119n27ZtWCwWJk2axGOPPdavBYs5\nYkK9GBXuzbH0YorKawkK8mrb97fks2zfl8mCyaHcfeMEB1YpIjK89XglvGPHDiZNmsRrr73Gxo0b\n2bBhQ9u+qqoqXnzxRV5//XU2b95Meno6R44c6deCxTyLp0VgBz778tJ0l8czSti+LxOAXcfyOZtb\n4aDqRESGvx5DeMWKFaxbtw6AvLw8QkJC2vY5OTnh5ORETU0NTU1N1NbW4uPj03/ViqnmJATj7mLj\nsy/zaGpuobKmgT++fQKrxSDx2ngA/vzRaVo08YeISL/o9ejoxMRE8vPz2bRpU9s2FxcXfvCDH7B0\n6VJcXFy48cYbiYvTKy9DhbOTlQWTw/jwQBZ7U/J4f3cG5VUNrFw8muvnRHM2r4J9qRfYk5LPgslh\nji5XRGTYMex9mN8wNTWVn/zkJ2zbtg3DMKiqqmLVqlW8+uqreHp6cuedd/LEE08wfvz4Ls/R1NSM\nzWY1pXi5clkFldz39Cd4uTtTWdPAlPhA/vNf5mOxGBSW1nLvUx/j7mrjhfXX4e6qiT1ERMzU45Vw\nSkoKAQEBhIWFkZCQQHNzMyUlJQQEBJCenk5UVBT+/v4AzJo1i5SUlG5DuLS0xrzqgaAgLwoLtXLQ\n5XK1wPhoX05mluHhamPt9WMpLq5q27/iqmi2fn6Ol7elcOuS+F6f95X3TuLuYuPWa3v/meFAfx/N\noT6aQ300hxl9/Prg16/r8ZnwgQMHeOmllwAoKiqipqYGPz8/ACIiIkhPT6eurg5oDezY2NgrKlQG\n3g1XxeDibOWuFQn4e7t+Y180Ad6ufLg/i7zi6l6dL7uwis++zOWD/VlU1zX2R8kiIsNCjyGcmJhI\nSUkJt912G/fccw+PP/44W7du5cMPPyQwMJC7776btWvXsnr1ahISEpg1a9ZA1C0mmjI6gL/+6kam\njw3qsM/Zycqqa+NpbrHzzp7zvTrfzi/zAGix2zmapslARES60uPtaFdXV5555pku9ycmJpKYmGhq\nUTLwDKPrqStnjAsi2M+N/ScvsHrpGDy6eTbc2NTCnuP5uDhZqW9s5tCZQuZNCu2PkkVEhjzNmCU9\nshgGi6aGtwZsSn63xx5JK6KqtpHF08MJ8Xfn2NlizbwlItIFhbD0yoLJYVgtBp8eyaW7AfU7v5r4\n45op4cwYE0hDYwsnMkoHqkwRkSFFISy94u3hzIyxQeQUVZOe0/ksWsXldRw/V8LoCG/CAz3anjEf\nOlM4kKWKiAwZCmHptcXTwgH49EjHRR8APj+Wh53Wq2CAUeHe+Hg4c+RMES0tmnVLROSbFMLSa+Ni\n/Aj2c2PfyQsdXj1qsdv5/GgeLk5WZo8PBlqfJU8fE0hVbSNnssscUbKIyKCmEJZe626AVmpGKcUV\ndcxJCMbN5dKg+4u3pA+fKRrQWkVEhgKFsPRJ2wCtLy8N0GpuaWlbl/iaqeHtjh8f7Yers5VDpwu7\nHdAlIjIS9XoBBxG4NEBr/8kL/O/Oc+QVV3Mio5Ta+ibCAz0YHe7d7ngnm4UpowPYl3qB7MJqooI9\nHVS5iMjgoyth6bNFXw3Qent3BgdPFeLhamPJjAge+N7kTif9mHFxlPRpjZIWEfk6XQlLnyXE+LHq\n2nisFoPJowII9nPrdsatyaMCsFoMtu/LZO/xfJqaW2hsthMe4M7DidOxWLr+rIjIcKYQlj4zDIPl\nc6J7fbybi43F0yPYezyfuoZmbFYLTU3NnMwsI+tCFTGhna8uIiIy3CmEZUDcvmwsty8b2/bfnx/N\n46V3U0nLKVcIi8iIpWfC4hBjIn0A9P6wiIxoCmFxiGA/N7zcnTiTXe7oUkREHKbH29G1tbWsX7+e\n4uJi6uvrue+++1iyZEnb/ry8PH70ox/R2NjIhAkT+I//+I9+LViGB8MwiI/w4fCZIorL6wjwcXV0\nSSIiA67HK+EdO3YwadIkXnvtNTZu3MiGDRva7d+wYQN33XUXW7ZswWq1kpub22/FyvAyJtIXgDM5\nuiUtIiNTj1fCK1asaPtzXl4eISEhbf/d0tLCwYMH+e1vfwvAE0880Q8lynB16blwOXMnhDq4GhGR\ngdfr0dGJiYnk5+ezadOmtm0lJSV4eHjw61//muPHjzNr1ix+/OMf90uhMvzEhHrhZLOQpufCIjJC\n9TqE33jjDVJTU3nkkUfYtm0bhmFgt9spKChg7dq1REREcM8995CcnMzixYu7PI+fnzs2m9WM2tsE\nBekVFzM4oo9jo/1IPVeMu6crHm5OA/79/UF/H82hPppDfTRHf/WxxxBOSUkhICCAsLAwEhISaG5u\npqSkhICAAPz8/AgPDyc6unXihnnz5nHmzJluQ7i0tMa04qG1MYWFlaaecyRyVB9jQzw5fraYfUdz\nmDQqYMC/32z6+2gO9dEc6qM5zOhjVyHe48CsAwcO8NJLLwFQVFRETU0Nfn5+ANhsNqKiosjIyADg\n+PHjxMXFXVGhMrLER1x6LiwiMtL0eCWcmJjIY489xm233UZdXR2PP/44W7duxcvLi2XLlvHoo4+y\nfv167HY7Y8eO5dprrx2IumWlW1VjAAAgAElEQVSYiP9qcFZazpWFsN1up6yqAR9PZyxdzGN96HQh\ngT6uRIfo9pyIDA49hrCrqyvPPPNMl/tjYmLYvHmzqUXJyOHh6kREoAfpueU0Nbdgs/Zu/pgWu520\n7HLOZJeRll1Oem4FVbWNXD87isTrxnQ4PvV8Kb9LOkZ0sCdP3jXH7B9DROSyaO5ocbj4SB9yiqrJ\nulBFXJh3t8e22O0cPFXI33dlkF1Y1bY9wNsVq8Xgw/1ZzBwX1PYOMkBjUwt/ev8UAJkXqiipqMPf\nW5ODiIjjKYTF4eIjfPj0SC5p2eVtIWy326msaaSusZnGxmYam1vILarm3b2Z5BZVYxgwd0IIM8cF\nMTrCB19PF9Kyy/n1awd56d2T/PyfZ+Ps1DoK/7295ykoqcHPy4XSynqOphezeHqEI39kERFAISyD\nwJior2bOyi7jupmRHDpdyHtfZHIur6LDsRbDYMHkUG6aF0uIv3u7ffGRPiydFcWHB7J4a9c5blkc\nT35JDW/vOY+vpzMPrpzCky/vVwiLyKChEBaHC/JxxcfDmRMZpfz0v/dQWFaHAUyM88fX0xlnmxUn\nmwV3FxtzJ4US7OvW5bn+ceEojqQVsv2LTGaNC2ZLcjpNzS3ctnQs0SFehAW4c+J8CQ2NzW1XyiIi\njqIQFoczDIMxUb4cOHmBhqYWFk0LZ/mcaEK/caXbGy7OVv7pWwn81+bD/PbNI1TXNTFldAAzxwUB\nMGV0AO/vy+JkZhlTRg/995JFZGhTCMugcMvi0YyL8mXW+GB8PJyv6FwJMX4snhZO8pFcnG0W7lg2\nFuOr15amjg7k/X1ZHE0v6jGEU8+XknWhimWzIts+LyJiJoWwDApBvm5cNzPStPPdsiSeyppGZo4L\nIvBrt6/jI31wc7FxNL0Yu93eZbg2Nbfwx7dPUFpZT4ifG1PjA02rTUTkot69lCkyxLi52PjBP05m\n7sT2qzPZrBYmxvlTVF5HblF1l58/dLqQ0sp6AP6yI43mlpZ+rVdERiaFsIw4U7+6DX00vbjLYz48\nkIUBTIrzJ6+4hp1H8waoOhEZSRTCMuJMHh2AAXzZRQin55aTnlPBlNEB3HVjAi5OVrbuPEdtfdPA\nFioiw55CWEYcb3dn4sK9Scsup7quscP+jw5kA7B0dhS+ni7ccFU0FdUNvL8vc6BLFZFhTiEsI9KU\n0QG02O2knC1pt720sp4DJy8QEejBhJjW1cKWz4nCx8OZ7fsy254Ti4iYQSEsI9LU0a2jnfelFtDS\nYm/b/smhbJpb7CybHdU2ctrV2cbNC0fR0NjCluQ0ispqqalrwm63d3puEZHe0itKMiJFh3gS6u/O\n4TNF/PyV/ay+bgyjwr359Egunm5OzJ0Q0u74qyeH8eH+LPYcL2DP8QIADKN14Yif3DadQJ+uZ/ES\nEemKQlhGJMMw+Mlt0/lbcjq7UvJ5evNhIoM8qapt5MZ5MR2mtLRYDB5YOYVPDmZTWdNIdV0j5VUN\nnC+o5J0957nzhvEO+klEZCjrMYRra2tZv349xcXF1NfXc99997FkyZIOxz3zzDMcOXKEV199tV8K\nFTGbr6cLd980gSUzItn88WnScyqwWgyundH5pCHBvm7t1ipuabHz2B/28vnRPL49P1bLI4pIn/UY\nwjt27GDSpEmsW7eOnJwc7rrrrg4hnJaWxv79+3Fycuq3QkX6y6hwbx69YyaHThdhsxr4ebn06nMW\ni8GKeTG8/O5J3vsik9uXje3nSkVkuOlxYNaKFStYt24dAHl5eYSEhHQ4ZsOGDfzwhz80vzqRAWIY\nBjPHBfV5esp5E0MJ8Hblsy9zKa/SyGkR6ZtePxNOTEwkPz+fTZs2tduelJTEnDlziIjo3fqsfn7u\n2GzmLiEXFORl6vlGKvXx8qxaNpb/97ejfJZSQHxcoPpoEvXRHOqjOfqrj70O4TfeeIPU1FQeeeQR\ntm3bhmEYlJWVkZSUxMsvv0xBQUGvzlNaWnPZxXYmKMiLwsJKU885EqmPl29qnD9+Xi68u+sc31sS\nT0Ntg6NLGvL099Ec6qM5zOhjVyHe4+3olJQU8vJa581NSEigubmZkpLWCQ727t1LSUkJt99+O/ff\nfz/Hjx/nV7/61RUVKjLUONks3HBVNPWNzWzbedbR5YjIENJjCB84cICXXnoJgKKiImpqavDza51J\n6IYbbuDdd9/lL3/5C7/73e+YOHEijz76aP9WLDIILZwajre7E29/fpbj50qob2x2dEkiMgT0eDs6\nMTGRxx57jNtuu426ujoef/xxtm7dipeXF8uWLRuIGkUGPRcnKyvmxfLGx2d45s0j2KwGo8K8GRvt\nR6CPK15uTni6O+Ht4Uywr1uX6xiLyMhi2Ad47j2zn0/omYc51McrZ7fbyS6pY/eX2ZzMLCMzv5LO\n/s8VEeTBirkxzEkIxmrRzLGd0d9Hc6iP5ujPZ8KaMUvEJIZhMGN8MFEBrVNY1tQ1cja3gvLqBipr\nGqmsbeBCSS2HzxTxh7+f4H8/O8sNV0VzzZRwnGwKY5GRSCEs0k/cXZ2YNCqgw/bCslq278tk55d5\nvPbBadJzyln37Ym9Ouexs8VU1zYyd2Ko2eWKiAPo12+RARbk68aa68fxX/fNJzrEkz3HC8jIr+jV\nZ1957yR/ePsEReW1/VyliAwEhbCIg/h4OHPrkngAtiSn93h8aWU9pZX12O2w43BOf5cnIgNAISzi\nQBNi/ZkY58+JjFKOnyvp9tiMvEtXy58dyaVBr0GJDHkKYREHW7loNAB/TU6jpZuXFc59dcs6LsyL\n6romvjjRu1nqRGTwUgiLOFhMqBdzJ4SQWVDF/tQLXR53Lq/1FYk7bxiPxTD4+GA2A/yGoYiYTCEs\nMgj8w8JRWC0GSZ+l09Tc0mG/3W4nI6+CIF9XokO8mDE2kMwLVZzJLndAtSJiFoWwyCAQ7OvGkukR\nFJbV8emR3A77C8tqqa5rIi7MG4DrZkYC8NHB7LZj6huaefX9U/zyTweoqWvs1feWVzfwlx1pFJZp\ntLWIIyiERQaJmxbE4myz8PHXgvWii7eiY0NbQ3hslC+RQZ4cOlVISUUdGfkVPPnKfnYcziE9t6JX\no6dbWuy88FYK27/I5P9tTen0ClxE+pdCWGSQ8HZ3ZvKoAPJLasgtqm6371zepUFZ0Do719JZkbTY\n7Tz/Vgq//NNBCkpquG5mJK7OVj46mE1jU/eh+s6eDE5mluHqbOV8fiVv787ojx9LRLqhEBYZRGaM\nCwLg4OnCdtsz8iowjNZBXBfNnRCCh6uN9JwKPN2c+NGqqdy+bCyLpoVTXtXQ7ejp01llvPV5Bv7e\nLvz8rjkEeLvw9u7zbWEvIgNDISwyiEwdHYDVYnDo1KUQbmmxc76givAAD1ydL8006+xkZc3ycSyZ\nEcHP757DpLjWKTKXzYrCajF4f19mp6Onq2ob+e+/H8eOnXu+PZEgXzfuunECLXY7f/j7CS3DKDKA\nFMIig4i7qxMJsX6cL6ik6KvBUrnF1dQ3NhMb1nEVljkJIay5fhze7s5t2/y9XZmTEExOUTXHzraf\nAMRut/PKeycpqajnuwviGBvlC0BCjB/LZkWRX1LD33oxe5eImKPHEK6treXBBx/kjjvu4JZbbmHH\njh3t9u/du5dbb72VxMREfvrTn9LSosEdIldi5tjWW9KHvrolfel5sHevz7F8TjQA278437atqbmF\n1z88zaHThYyL8uWm+bHtPvO9RaMIC3Dno4PZpJ4vvZIfQUR6qccQ3rFjB5MmTeK1115j48aNbNiw\nod3+xx9/nGeffZY33niD6upqdu7c2W/FiowE08cEYXDpuXDGVyOj+xLC0SFeTIz142RmGRn5FRSX\n17Hh9UN8ciiH8EAP7vnORCwWo91nnJ2sfP+mCRjAm5+c6Xb2rpq6JlLPl/LeF+d5YdvxTkd0i0jP\nelzKcMWKFW1/zsvLIyQkpN3+pKQkPD09AfD396e0VL9Bi1wJbw9nxkT5ciarjPKqes7lVWC1GEQG\nefbpPMuviuZ4Rimvf3CagtJaqmobmTsxhDuXj8fF2drpZ+LCvJk7MYQ9xws4eKqQ2eOD2+2vb2jm\n/275kpOZZe227ztRwKhw7z79oiAifVhPODExkfz8fDZt2tRu+8UAvnDhArt27eLBBx/s9jx+fu7Y\nbJ3/A3C5goI6PiuTvlMfzWFGHxfNiOR0VhknssvJLqwiLsKH8DCfPp1jcaAnSZ+dIz23ApvVwn0r\np3LD3BgMw+j2c//8ncnsS73Atl0ZLJ8fh9V66YbZC0lHOZlZxpgoX6bEBxIf5Yu9BZ5+7QB//vgM\nz/zbwnbHXwn9fTSH+miO/upjr0P4jTfeIDU1lUceeYRt27a1+z9ycXEx9957L0888QR+fn7dnqe0\ntObyq+1EUJAXhYWVpp5zJFIfzWFWH8eGt15R/uWj0zQ124kK9Lis865aMpp392Zy88I4YkO9KSqq\n6vEzNuCaqeEkH87hreQzXDMlHIATGSW8vesc4YEePLxqKk5f+2V6/qRQdqfk8+b7J1k2O6rPdX6T\n/j6aQ300hxl97CrEe/yVNSUlhby8PAASEhJobm6mpOTSiMuqqirWrVvHQw89xNVXX31FRYpIqwAf\nV+LCvCivagDodGR0b4yL9uOHt05tm2mrt749PxYnm4Vtn5+jsamFmromXno3FYth8P2bEtoFMMCt\n18bj4WojaedZSirqLqtWkZGoxxA+cOAAL730EgBFRUXU1NS0u9rdsGEDd955JwsXLuy/KkVGoBlf\njZKGvg3KMoOflwvXzoiguKKeT4/k8MbHZyipqOem+TGdBrq3uzO3LImnvqGZzR+dGdBaRYayHkM4\nMTGRkpISbrvtNu655x4ef/xxtm7dyocffkhtbS1bt25ly5YtrFmzhjVr1vDmm28ORN0iw97Mca2D\nolycrIQHeAz496+YG4OLs5W/fXqWz4/lERPi1eG1pq+7ekoY8ZE+HDxdyJG0ooErVGQI6/GZsKur\nK88880yX+1NSUkwtSERahfq7MychGB8Plw6vEw0EL3dnls+OYtuuDGzW1tvQtm4GXVkMgzuXj+PJ\nl/fz+genGB/t226Gr4vSssvZlZKHi5MVV2crrs423F1t+Hu7EODtir+Xa3/+WJ2qrW+irqEZPy+X\nAf9uGdl6PTBLRAbevd+d5NDvXz4nmjPZ5Vw1IYSIXrwiFRHkyQ1XRfPOnvMkfXqW25aNbbe/vLqB\n55KOUlnT/VKLfl4uLJwazrJZUbi79u8/Uy12O8+8eYSCkhp+84MFuDiZ+/aGSHcUwiLSJTcXG4+s\nnt6nz3xnQSwHThXy8cFs5kwIIT6i9dUqu93OK++mUlnTyM3XxDF5dAC19c3UNTRRVdtIaWU9JRV1\nlFTUc76gkrc+P8dHB7JYPiea62ZG4ubS+T9XTc0tJH16lmljAtum4eyLXcfyOJvbOitZWnY5E+P8\n+3wOkculEBYRUznZrPzzt8az4fVDvPLeSZ74p9k42Sx8eiSXL9OLSYjx48b5sVi6eV/Z09uNN99P\nZfsXmSR9dpYP9mfxcOI0okM6jhI/draY7fsyOZtbzvo7Zvap1tr6JpI+Pdv23ycyShTCMqC0gIOI\nmG5slC9LZkSQW1TNO3syyC+p4Y1PzuDhauPuGxO6DWBovQK/cV4sT//rfG6aH0NVbSPv7j3f6bEX\nl2w8k1NORXVDn+p8Z895yqsb+NbcaGxWgxMZmvFPBpZCWET6xcpFo/HzcuGdPef5XdIxGhpbWLN8\nHP7evR945eZi4+ZrRhHq786h00VU17V/llxb38SRM60jse12+jQq+0JpDR/sz8Tf24XvLIgjPsKH\nzIJKKmv6FuQiV0IhLCL9ws3Fxtrl42husZNbVM28iSHMSQjp+YPfYBgG10wJo6m5pe2q96IjZ4po\naGph3sRQAA6fLuzsFJ36y450mprt3LI4HhcnKxNi/bGDVpCSAaUQFpF+MzU+kOtnRzE63Jvbl427\n7PPMmxSKxTD4/Gheu+17vwrl7yyIJSLIg+MZpdTWN/V4vtTzpRw6XUh8pA9zElrfx54Q2/osWLek\nZSBpYJaI9KvE68Zc8Tl8PV2YNMqfo+nFZF+oIjLYk4qaBo6fKyEuzIsQf3dmjAni77szOH6uhFnf\nWP3povP5lXz2ZS57T+RjALctHdM2D35sqBduLjZOZJR0+lmR/qArYREZEq6ZEgbA58dar4b3p16g\nxW7nqgmtt6IvTvN56EzHW9L7T17gyZf38fNX9rPjcA4uTlbuWD6u3RScFotBQowfReV1XCir7e8f\nRwTQlbCIDBFT4wPxdHNiz/F8Vi4ezRcnCjCg7XZydIgnAd4ufJlWTFNzS9vsXqcyS9m0NQXDMJg+\nJpBrpoYzeZQ/VkvHa5AJsX4cOl3IiYwSgqdFDOSPJyOUroRFZEiwWS3MnRhCZU0jnxzMJi2nnPEx\nfvh6tk412RqyQdTWN3EqswyAmrom/vj2CTDg32+fzgPfm8K0+MBOAxhgop4LywBTCIvIkHH15NZb\n0n9NTgdg7oT2o62nX7wl/dUo6T9/dJriinpumhfLmMieZ9MK9nMjwNuF1IwSWuz2Lo87nVXGf/7P\nAfKKqy/r5xC5SCEsIkNGdIgXMSFeNLfYsVktbStNXTQ2ygcPVxuHzxSyL7WA3Sn5xIZ68e0Fsb06\nv2EYJMT6U13XRFZBVafH2O12Nn98hnN5Fbz6/ins3YR1d06eL9Xay6IQFpGh5eqvBmhNHR3QYXEH\nq8XCtPhAyqoaePGdVJxtFtZ9e0K3qz9904TY1vXSuxolfSStiPP5lVgMg5OZZew/eaHPP8OOQ9k8\nvfkwT//5MA2NzX3+vAwfPf7NrK2t5cEHH+SOO+7glltuYceOHe327969m5UrV7Jq1Sp+//vf91uh\nIiLQekv62hkRfPeauE73Xxwl3djUwq3XxhPWx7WYE2IuPhfuGMJ2u523Pj+HAdz/vcnYrBbe/CSN\nuoae302+aF9qAa99cBoDuFBWyzt7Op+OU0aGHkN4x44dTJo0iddee42NGzeyYcOGdvt/8Ytf8Nxz\nz7F582Z27dpFWlpavxUrIuLibOWO68cR2cXSihPi/PH1dGbG2CCWTO/7CGcfD2cigzw5nV1OeVV9\nu32HzxSRWVDFnAkhTIsP5FtXRVNaWc/bu3sXpMfPlfCHv5/AxdnKT26bjp+XC+/uPa9nyyNYjyG8\nYsUK1q1bB0BeXh4hIZcGQmRlZeHj40NYWBgWi4VFixaxZ8+e/qtWRKQHLk5Wnv7X+dx386S2iTj6\nasHkUBqbWnh68+G2IG65eBVstM7QBbBiXgwB3i68vy+T/JKabs95Lq+C3yUdwzDgge9NYVy0H7ct\nHUtzi71Xz5bP5VVwNL34sn4eGbx6/Z5wYmIi+fn5bNq0qW1bYWEh/v6Xlv3y9/cnKyur2/P4+blj\ns5m7aHZQUMflzaTv1EdzqI/mcGQfb18xgfpmO1s/TeeZv3zJL/91AaczSsi6UMXiGZFMGR/aduw9\nN0/h1/+zny2fnuXJdXM7BP+F0hq278ng3V3naGxqZv2ds5k3ORyA5YGe7D9VyL4T+aRklnPtrKgO\ntTQ1t/DGB6f468enabHDo/80h3lfjRLvDf19NEd/9bHXIfzGG2+QmprKI488wrZt2y77N8zS0u5/\nW+yroCAvCgsrTT3nSKQ+mkN9NMdg6OO350ZTV9vI9n2Z/PtzOzEMMAy4flZku9riQz2ZGOvHoVMX\n+NenPiYq2JPIIE/8vVzYf/ICR9KKsNvBw9XGXTcmEB/a/mdbuSiOI2cu8Me3jhEX7IGnm1Pbvgtl\ntfxh23HScysI8HalsraB/7P5IF7Oswnxd+/xZxgMfRwOzOhjVyHeYwinpKQQEBBAWFgYCQkJNDc3\nU1JSQkBAAMHBwRQVXVo6rKCggODgzudsFREZSgzD4JYlo8GA7V9kAjB/Uiih3wg/wzBYc8N4Xn4n\nlXP5FeQUVgOXVnuKCfXi2hkRXJUQgrNTx7uAgT5ufHdBHH9NTudXrx4kxM8Nd1cnXJ2t7DmeT11D\nM3MSglm7fDxfphfxh7+f4Hf/e4z/b80sXJwvne9ERgknM0u5fnZ0uyCXwa3HED5w4AA5OTk89thj\nFBUVUVNTg59f6xD+yMhIqqqqyM7OJjQ0lB07dvCb3/ym34sWERkIhmFwy+LROFktfJFawHeu7nxE\ndrCvG/9++wxa7HYKy2rJvlDNhbIaxkb5MirMu8c7h8tmR3Eqq4xjZ4vbPVt2cbZy940JzJ8UimEY\nzJsYSnpOOZ8cyuF/3j/JupsmUFxex5ufpHHwqwlKdqfk8y/fmdiryUnE8Qx7D6MB6urqeOyxx8jL\ny6Ouro7777+fsrIyvLy8WLZsGfv3728L3uuvv56777672y80+9aIbreYQ300h/pojpHaxxa7nfqG\nZqrrGqmpa8LPywUvd+d2xzQ1t/DU64dIz61g+phAUs6V0NjUQnykD2MifNi+LxMDg5sXxrH2pkkU\nF3c+6Yj0Xn/eju4xhM2mEB6c1EdzqI/mUB+7V1JRx89f2U9lTSM+ns7cuiSeuRNCMAyDU5ml/Pff\nT1BaWU9CrD++Hs7UNTRR39iMxWKwctFookM0WKsvHPpMWEREBhd/b1d+vGoaJ8+Xcs3UcNxcLv1T\nPi7ajyf/eTYvvpPa6StNuUXVPP5Ps/H+xhW2WVrsdrIKqjieUcLxc60Tnjx0y1ScbJqgsTMKYRGR\nISg6xKvLK1ovd2ceXDmFZouVstJqXF1suDpbeW/vef535zleeOs4P1o1tcvVpC7X9i8yee+L81TW\nNLbbfjq7rG2FKmlPv5qIiAxDhmEQFuhBoK8bnm5O2KwWbpwfy/QxgaSeL2XLVytRmWXn0Vz+siMN\nux0WTArlnm9PYN1NEwA4ca7zebhFISwiMmJYDIPv3zSBUH933t+XxRcnCnr+UC+cyizlT9tP4eFq\n49E1M7n7pgnMnRjKjHFB2KwGx7tYDEMUwiIiI4qbi437/3EyLs5WXn4vlezCKxs9XVBSw++SjgFw\n382T271H7eJkJT7Ch8yCKiprGvp87vKqev66I42couE7t7ZCWERkhAkP9OD7N06gobGFV947Sctl\nviRTXdfIxi1Hqa5rYs3ycSTE+HU4ZmJc67Pg1POlfTr32dwK/uN/DvDeF5n81+bDXCirvawaBzsN\nzBIRGYFmjgti9vhg9p+8wK6jeVwzNbzb44vKa3l7dwY5hdXUNzZT99X7zLX1zdxwVTQLu/j8hFh/\n/vbpWY6fK2FOQki7fbX1TRw6XUhMiBcRQR5tk5rsOpbH/2w/RXNLCzPHBnHwdCG/feMIP10zEx+P\n/hnV7SgKYRGREWrVtfEcTS/mr8npTB8b1Ol0lzV1TbyzJ4MPD2TT1NyC1WLg6mzF2cmKr6cLCyb5\ns3Lx6C6/IybECw9XGycySrDb7e1mD/vbp+l8cigHaF1CckKsP1aLwefH8nB3sXHvdyczaVQASZ+l\n8/bu8/yfvxzh32+b0e6VrKFu+PwkIiLSJ/7ernxnQSx/TU7nf3eeZc3149r2tbTY2XE4h7c+P0dV\nbSP+3i58b+ForpoYgqUPC/hYLAYJMX4cOFVIQWlt2zPjiuoGdh7Nw8/LhXHRvpzIKGXP8Xyg9Xb5\nA9+bTIhf67E3XzOKiupGPvsyl98lHRtW7x0rhEVERrBls6P4/FgeyYdyWDglnJhQL/KKq3np3VTS\ncypwc7GycvFols6M7HQBit6YEOfPgVOFHD9X0hbCHx3MorGphRVzY7huZiR2u53swmryS2qYFOff\n7mrXMAzWLB9LZU0Dh88U8bMXv+D62VEsmByGy2XWBGC323nlvZMcOHWBSXEBTB8byJRRAbi7DtwC\nGNYnn3zyyQH7NqDmMkbIdcfDw8X0c45E6qM51EdzqI/m6E0fLRaDsAB3dqfkk32hiuraRp5/6zjF\n5XXMSQjmh7dMY9KoAKzWy7/y9HB14qMD2disBldNCKG2von/3nYCF2cr379pAjarBcMw8PFwJiLQ\no9OrXIthMGNsIFW1TZzKLONIWhHJh3Ooa2gmKtizy18QLs7M3NkiGv+78ywfHcjGMAyyLlRx8FQh\n7+/LIrOgiuljAtsmMzHj76OHh0un23UlLCIywk2I9W8bpJWeW4G3hzNrrh/HzHFBppw/yNeNYF83\nTmaW0tzSwqdHcqmpb+Lma+L6dCXrZLOydvk4vnt1HJ8czGbH4Rze3p3BvhMF/OS26fh7u7Y7vryq\nnmf/dpT6xhbuvGFcu5WlPvsyl7d3nyfY141H186koqqBw2cKOXymiIz8ChqbWnCyXf5Vdm8phEVE\nhFXXxpNbVE1sqBerrhtj+prEE+L8ST6cw5mscj7Yn4mLs5VrZ0Ze1rl8PJy5eeEoVsyL4a3Pz7H9\ni0w2vH6In9w2nUAfNwAKy2p55o0jba82bXjtEMtmR3HzwlGcySprm1zkoVun4u3ujLe7M5HBnnx7\nQefLVfYXhbCIiODv7cp/fv+qfjv/xFg/kg/n8OoHpyiramD5nCg8rvDZq4uTlVsWj8bZZmHbrgye\n/vNhfrJ6OnUNzTzzlyOUVzXw7fmxTBrlz0vvnuSD/VkcSSuioroBi8Xg31ZOaTe5iCP0KoSffvpp\nDh48SFNTE//yL//C9ddf37bv9ddfZ9u2bVgsFiZNmsRjjz3Wb8WKiMjQND7GD8OAvOIarBaD62dH\nm3JewzD4h2tGYTEMtn5+jg1/PvTVmsxNrL5uDMtmRwHw83+ezdad53h/XyZ24F//YVK729OO0mMI\n7927lzNnzvDmm29SWlrKzTff3BbCVVVVvPjii3zwwQfYbDbuuusujhw5wrRp0/q9cBERGTo8XJ2I\nDfXmXF4F8yaF4ufV+UCly/Wdq+OwWAySPjv71RzZCcyfFNa239nJyq3XxnPVhBBq6ps6nd3LEXoM\n4dmzZzNlyhQAvL29qa/iiP8AAAcsSURBVK2tpbm5GavVipOTE05OTtTU1ODu7k5tbS0+Pj79XrSI\niAw9cyeGkF9SzYq5Mf1y/pvmxxIW4IGXuxNjozq/yo0J7Xz5R0fpMYStVivu7q33zLds2cLChQux\nWltHjLm4uPCDH/yApUuX4uLiwo033khc3MA+1BYRkaFh2aworpsRicXS+8k++sqsEd0DpdcDsz76\n6CO2bNnCSy+91LatqqqKF154ge3bt+Pp6cmdd97JyZMnGT9+fJfn8fNzx2bysO+goMH1m81QpT6a\nQ300h/poDvXRHP3Vx16F8M6dO9m0aRN//OMf8fK6VEh6ejpRUVH4+7eukjFr1ixSUlK6DeHS0por\nLLm9oCAvCgsrTT3nSKQ+mkN9NIf6aA710Rxm9LGrEO9xCpTKykqefvppXnjhBXx9299jj4iIID09\nnbq6OgBSUlKIjY29okJFRERGih6vhN99911KS0t56KGH2rZdddVVjBs3jmXLlnH33Xezdu1arFYr\n06dPZ9asWf1asIiIyHBh2O2XuZrzZTL71ohut5hDfTSH+mgO9dEc6qM5HHo7WkRERPqHQlhERMRB\nFML/f3v3EtrEFocB/IuNIbRN6YNEqKhIF+0mtoouqvGFGhcVF0K7kEFciI8UFFy0sQQfCGprFKUu\nFFtBiqCSgnYhKi4CLsZCDQQriNSVprYa+0gdk2Ls38WFcOXmepsR7unA99vNTOH8+TjloyekQ0RE\npAhLmIiISBGWMBERkSIsYSIiIkVYwkRERIr8798TJiIior/wL2EiIiJFWMJERESKsISJiIgUYQkT\nEREpwhImIiJShCVMRESkyH++T3ghO3fuHOLxOGw2Gzo6OrBq1SrVI1lGV1cXXr58iWw2i0OHDsHr\n9aKtrQ0/fvyA2+3GxYsX4XA4VI9pCZlMBrt27UIgEEBjYyNzNGFgYAA9PT2w2+04evQoamtrmWOB\nDMNAe3s7pqen8f37d7S2tsLtduP06dMAgNraWpw5c0btkAvc27dvEQgEsH//fmiaho8fP+bdhwMD\nA7h9+zYWLVqElpYWNDc3m19ULGpwcFAOHjwoIiIjIyPS0tKieCLr0HVdDhw4ICIiExMTsnnzZgkG\ng/Lo0SMREbl06ZLcuXNH5YiWcvnyZdmzZ4/09/czRxMmJibE7/fLzMyMjI+PSygUYo4m9PX1STgc\nFhGRsbEx2blzp2iaJvF4XEREjh8/LtFoVOWIC5phGKJpmoRCIenr6xMRybsPDcMQv98vqVRK0um0\nNDU1yeTkpOl1LXscres6tm/fDgCoqanB9PQ0vn79qngqa1i3bh2uXr0KACgrK0M6ncbg4CC2bdsG\nANi6dSt0XVc5omW8e/cOIyMj2LJlCwAwRxN0XUdjYyNKS0vh8Xhw9uxZ5mhCRUUFpqamAACpVArl\n5eVIJBK5E0Lm+HsOhwM3b96Ex+PJ3cu3D+PxOLxeL1wuF5xOJ9asWYNYLGZ6XcuWcDKZREVFRe66\nsrISnz9/VjiRdRQVFaG4uBgAEIlEsGnTJqTT6dxxX1VVFbOcp87OTgSDwdw1cyzchw8fkMlkcPjw\nYezduxe6rjNHE5qamjA6OoodO3ZA0zS0tbWhrKws95w5/p7dbofT6fzlXr59mEwmUVlZmfuZP+0e\nS38m/HfC/75ZsGfPniESieDWrVvw+/25+8xyfh48eICGhgYsW7Ys73PmOH9TU1O4du0aRkdHsW/f\nvl+yY47z8/DhQ1RXV6O3txdv3rxBa2srXC5X7jlz/DP/lt+f5mrZEvZ4PEgmk7nrT58+we12K5zI\nWp4/f47r16+jp6cHLpcLxcXFyGQycDqdGB8f/+VIhvKLRqN4//49otEoxsbG4HA4mKMJVVVVWL16\nNex2O5YvX46SkhIUFRUxxwLFYjH4fD4AQF1dHWZnZ5HNZnPPmWPh8v0+5+uehoYG02tY9jh6w4YN\nePLkCQDg9evX8Hg8KC0tVTyVNczMzKCrqws3btxAeXk5AGD9+vW5PJ8+fYqNGzeqHNESrly5gv7+\nfty/fx/Nzc0IBALM0QSfz4cXL15gbm4Ok5OT+PbtG3M0YcWKFYjH4wCARCKBkpIS1NTUYGhoCABz\nNCPfPqyvr8erV6+QSqVgGAZisRjWrl1reg1Lv0UpHA5jaGgINpsNp06dQl1dneqRLOHevXvo7u7G\nypUrc/cuXLiAUCiE2dlZVFdX4/z581i8eLHCKa2lu7sbS5cuhc/nQ3t7O3Ms0N27dxGJRAAAR44c\ngdfrZY4FMgwDHR0d+PLlC7LZLI4dOwa3242TJ09ibm4O9fX1OHHihOoxF6zh4WF0dnYikUjAbrdj\nyZIlCIfDCAaD/9iHjx8/Rm9vL2w2GzRNw+7du02va+kSJiIisjLLHkcTERFZHUuYiIhIEZYwERGR\nIixhIiIiRVjCREREirCEiYiIFGEJExERKcISJiIiUuQnZaMZgDx9g44AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "oG7mZGRR7-vP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Utility functions\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "6pc_b5z879MR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plotting the results\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)\n",
        "    \n",
        "# Calculating the time since start    \n",
        "import time\n",
        "import math\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tKrLsA26726y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Evaluating the trained model"
      ]
    },
    {
      "metadata": {
        "id": "fPASvBJg8JG-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]\n",
        "\n",
        "def evaluateRandomly(encoder, decoder, dataset, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(dataset)\n",
        "        print(\"Input: \", pair[0])\n",
        "        print(\"Target: \", pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print(\"Prediction: \", output_sentence, \"\\n\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "863FO6hGwdjY",
        "colab_type": "code",
        "outputId": "dc6c712d-ca39-43aa-be5e-59bbad405e64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "cell_type": "code",
      "source": [
        "output_words, attentions = evaluate(encoder, attn_decoder, random.choice(pairs)[0])\n",
        "print(output_words)\n",
        "plt.matshow(attentions.numpy())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['hast', 'du', 'mit', 'fertig', '?', '<EOS>']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f106596e5c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhoAAAFSCAYAAAC0Wna6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 660x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "-wxLKVD4qrWF",
        "colab_type": "code",
        "outputId": "285c9ea8-adb5-41f7-c97b-cd25b38c690a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 712
        }
      },
      "cell_type": "code",
      "source": [
        "evaluateRandomly(encoder, attn_decoder, test_data, n=10)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:  i don t like those people .\n",
            "Target:  ich mag diese leute nicht .\n",
            "Prediction:  ich mag diese leute nicht . <EOS> \n",
            "\n",
            "Input:  tom lacks discipline .\n",
            "Target:  es fehlt tom an disziplin .\n",
            "Prediction:  tom fehlt das an . . <EOS> \n",
            "\n",
            "Input:  she watches television from four to six .\n",
            "Target:  sie sieht von vier bis sechs fern .\n",
            "Prediction:  sie hat sich zu den . . . <EOS> \n",
            "\n",
            "Input:  tom is no longer living in boston .\n",
            "Target:  tom lebt nicht mehr in boston .\n",
            "Prediction:  tom ist nicht nicht nicht boston . . <EOS> \n",
            "\n",
            "Input:  how many foreign languages do you know ?\n",
            "Target:  wie viele fremdsprachen könnt ihr ?\n",
            "Prediction:  wie viele kennst kennst du ? <EOS> \n",
            "\n",
            "Input:  tom passed out from the heat .\n",
            "Target:  tom hat wegen der hitze das bewusstsein verloren .\n",
            "Prediction:  tom hat aus von der . . <EOS> \n",
            "\n",
            "Input:  i am your father .\n",
            "Target:  ich bin dein vater .\n",
            "Prediction:  ich bin vater vater . . <EOS> \n",
            "\n",
            "Input:  you have to cope with those difficult problems .\n",
            "Target:  du musst diese schwierigen probleme bewältigen .\n",
            "Prediction:  du musst mit mit mit . . <EOS> \n",
            "\n",
            "Input:  i ll sue you .\n",
            "Target:  ich werde dich verklagen .\n",
            "Prediction:  ich werde dir dir . . <EOS> \n",
            "\n",
            "Input:  it looks like a ufo .\n",
            "Target:  es sieht wie ein ufo aus .\n",
            "Prediction:  das sieht ein gut . . . <EOS> \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}