{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial9.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dgromann/SemComp_WS2018/blob/master/Tutorial9/Tutorial9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "NqBj76rSaQyU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Lesson 0.0.0: Store this notebook! \n",
        "\n",
        "Go to \"File\" and make sure you store this file as a local copy to either GitHub or your Google Drive. If you do not have a Google account and also do not want to create one, please check Option C below. "
      ]
    },
    {
      "metadata": {
        "id": "Ddv-6ADkjtQ-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Option A) Google Drive WITH collaboration\n",
        "\n",
        "If you want to work in a collaborative manner where each of you in the group can see each other's contributions, one of you needs to store the notebook in Google Drive and share it with the others. You share it by clicking on the SHARE button on the top right of this page and share the link with the \"everyone who receives this link can edit\" option with the other team members per e-mail, skype, or any other way you prefer.\n",
        "\n",
        "If you work with others, keep in mind to always copy the code before you edit it and always indicate your name as a comment (e.g. #Dagmar ) in the cell that it is clear who wrote which part. I also recommend creating a new code cell for your contributions."
      ]
    },
    {
      "metadata": {
        "id": "MWvHoYoAjwtm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Option B) Github without collaboration\n",
        "\n",
        "Collaborative functions are not available when storing the notebook in GitHub; you will see your own work but not that of others.\n"
      ]
    },
    {
      "metadata": {
        "id": "FK33HnCZjyot",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Option C) Download this notebook as ipynb (Jupyter notebook) or py (Python file)\n",
        "\n",
        "To run either of these on your local machine requires the installation of the required programs, which for the first tutorial are Python and NLTK. This will become more as we continue on to machine learning (requiring sklearn) and deep learning (requiring tensorflow and/or pytorch). In Google Codelab all of these are provided and do not need to be installed locally.\n",
        "\n",
        "This tutorial has been adapted from [this pytorch Seq2Seq tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)."
      ]
    },
    {
      "metadata": {
        "id": "ddEJOIz1NkYF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lesson 1.0: Encoder-Decoder Sequence to Sequence Learning with Attention\n",
        "\n",
        "Before we get into ontology learning we need to understand the architecture. So today we will build an encoder decoder sequence to sequence learning model with attention that we will use next week to build ontologies automatically from text. The problem will be restricted to certain type of sequences to translate to a specific case of ontological statement, since the overall problem of ontology learning is an open problem. "
      ]
    },
    {
      "metadata": {
        "id": "PjNNMifuXWYV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercises in this lesson\n",
        "\n",
        "**Before** you get started with coding, answer the following questions: \n",
        "1.   What is the problem we want to solve here? What is our prediction?\n",
        "2.   What are the preprocessing steps? (here you can run the cells under point 1.1.)\n",
        "3.   Which type of optimizer does this code use and with which batch configuration?\n",
        "4.   Which kind of other optimization is done below? \n",
        "\n",
        "\n",
        "**These are the programming steps of today: **\n",
        "1.   Write your own Encoder\n",
        "2.   Complete the Decoder \n",
        "3.   Start a training run (see # Exercise comments below)\n",
        "4.   Evaluate the model using the existing functions \n",
        "\n",
        "**After** you have finished training and evlauating, answer the following questions:\n",
        "1.   Where do we have to place the attention layer? What does it take as input? What is the output?\n",
        "2.   What is the difference between loss and perplexity (ppl)?\n",
        "3.   How could we improve the optimization in training?\n",
        "4.   How could we improve the evaluation? \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Q_PYsx-NMblB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#from __future__ import unicode_literals, print_function, division\n",
        "!pip3 install torch\n",
        "\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IzeuNvHPQtmN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.1 Datasets\n",
        "\n",
        "In Neural Machine Translation (NMT) research it is common to use the datasets provided by the Shared Task on Machine Translation at the Workshop on Statistical Machine Translation (WMT), especially [WMT'14](http://www.statmt.org/wmt14/translation-task.html) with a more polished version provided on this [Stanford project page](https://nlp.stanford.edu/projects/nmt/). However, those datasets have 4.5 M parallele sentences, which takes too long for us to compute today. \n",
        "\n",
        "Instead, we are going to use a smaller corpus provided at the [Tatoeba project](https://www.manythings.org/anki/). The version below is targeted at German to English but feel free to replace the languages by any pair you are interested in. Translation pairs are tab delimitied: \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "I'm up.\t  Ich bin wach.\n",
        "Go ahead!\tNur zu!\n",
        "I get by.\tIch komme klar.\n",
        "```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "O0gUALFSSEWU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Loading the dataset\n",
        "!wget https://raw.githubusercontent.com/dgromann/SemComp_WS2018/master/Tutorial9/en_de.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dy9EP8BlU_vp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Preparing the data\n",
        "\n",
        "As usual with natural language and neural networks, we need to find out the exact vocabulary of our dataset in both languages. This vocabulary is then matched with a quantifiable index. The index in turn is used to create our one-hot encodings. In contrast, to the toy example on character-level text generation last time, we will do a proper preparing of our data. \n",
        "\n",
        "This means that by convention we mark our sequences with a \"Start of Sentence\" (**SOS**) token to indicate the start and \"End of Sentence\" (**EOS**), which clearly demarcates the end of the input and has to be the end of the prediction when we finished training. "
      ]
    },
    {
      "metadata": {
        "id": "nd0Ut99VWm1S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "SOS_token = 0 \n",
        "EOS_token = 1\n",
        "\n",
        "class WordIndexer: \n",
        "  def __init__(self):\n",
        "    self.word2index = {}\n",
        "    self.word2count = {}\n",
        "    self.index2word = {SOS_token:\"<SOS>\", EOS_token:\"<EOS>\"} \n",
        "    \n",
        "  def addSequence(self, sequence):\n",
        "    for word in sequence.split():\n",
        "      self.addWord(word)\n",
        "  \n",
        "  def addWord(self, word): \n",
        "    if word not in self.word2index:\n",
        "      self.word2index[word] = len(self.index2word)\n",
        "      self.word2count[word] = 1\n",
        "      self.index2word[len(self.index2word)] = word \n",
        "    else: \n",
        "      self.word2count[word] += 1 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NAr1s6TC6enS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 10\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rp8M5rq8bYp4",
        "colab_type": "code",
        "outputId": "20f2674a-6a56-4e11-bc56-2705fbba0654",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "# We need to read the text file and align the translation pairs\n",
        "# that are tab delimited in the original data set \n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "def normalize(sequence):\n",
        "    sequence = sequence.lower().strip()\n",
        "    sequence = re.sub(r\"([.!?])\", r\" \\1\", sequence)\n",
        "    sequence = re.sub(r\"[^a-zA-Z.!?äüöß]+\", r\" \", sequence)\n",
        "    return sequence\n",
        "\n",
        "#We read the data, split them into lines, and then separate them into pairs\n",
        "def readData(filename):\n",
        "    lines = open(filename, encoding='utf8').read().strip().split('\\n')\n",
        "    pairs = [[normalize(s) for s in l.split('\\t')] for l in lines]      \n",
        "    return pairs\n",
        "  \n",
        "def prepareData(filename): \n",
        "  pairs = readData(filename)\n",
        "  print(\"Length of sentence pairs: \", len(pairs))\n",
        "  pairs = filterPairs(pairs)\n",
        "  print(\"Length of filtered pairs trimmed is %s with Max_legnth %s \" % (len(pairs), MAX_LENGTH))\n",
        "  input_lang = WordIndexer()\n",
        "  output_lang = WordIndexer()\n",
        " \n",
        "  for pair in pairs:\n",
        "    input_lang.addSequence(pair[0])\n",
        "    output_lang.addSequence(pair[1])\n",
        "    \n",
        "  print(\"Number of words in input language: \", len(input_lang.index2word))\n",
        "  print(\"Number of words in output language: \", len(output_lang.index2word))\n",
        "  \n",
        "  return input_lang, output_lang, pairs\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData(\"en_de.txt\")\n",
        "print(random.choice(pairs))"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of sentence pairs:  176692\n",
            "Length of filtered pairs trimmed is 132991 with Max_legnth 10 \n",
            "Number of words in input language:  12434\n",
            "Number of words in output language:  24789\n",
            "['he often lets me use his typewriter .', 'er lässt mich oft seine schreibmaschine benutzen .']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4YWR_v9Nq168",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.2 Building the model\n",
        "\n",
        "The model today is an encoder-decoder sequence to sequence model with attention. So basically it is two models (encoder and decoder) combined into one bigger architecture."
      ]
    },
    {
      "metadata": {
        "id": "cybhrpr7rW1F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Encoder"
      ]
    },
    {
      "metadata": {
        "id": "pXZQuJaprV1b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        " \n",
        "        # Exercise: Initialize the Embedding layer and a Gated Recurrent Unit layer\n",
        "\n",
        "  \n",
        "    def forward(self, input, hidden):\n",
        "        # Exercise: Complete the forward function (embedding requires view(1, 1, -1)) \n",
        "        \n",
        "        \n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G7O2vRQVr7NI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Decoder with Attention"
      ]
    },
    {
      "metadata": {
        "id": "IOge7MtYzNkj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "        \n",
        "        # Exercise: Write an Embedding layer \n",
        "        \n",
        "        # Attention and dropout layer are already provided for you \n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        \n",
        "        \n",
        "        # Exercise: Write a hidden layer as GRU and a Linear layer to the output\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)  # Unsqueeze teturns a new tensor with a dimension of size one inserted at the specified position.\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4Q95Gank6A8T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training"
      ]
    },
    {
      "metadata": {
        "id": "72DVrHMq6Cn0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = [lang.word2index[word] for word in sentence.split(' ') if len(word) > 0]\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "  \n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oto3DCSsuNpj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def training(encoder, decoder, n_iters=10000, print_every=100, plot_every=100, learning_rate=0.01):\n",
        "    \n",
        "    start = time.time()\n",
        "    losses = [] # Stores the actual losses\n",
        "    plot_losses = [] # Stores the average losses\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    criterion = nn.NLLLoss()\n",
        "    \n",
        "    training_size = math.ceil(len(pairs)*0.8)\n",
        "    val_size = int((len(pairs) - training_size) / 2)\n",
        "    \n",
        "    training_data, validation_data, test_data = random_split(pairs, [training_size, val_size, val_size])\n",
        "    training_pairs = [tensorsFromPair(random.choice(training_data)) for i in range(n_iters)]                                      \n",
        "    \n",
        "    for iter in range(1, n_iters + 1):\n",
        "        #as always, initialize hidden state and reset gradients \n",
        "        encoder_hidden = encoder.initHidden()\n",
        "        encoder_optimizer.zero_grad()\n",
        "        decoder_optimizer.zero_grad()\n",
        "        \n",
        "        # get training pair \n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "        \n",
        "        input_length = input_tensor.size(0)\n",
        "        target_length = target_tensor.size(0)\n",
        "    \n",
        "        loss = 0\n",
        "         \n",
        "        encoder_outputs = torch.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\n",
        "        \n",
        "        # We run through the input sequence\n",
        "        for pos in range(input_length):\n",
        "          encoder_output, encoder_hidden = encoder(input_tensor[pos], encoder_hidden)\n",
        "          encoder_outputs[pos] = encoder_output[0, 0]\n",
        "          \n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        \n",
        "        # We keep on predicting until we hit EOS - in training we know how long the output sequence should be\n",
        "        for pos in range(target_length):\n",
        "          decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "          topv, topi = decoder_output.topk(1) # Returns the k largest elements of the given input tensor along a given dimension.\n",
        "          decoder_input = topi.squeeze().detach() # Squeeze returns a tensor with all the dimensions of input of size 1 removed.\n",
        "\n",
        "          loss += criterion(decoder_output, target_tensor[pos])\n",
        "          if decoder_input.item() == EOS_token:\n",
        "            break\n",
        "        \n",
        "        loss.backward()\n",
        "\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "    \n",
        "        loss = loss.item() / target_length \n",
        "        \n",
        "        losses.append(loss)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "       \n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) loss: %s ppl: %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg, np.exp(np.mean(losses))))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "    \n",
        "    showPlot(plot_losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e9aEjRwB9ZB4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hidden_size = 256\n",
        "# Exercise: initialize the encoder and attn_decoder and do a training run \n",
        "# for all of these the functions are provided - you need to set the right parameters\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oG7mZGRR7-vP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Utility functions\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "6pc_b5z879MR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plotting the results\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)\n",
        "    \n",
        "# Calculating the time since start    \n",
        "import time\n",
        "import math\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tKrLsA26726y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Evaluating the trained model"
      ]
    },
    {
      "metadata": {
        "id": "fPASvBJg8JG-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]\n",
        "\n",
        "def evaluateRandomly(encoder, decoder, dataset, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(dataset)\n",
        "        print(\"Input: \", pair[0])\n",
        "        print(\"Target: \", pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print(\"Prediction: \", output_sentence, \"\\n\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "863FO6hGwdjY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "output_words, attentions = evaluate(encoder, attn_decoder, random.choice(pairs)[0])\n",
        "print(output_words)\n",
        "plt.matshow(attentions.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-wxLKVD4qrWF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "evaluateRandomly(encoder, attn_decoder, test_data, n=10)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}