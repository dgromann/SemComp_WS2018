{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial7_model_solutions.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dgromann/SemComp_WS2018/blob/master/Tutorial7/Tutorial7_model_solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "NqBj76rSaQyU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Lesson 0.0.0: Store this notebook! \n",
        "\n",
        "Go to \"File\" and make sure you store this file as a local copy to either GitHub or your Google Drive. If you do not have a Google account and also do not want to create one, please check Option C below. "
      ]
    },
    {
      "metadata": {
        "id": "Ddv-6ADkjtQ-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Option A) Google Drive WITH collaboration\n",
        "\n",
        "If you want to work in a collaborative manner where each of you in the group can see each other's contributions, one of you needs to store the notebook in Google Drive and share it with the others. You share it by clicking on the SHARE button on the top right of this page and share the link with the \"everyone who receives this link can edit\" option with the other team members per e-mail, skype, or any other way you prefer.\n",
        "\n",
        "If you work with others, keep in mind to always copy the code before you edit it and always indicate your name as a comment (e.g. #Dagmar ) in the cell that it is clear who wrote which part. I also recommend creating a new code cell for your contributions."
      ]
    },
    {
      "metadata": {
        "id": "MWvHoYoAjwtm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Option B) Github without collaboration\n",
        "\n",
        "Collaborative functions are not available when storing the notebook in GitHub; you will see your own work but not that of others.\n"
      ]
    },
    {
      "metadata": {
        "id": "FK33HnCZjyot",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Option C) Download this notebook as ipynb (Jupyter notebook) or py (Python file)\n",
        "\n",
        "To run either of these on your local machine requires the installation of the required programs, which for the first tutorial are Python and NLTK. This will become more as we continue on to machine learning (requiring sklearn) and deep learning (requiring tensorflow and/or pytorch). In Google Codelab all of these are provided and do not need to be installed locally.\n"
      ]
    },
    {
      "metadata": {
        "id": "Usqe3lwcCGVm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Lesson 0.0.1: Repository of PyTorch tutorials:\n",
        "\n",
        "Online free PyTorch tutorials:\n",
        "\n",
        "* Official PyTorch tutorials: https://pytorch.org/tutorials/\n",
        "* Official PyTorch documentation: https://pytorch.org/docs/0.4.1/\n",
        "* Basic nice pytorch tutorials: https://github.com/yunjey/pytorch-tutorial\n",
        "* Sequence Modeling Toolkit in Pytorch, for e.g. Neural Machine Translation: https://github.com/pytorch/fairseq\n"
      ]
    },
    {
      "metadata": {
        "id": "1janmZiuahJe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lesson 0.1: PyTorch tutorial - Brief refresher backprop and optimizer\n",
        "\n",
        "Let's look at our first neural network in Pytorch. \n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "4A8YbV5E8iWS",
        "colab_type": "code",
        "outputId": "2c9ae084-fa47-42e5-f0b1-b756ca6010a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "cell_type": "code",
      "source": [
        "!pip3 install torch\n",
        "\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/60/66415660aa46b23b5e1b72bc762e816736ce8d7260213e22365af51e8f9c/torch-1.0.0-cp36-cp36m-manylinux1_x86_64.whl (591.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 591.8MB 24kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x62460000 @  0x7f8d9460f2a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n",
            "\u001b[?25hInstalling collected packages: torch\n",
            "Successfully installed torch-1.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fbbda6f71d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "CW5mJfzM9olY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We have looked at how backpropagation works in PyTorch a little bit before. For a quick refresher, please look \n",
        "at the code below and do the small exercises. "
      ]
    },
    {
      "metadata": {
        "id": "4rfvpWur8jHa",
        "colab_type": "code",
        "outputId": "b5fdba40-a02d-4000-8713-275088a7f2c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "cell_type": "code",
      "source": [
        "x = torch.randn(2, 2) # as oppsosed to torch.tensor([1, 2, 3], requires_grad=True)\n",
        "y = torch.randn(2, 2)\n",
        "\n",
        "# By default, user created Tensors have \"requires_grad=False\"\n",
        "print(x.requires_grad, y.requires_grad)\n",
        "z = x + y\n",
        "\n",
        "# So backpropagation on z is not possible\n",
        "print(\"z does not have enough info to compute gradients: \", z.grad_fn)\n",
        "\n",
        "# \".requires_grad_( ... )\" changes an existing tensor's \"requires_grad\"\n",
        "# flag in-place. The input flag defaults to \"True\" if not given.\n",
        "x = x.requires_grad_()\n",
        "y = y.requires_grad_()\n",
        "\n",
        "# now z contains enough information to compute the gradients\n",
        "z = x + y\n",
        "print(\"z now has enough info:\", z.grad_fn)\n",
        "\n",
        "# If the input to an operation has \"requires_grad=True\", also the output has the same flag\n",
        "print(\"z allows for backprop: \", z.requires_grad)\n",
        "\n",
        "# Now z has the computation history that relates itself to x and y\n",
        "# EXERCISE: How can we detach the values of z from this history and relation to x and y? \n",
        "new_z = z.detach()\n",
        "\n",
        "# EXERCISE: does new_z have information to backpropagate to x and y? \n",
        "# NO! See print output below. \n",
        "print(\"the new_z does not allow for backprop: \", new_z.grad_fn)\n",
        "\n",
        "# And it also should not be able to backpropagate to x and y since detach \n",
        "# \"forgets\" the comutation history and only copies the values of z to new_z.\n",
        "# Thus, it does not know how it was computed. \n",
        "\n",
        "# We can also detach a tensor temporarily from its history by using torch.no_grad():\n",
        "\n",
        "print(\"x requires_grad == True: \", x.requires_grad)\n",
        "print(\"x ** 2 requires_grad == \", (x ** 2).requires_grad)\n",
        "\n",
        "with torch.no_grad():\n",
        "  print(\"x ** 2 requires_grad == \", (x ** 2).requires_grad)\n",
        "\n",
        "print(\"x ** 2 requires_grad == \", (x ** 2).requires_grad)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False False\n",
            "z does not have enough info to compute gradients:  None\n",
            "z now has enough info: <AddBackward0 object at 0x7f0631f775f8>\n",
            "z allows for backprop:  True\n",
            "the new_z does not allow for backprop:  None\n",
            "x requires_grad == True:  True\n",
            "x ** 2 requires_grad ==  True\n",
            "x ** 2 requires_grad ==  False\n",
            "x ** 2 requires_grad ==  True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ua4GMYyo__O4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lesson 1: PyTorch Tutorial - First Model\n",
        "\n",
        "Neural networks obtain much of their power by combining linear and non-linear functions in clever ways. In this lesson, we will learn these core components, make up an objective function and see how to train a model. \n",
        "\n",
        "## Base Model - Linear\n",
        "\n",
        "The base class for all neural network modules is  ```torch.nn.Module``` and all of your models should be subclasses of \n",
        "this base class, e.g. ```torch.nn.Linear```generates a linear layer. Each module has a list of parameters (e.g. size of input features and size of output features) that are subclasses of the class ```torch.nn.Parameter```. \n",
        "\n",
        "Remember a simple linear NN connects the input and the hidden layer in the following way: \n",
        "\n",
        "> $F(x) = Wx + b$\n"
      ]
    },
    {
      "metadata": {
        "id": "il1_6HJ9CtDv",
        "colab_type": "code",
        "outputId": "e48e1084-f7f9-4f88-c93f-765858b9f3e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "cell_type": "code",
      "source": [
        "# Initialize a linear model of shape 5 for input features and shape 3 \n",
        "# for output features with a bias set to True per default\n",
        "linear = nn.Linear(5, 3)\n",
        "print(linear)\n",
        "\n",
        "# Randomly initialize a tensor\n",
        "data = torch.randn(2, 5)\n",
        "print(data)\n",
        "\n",
        "# Question: Can we map data under A? That is, map from a five dimensional to a 3 dimensional space as defined above?\n",
        "print(linear(data))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear(in_features=5, out_features=3, bias=True)\n",
            "tensor([[ 0.3935,  1.1322, -0.5404, -2.2102,  2.1130],\n",
            "        [-0.0040,  1.3800, -1.3505,  0.3455,  0.5046]])\n",
            "tensor([[-0.4231,  0.5703, -0.1399],\n",
            "        [-0.1424,  0.4146,  0.6761]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "41DiP_2KTISF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Non-Linearity\n",
        "\n",
        "If we want to add a non-linear activation function to the above equation, we can for instance choose ReLU: \n",
        "\n",
        "> $ F(x) = ReLU (Wx + b)$"
      ]
    },
    {
      "metadata": {
        "id": "1Ab7-mp4THsJ",
        "colab_type": "code",
        "outputId": "6676bc0f-24dc-49ab-c9f2-0eb67a869e1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        }
      },
      "cell_type": "code",
      "source": [
        "# F is the library \"torch.nn.functional\" imported above\n",
        "# EXERCISE: What happens here? What does ReLU do again?\n",
        "print(F.relu(data))\n",
        "\n",
        "# EXERCISE: Put \"data\" through a softmax and explain the output\n",
        "# Hint: for softmax you need to define a dimension by saying (data, dim=0)\n",
        "# dimension specifies along which dunebsion softmax will be computed\n",
        "print(F.softmax(data, dim=0))\n",
        "\n",
        "# EXERCISE: What happens if you change the dimensionality to 1? \n",
        "# What does the \"dim\" indiciation mean? \n",
        "print(\"View our tensor data as a matrix with two rows and five columns\")\n",
        "print(\"Softmax sums along the columns with dimension 0 so that each column sums up to 1\")\n",
        "print(F.softmax(data, dim=0))\n",
        "print(\"Softmax sums along the rows with dimension 1 so that each row sums up to 1\") \n",
        "print(F.softmax(data, dim=1))\n",
        "\n",
        "print(\"Check its sum\", F.softmax(data, dim=1)[1].sum())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.3935, 1.1322, 0.0000, 0.0000, 2.1130],\n",
            "        [0.0000, 1.3800, 0.0000, 0.3455, 0.5046]])\n",
            "tensor([[0.5981, 0.4384, 0.6921, 0.0720, 0.8332],\n",
            "        [0.4019, 0.5616, 0.3079, 0.9280, 0.1668]])\n",
            "View our tensor data as a matrix with two rows and five columns\n",
            "Softmax sums along the columns with dimension 0 so that each column sums up to 1\n",
            "tensor([[0.5981, 0.4384, 0.6921, 0.0720, 0.8332],\n",
            "        [0.4019, 0.5616, 0.3079, 0.9280, 0.1668]])\n",
            "Softmax sums along the rows with dimension 1 so that each row sums up to 1\n",
            "tensor([[0.1094, 0.2290, 0.0430, 0.0081, 0.6106],\n",
            "        [0.1200, 0.4790, 0.0312, 0.1702, 0.1996]])\n",
            "Check its sum tensor(1.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AvzPQI5_YMHA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Loss function\n",
        "\n",
        "The loss funtion is the function that your network is trained to minimize. It computes how far off your network is from the correct answer. Thus, it shows how confident your network is with its prediction. If the loss is very high, the network is confidentn in its answer and the answer is wrong. Uf the answer is correct and the network is confident in its answer, the loss will be low. With a small loss, it will hopefully generalize well unless it overfitted to the training data. \n",
        "\n",
        "In backpropagation, we take the derivative of the loss function to start computing the gradient as we go back through the network. All network components inherit from the ```nn.Module``` function and overried the ```forward()``` function. "
      ]
    },
    {
      "metadata": {
        "id": "J9b8AOcT2Ak0",
        "colab_type": "code",
        "outputId": "e429fc33-b77b-4c64-ff2b-d708c0d31db6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 829
        }
      },
      "cell_type": "code",
      "source": [
        "class FirstModel(nn.Module):\n",
        "  \n",
        "  def __init__(self, vocab_size, num_labels):\n",
        "    super(FirstModel, self).__init__()\n",
        "    self.linear = nn.Linear(vocab_size, num_labels)\n",
        "  \n",
        "  def forward(self, vec):\n",
        "    return F.log_softmax(self.linear(vec), dim=1)\n",
        "\n",
        "def generate_input(num):\n",
        "  return torch.rand(num, 10)\n",
        "\n",
        "def generate_target():\n",
        "  return torch.LongTensor([random.randint(0,2)])\n",
        "\n",
        "model = FirstModel(10, 3) \n",
        "# EXERCISE: use torch.randn to create a random vector of the correct input size for this model\n",
        "vector = torch.randn(4, 10)\n",
        "print(\"Output: \", vector.view(1,1,-1))\n",
        "\n",
        "      \n",
        "loss_function = nn.NLLLoss() # Negative Log Likelihood, aka multi-class cross-entropy \n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1) # Stochastic Gradient Descent as optimizer \n",
        "\n",
        "\n",
        "for epoch in range(10): \n",
        "    for vec in generate_input(4):\n",
        "      # Let's clear the gradients before we start training since PyTorch accumulates them\n",
        "      model.zero_grad()\n",
        "      \n",
        "      # EXERCISE: What happens to the loss currently if we change the target to one of three classes?\n",
        "      # You can use the provided function generate_target for that purpose\n",
        "      # Binary classification but we always set the label to 1 currently\n",
        "      target = torch.LongTensor([1])\n",
        "\n",
        "      # This function gets the logit probabilities \n",
        "      log_probs = model(vec.view(1,-1))\n",
        "      \n",
        "      # This function calculates the loss \n",
        "      loss = loss_function(log_probs, target)\n",
        "      \n",
        "      print(\"Loss: \", loss)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output:  tensor([[[-1.2673, -0.5943, -0.1485, -0.2529, -1.3820,  0.9392,  0.3562,\n",
            "          -0.6562, -1.0002, -0.6830, -0.8332, -0.3416,  0.7182,  0.8067,\n",
            "          -0.7783,  0.0177,  0.3765,  2.2569, -0.4671,  0.1331, -0.7547,\n",
            "          -0.1682,  0.3467,  1.1960,  0.5974, -0.6193,  1.2090,  0.8097,\n",
            "          -0.3727, -0.5333, -1.2778, -0.8878, -0.7815,  0.3195,  0.8211,\n",
            "           0.8643,  0.4917,  0.4454, -0.4788,  0.7548]]])\n",
            "Loss:  tensor(1.5819, grad_fn=<NllLossBackward>)\n",
            "Loss:  tensor(1.5080, grad_fn=<NllLossBackward>)\n",
            "Loss:  tensor(0.8003, grad_fn=<NllLossBackward>)\n",
            "Loss:  tensor(0.6988, grad_fn=<NllLossBackward>)\n",
            "Loss:  tensor(0.4032, grad_fn=<NllLossBackward>)\n",
            "Loss:  tensor(0.5502, grad_fn=<NllLossBackward>)\n",
            "Loss:  tensor(0.3235, grad_fn=<NllLossBackward>)\n",
            "Loss:  tensor(0.4102, grad_fn=<NllLossBackward>)\n",
            "Loss:  tensor(0.4505, grad_fn=<NllLossBackward>)\n",
            "Loss:  tensor(0.4817, grad_fn=<NllLossBackward>)\n",
            "Loss:  tensor(0.2840, grad_fn=<NllLossBackward>)\n",
            "Loss:  tensor(0.3674, grad_fn=<NllLossBackward>)\n",
            "Loss:  tensor(0.1091, grad_fn=<NllLossBackward>)\n",
            "Loss:  tensor(0.1614, grad_fn=<NllLossBackward>)\n",
            "Loss:  tensor(0.1667, grad_fn=<NllLossBackward>)\n",
            "Loss:  tensor(0.1611, grad_fn=<NllLossBackward>)\n",
            "Loss:  tensor(0.1009, grad_fn=<NllLossBackward>)\n",
            "Loss:  tensor(0.0645, grad_fn=<NllLossBackward>)\n",
            "Loss:  tensor(0.1778, grad_fn=<NllLossBackward>)\n",
            "Loss:  tensor(0.1633, grad_fn=<NllLossBackward>)\n",
            "Loss:  tensor(0.2993, grad_fn=<NllLossBackward>)\n",
            "Loss:  tensor(0.0725, grad_fn=<NllLossBackward>)\n",
            "Loss:  tensor(0.0520, grad_fn=<NllLossBackward>)\n",
            "Loss:  tensor(0.0712, grad_fn=<NllLossBackward>)\n",
            "Loss:  tensor(0.0825, grad_fn=<NllLossBackward>)\n",
            "Loss:  tensor(0.1431, grad_fn=<NllLossBackward>)\n",
            "Loss:  tensor(0.1231, grad_fn=<NllLossBackward>)\n",
            "Loss:  tensor(0.2034, grad_fn=<NllLossBackward>)\n",
            "Loss:  tensor(0.1363, grad_fn=<NllLossBackward>)\n",
            "Loss:  tensor(0.0769, grad_fn=<NllLossBackward>)\n",
            "Loss:  tensor(0.0566, grad_fn=<NllLossBackward>)\n",
            "Loss:  tensor(0.0356, grad_fn=<NllLossBackward>)\n",
            "Loss:  tensor(0.1089, grad_fn=<NllLossBackward>)\n",
            "Loss:  tensor(0.0306, grad_fn=<NllLossBackward>)\n",
            "Loss:  tensor(0.0511, grad_fn=<NllLossBackward>)\n",
            "Loss:  tensor(0.0466, grad_fn=<NllLossBackward>)\n",
            "Loss:  tensor(0.0382, grad_fn=<NllLossBackward>)\n",
            "Loss:  tensor(0.0304, grad_fn=<NllLossBackward>)\n",
            "Loss:  tensor(0.0748, grad_fn=<NllLossBackward>)\n",
            "Loss:  tensor(0.0627, grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hEs5jWJ_EWOU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lesson 2: Long-Short Term Memory (LSTM)\n",
        "\n",
        "LSTMs are our first non-linear model that we are going to build. Below is a very simple example on how to start building a very simple model."
      ]
    },
    {
      "metadata": {
        "id": "JVJMdEz_-KjC",
        "colab_type": "code",
        "outputId": "c653d766-f6ea-4356-bd5c-569ed79e74d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "lstm = nn.LSTM(3, 3)\n",
        "inputs = torch.rand(1,3)\n",
        "\n",
        "hidden = (torch.randn(1,1,3), torch.randn(1,1,3))\n",
        "\n",
        "for i in inputs: \n",
        "  out, hidden = lstm(i.view(1,1,-1), hidden)  \n",
        "print(\"Toy output: \", out, \"Hidden: \", hidden)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Toy output:  tensor([[[ 0.1219,  0.4065, -0.0905]]], grad_fn=<StackBackward>) Hidden:  (tensor([[[ 0.1219,  0.4065, -0.0905]]], grad_fn=<StackBackward>), tensor([[[ 0.5348,  0.8511, -0.2229]]], grad_fn=<StackBackward>))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YsTFMOIZH5CR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's make it a bit more complicated and build a POS tagger as an LSTM. Remember, LSTMs have the following shape and calculcates the following functions for each input sequence: \n",
        "\n",
        "\n",
        "> $f_t = \\sigma(W_f x_t + U_f h_{(t-1)} + b_f)$\n",
        "\n",
        "> $i_t = \\sigma(W_i x_t + U_i h_{(t-1)} + b_i)$\n",
        "\n",
        "> $\\tilde c_t = \\tanh(W_g x_t + U_g h_{(t-1)} + b_g)$\n",
        "\n",
        "> $o_t = \\sigma(W_0 x_t + U_0 h_{(t-1)} + b_o)$\n",
        "\n",
        "> $c_t = f_t * c_{(t-1)} + i_t * \\tilde c_t$\n",
        "\n",
        "> $h_t = o_t * \\tanh(c_t)$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "AVeautIZGqlw",
        "colab_type": "code",
        "outputId": "6885d737-22cd-4786-eb36-c6ee644df230",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# Functions to prepare the input\n",
        "def prepare_sequence(seq, to_ix):\n",
        "    idxs = [to_ix[w] for w in seq]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "\n",
        "training_data = [\n",
        "    (\"The frog ate the fly\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
        "    (\"Paul likes the book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"]),\n",
        "    (\"All type the word\".split(), [\"NN\", \"V\", \"DET\", \"NN\"]),\n",
        "    (\"The car broke\".split(), [\"DET\", \"NN\", \"V\"]),\n",
        "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])]\n",
        "word_to_ix = {}\n",
        "\n",
        "for sent, tags in training_data:\n",
        "    for word in sent:\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "\n",
        "print(word_to_ix)\n",
        "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n",
        "\n",
        "# These will usually be more like 300 or 64 dimensional.\n",
        "# We will keep them small, so we can see how the weights change as we train.\n",
        "EMBEDDING_DIM = 6\n",
        "HIDDEN_DIM = 6"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'The': 0, 'frog': 1, 'ate': 2, 'the': 3, 'fly': 4, 'Paul': 5, 'likes': 6, 'book': 7, 'All': 8, 'type': 9, 'word': 10, 'car': 11, 'broke': 12, 'Everybody': 13, 'read': 14, 'that': 15}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nU-p835GITXP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Code cell to define the model \n",
        "class LSTMTagger(nn.Module):\n",
        "    \"\"\" LSTM model to tag words with their correct part-of-speeches\"\"\"\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
        "        super(LSTMTagger, self).__init__()\n",
        "       \n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # input dimensionality (embedding_dim) and output dimensionality (hidden_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
        "\n",
        "        # EXERCISE: initialize the following linear layer to connect the hidden and the output (tag space) layer\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        # The variables here are (num_lyers, minibatch_size, hidden_dim)        \n",
        "        h0 = torch.zeros(1, 1, self.hidden_dim)\n",
        "        c0 = torch.zeros(1, 1, self.hidden_dim)\n",
        "        return (h0, c0)\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        lstm_out, self.hidden = self.lstm(embeds.view(len(sentence), 1, -1), self.hidden)\n",
        "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        return tag_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SSxyX23JP7aE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Auxiliary functions\n",
        "\n",
        "def get_accuracy(targets, prediction): \n",
        "  (max_vals, arg_maxs) = torch.max(prediction.data, dim=1) \n",
        "  num_correct = torch.sum(targets==arg_maxs)\n",
        "  acc = (num_correct * 100.0 / len(targets))\n",
        "  return acc.item()  # percentage based\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wrl0jrbMIchE",
        "colab_type": "code",
        "outputId": "f800e854-833d-4597-a773-b156fe7c649b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1959
        }
      },
      "cell_type": "code",
      "source": [
        "# Code cell to train the model \n",
        "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
        "\n",
        "# EXERCISE: define the loss function and an optimizer\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# Here you can output some sample scores\n",
        "with torch.no_grad():\n",
        "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
        "    tag_scores = model(inputs)\n",
        "    print(\"Sample output: \", tag_scores)\n",
        "    print(\"Sample output after softmax: \", F.softmax(tag_scores, dim=1))\n",
        "  \n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):  # again, normally you would NOT do 300 epochs, it is toy data\n",
        "    accuracy = 0 \n",
        "    for sentence, tags in training_data:\n",
        "        \n",
        "        # EXERCISE: fill in this part of the code - the required steps are provided\n",
        "        # Step 1: clear the accumulated gradients before we start training \n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2: since this is an LSTM we need to initialize the hidden states\n",
        "        # and clear the history from the last instance\n",
        "        model.hidden = model.init_hidden()\n",
        "\n",
        "        # Step 3: prepare the input sequence for the network (words to indices) - see available functions\n",
        "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
        "       \n",
        "        # Step 4: prepare the target sequences to be able to calculate the loss - see available functions \n",
        "        targets = prepare_sequence(tags, tag_to_ix)\n",
        "\n",
        "        # Step 5: run a forward pass \n",
        "        prediction = model(sentence_in)\n",
        "\n",
        "        # Step 6: compute the loss, calculate the gradient and optimize the weights\n",
        "        loss = loss_function(prediction, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        accuracy += get_accuracy(targets, prediction)\n",
        "        \n",
        "    print(\"Epoch %s of %s, Loss %s, Accuracy %s \" % (epoch, num_epochs, loss, accuracy/len(training_data)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample output:  tensor([[-0.9927, -1.5522, -0.8731],\n",
            "        [-0.9463, -1.5547, -0.9148],\n",
            "        [-1.0450, -1.6684, -0.7770],\n",
            "        [-1.0024, -1.5338, -0.8739],\n",
            "        [-0.9898, -1.5225, -0.8912]])\n",
            "Sample output after softmax:  tensor([[0.3706, 0.2118, 0.4176],\n",
            "        [0.3882, 0.2112, 0.4006],\n",
            "        [0.3517, 0.1886, 0.4598],\n",
            "        [0.3670, 0.2157, 0.4173],\n",
            "        [0.3717, 0.2182, 0.4102]])\n",
            "Epoch 0 of 100, Loss tensor(1.1877, grad_fn=<NllLossBackward>), Accuracy 30.6 \n",
            "Epoch 1 of 100, Loss tensor(1.1458, grad_fn=<NllLossBackward>), Accuracy 30.6 \n",
            "Epoch 2 of 100, Loss tensor(1.1133, grad_fn=<NllLossBackward>), Accuracy 30.6 \n",
            "Epoch 3 of 100, Loss tensor(1.0877, grad_fn=<NllLossBackward>), Accuracy 30.6 \n",
            "Epoch 4 of 100, Loss tensor(1.0672, grad_fn=<NllLossBackward>), Accuracy 41.2 \n",
            "Epoch 5 of 100, Loss tensor(1.0505, grad_fn=<NllLossBackward>), Accuracy 55.2 \n",
            "Epoch 6 of 100, Loss tensor(1.0365, grad_fn=<NllLossBackward>), Accuracy 48.6 \n",
            "Epoch 7 of 100, Loss tensor(1.0243, grad_fn=<NllLossBackward>), Accuracy 48.6 \n",
            "Epoch 8 of 100, Loss tensor(1.0133, grad_fn=<NllLossBackward>), Accuracy 48.6 \n",
            "Epoch 9 of 100, Loss tensor(1.0031, grad_fn=<NllLossBackward>), Accuracy 48.6 \n",
            "Epoch 10 of 100, Loss tensor(0.9934, grad_fn=<NllLossBackward>), Accuracy 48.6 \n",
            "Epoch 11 of 100, Loss tensor(0.9840, grad_fn=<NllLossBackward>), Accuracy 48.6 \n",
            "Epoch 12 of 100, Loss tensor(0.9747, grad_fn=<NllLossBackward>), Accuracy 48.6 \n",
            "Epoch 13 of 100, Loss tensor(0.9653, grad_fn=<NllLossBackward>), Accuracy 48.6 \n",
            "Epoch 14 of 100, Loss tensor(0.9557, grad_fn=<NllLossBackward>), Accuracy 48.6 \n",
            "Epoch 15 of 100, Loss tensor(0.9460, grad_fn=<NllLossBackward>), Accuracy 48.6 \n",
            "Epoch 16 of 100, Loss tensor(0.9360, grad_fn=<NllLossBackward>), Accuracy 48.6 \n",
            "Epoch 17 of 100, Loss tensor(0.9257, grad_fn=<NllLossBackward>), Accuracy 48.6 \n",
            "Epoch 18 of 100, Loss tensor(0.9150, grad_fn=<NllLossBackward>), Accuracy 53.6 \n",
            "Epoch 19 of 100, Loss tensor(0.9040, grad_fn=<NllLossBackward>), Accuracy 58.6 \n",
            "Epoch 20 of 100, Loss tensor(0.8927, grad_fn=<NllLossBackward>), Accuracy 58.6 \n",
            "Epoch 21 of 100, Loss tensor(0.8809, grad_fn=<NllLossBackward>), Accuracy 58.6 \n",
            "Epoch 22 of 100, Loss tensor(0.8688, grad_fn=<NllLossBackward>), Accuracy 58.6 \n",
            "Epoch 23 of 100, Loss tensor(0.8563, grad_fn=<NllLossBackward>), Accuracy 58.6 \n",
            "Epoch 24 of 100, Loss tensor(0.8433, grad_fn=<NllLossBackward>), Accuracy 63.6 \n",
            "Epoch 25 of 100, Loss tensor(0.8299, grad_fn=<NllLossBackward>), Accuracy 70.2 \n",
            "Epoch 26 of 100, Loss tensor(0.8162, grad_fn=<NllLossBackward>), Accuracy 70.2 \n",
            "Epoch 27 of 100, Loss tensor(0.8020, grad_fn=<NllLossBackward>), Accuracy 70.2 \n",
            "Epoch 28 of 100, Loss tensor(0.7874, grad_fn=<NllLossBackward>), Accuracy 70.2 \n",
            "Epoch 29 of 100, Loss tensor(0.7724, grad_fn=<NllLossBackward>), Accuracy 70.2 \n",
            "Epoch 30 of 100, Loss tensor(0.7571, grad_fn=<NllLossBackward>), Accuracy 70.2 \n",
            "Epoch 31 of 100, Loss tensor(0.7414, grad_fn=<NllLossBackward>), Accuracy 70.2 \n",
            "Epoch 32 of 100, Loss tensor(0.7255, grad_fn=<NllLossBackward>), Accuracy 70.2 \n",
            "Epoch 33 of 100, Loss tensor(0.7093, grad_fn=<NllLossBackward>), Accuracy 70.2 \n",
            "Epoch 34 of 100, Loss tensor(0.6929, grad_fn=<NllLossBackward>), Accuracy 74.2 \n",
            "Epoch 35 of 100, Loss tensor(0.6764, grad_fn=<NllLossBackward>), Accuracy 74.2 \n",
            "Epoch 36 of 100, Loss tensor(0.6598, grad_fn=<NllLossBackward>), Accuracy 74.2 \n",
            "Epoch 37 of 100, Loss tensor(0.6432, grad_fn=<NllLossBackward>), Accuracy 74.2 \n",
            "Epoch 38 of 100, Loss tensor(0.6266, grad_fn=<NllLossBackward>), Accuracy 85.0 \n",
            "Epoch 39 of 100, Loss tensor(0.6101, grad_fn=<NllLossBackward>), Accuracy 90.0 \n",
            "Epoch 40 of 100, Loss tensor(0.5938, grad_fn=<NllLossBackward>), Accuracy 95.0 \n",
            "Epoch 41 of 100, Loss tensor(0.5776, grad_fn=<NllLossBackward>), Accuracy 95.0 \n",
            "Epoch 42 of 100, Loss tensor(0.5616, grad_fn=<NllLossBackward>), Accuracy 95.0 \n",
            "Epoch 43 of 100, Loss tensor(0.5459, grad_fn=<NllLossBackward>), Accuracy 95.0 \n",
            "Epoch 44 of 100, Loss tensor(0.5304, grad_fn=<NllLossBackward>), Accuracy 95.0 \n",
            "Epoch 45 of 100, Loss tensor(0.5151, grad_fn=<NllLossBackward>), Accuracy 95.0 \n",
            "Epoch 46 of 100, Loss tensor(0.5001, grad_fn=<NllLossBackward>), Accuracy 95.0 \n",
            "Epoch 47 of 100, Loss tensor(0.4852, grad_fn=<NllLossBackward>), Accuracy 95.0 \n",
            "Epoch 48 of 100, Loss tensor(0.4706, grad_fn=<NllLossBackward>), Accuracy 100.0 \n",
            "Epoch 49 of 100, Loss tensor(0.4560, grad_fn=<NllLossBackward>), Accuracy 100.0 \n",
            "Epoch 50 of 100, Loss tensor(0.4415, grad_fn=<NllLossBackward>), Accuracy 100.0 \n",
            "Epoch 51 of 100, Loss tensor(0.4270, grad_fn=<NllLossBackward>), Accuracy 100.0 \n",
            "Epoch 52 of 100, Loss tensor(0.4125, grad_fn=<NllLossBackward>), Accuracy 100.0 \n",
            "Epoch 53 of 100, Loss tensor(0.3977, grad_fn=<NllLossBackward>), Accuracy 100.0 \n",
            "Epoch 54 of 100, Loss tensor(0.3828, grad_fn=<NllLossBackward>), Accuracy 100.0 \n",
            "Epoch 55 of 100, Loss tensor(0.3677, grad_fn=<NllLossBackward>), Accuracy 100.0 \n",
            "Epoch 56 of 100, Loss tensor(0.3523, grad_fn=<NllLossBackward>), Accuracy 100.0 \n",
            "Epoch 57 of 100, Loss tensor(0.3367, grad_fn=<NllLossBackward>), Accuracy 100.0 \n",
            "Epoch 58 of 100, Loss tensor(0.3210, grad_fn=<NllLossBackward>), Accuracy 100.0 \n",
            "Epoch 59 of 100, Loss tensor(0.3055, grad_fn=<NllLossBackward>), Accuracy 100.0 \n",
            "Epoch 60 of 100, Loss tensor(0.2902, grad_fn=<NllLossBackward>), Accuracy 100.0 \n",
            "Epoch 61 of 100, Loss tensor(0.2754, grad_fn=<NllLossBackward>), Accuracy 100.0 \n",
            "Epoch 62 of 100, Loss tensor(0.2613, grad_fn=<NllLossBackward>), Accuracy 100.0 \n",
            "Epoch 63 of 100, Loss tensor(0.2480, grad_fn=<NllLossBackward>), Accuracy 100.0 \n",
            "Epoch 64 of 100, Loss tensor(0.2355, grad_fn=<NllLossBackward>), Accuracy 100.0 \n",
            "Epoch 65 of 100, Loss tensor(0.2239, grad_fn=<NllLossBackward>), Accuracy 100.0 \n",
            "Epoch 66 of 100, Loss tensor(0.2131, grad_fn=<NllLossBackward>), Accuracy 100.0 \n",
            "Epoch 67 of 100, Loss tensor(0.2031, grad_fn=<NllLossBackward>), Accuracy 100.0 \n",
            "Epoch 68 of 100, Loss tensor(0.1939, grad_fn=<NllLossBackward>), Accuracy 100.0 \n",
            "Epoch 69 of 100, Loss tensor(0.1853, grad_fn=<NllLossBackward>), Accuracy 100.0 \n",
            "Epoch 70 of 100, Loss tensor(0.1773, grad_fn=<NllLossBackward>), Accuracy 100.0 \n",
            "Epoch 71 of 100, Loss tensor(0.1699, grad_fn=<NllLossBackward>), Accuracy 100.0 \n",
            "Epoch 72 of 100, Loss tensor(0.1631, grad_fn=<NllLossBackward>), Accuracy 100.0 \n",
            "Epoch 73 of 100, Loss tensor(0.1567, grad_fn=<NllLossBackward>), Accuracy 100.0 \n",
            "Epoch 74 of 100, Loss tensor(0.1507, grad_fn=<NllLossBackward>), Accuracy 100.0 \n",
            "Epoch 75 of 100, Loss tensor(0.1452, grad_fn=<NllLossBackward>), Accuracy 100.0 \n",
            "Epoch 76 of 100, Loss tensor(0.1400, grad_fn=<NllLossBackward>), Accuracy 100.0 \n",
            "Epoch 77 of 100, Loss tensor(0.1351, grad_fn=<NllLossBackward>), Accuracy 100.0 \n",
            "Epoch 78 of 100, Loss tensor(0.1305, grad_fn=<NllLossBackward>), Accuracy 100.0 \n",
            "Epoch 79 of 100, Loss tensor(0.1262, grad_fn=<NllLossBackward>), Accuracy 100.0 \n",
            "Epoch 80 of 100, Loss tensor(0.1221, grad_fn=<NllLossBackward>), Accuracy 100.0 \n",
            "Epoch 81 of 100, Loss tensor(0.1183, grad_fn=<NllLossBackward>), Accuracy 100.0 \n",
            "Epoch 82 of 100, Loss tensor(0.1147, grad_fn=<NllLossBackward>), Accuracy 100.0 \n",
            "Epoch 83 of 100, Loss tensor(0.1113, grad_fn=<NllLossBackward>), Accuracy 100.0 \n",
            "Epoch 84 of 100, Loss tensor(0.1081, grad_fn=<NllLossBackward>), Accuracy 100.0 \n",
            "Epoch 85 of 100, Loss tensor(0.1050, grad_fn=<NllLossBackward>), Accuracy 100.0 \n",
            "Epoch 86 of 100, Loss tensor(0.1021, grad_fn=<NllLossBackward>), Accuracy 100.0 \n",
            "Epoch 87 of 100, Loss tensor(0.0993, grad_fn=<NllLossBackward>), Accuracy 100.0 \n",
            "Epoch 88 of 100, Loss tensor(0.0967, grad_fn=<NllLossBackward>), Accuracy 100.0 \n",
            "Epoch 89 of 100, Loss tensor(0.0942, grad_fn=<NllLossBackward>), Accuracy 100.0 \n",
            "Epoch 90 of 100, Loss tensor(0.0918, grad_fn=<NllLossBackward>), Accuracy 100.0 \n",
            "Epoch 91 of 100, Loss tensor(0.0895, grad_fn=<NllLossBackward>), Accuracy 100.0 \n",
            "Epoch 92 of 100, Loss tensor(0.0873, grad_fn=<NllLossBackward>), Accuracy 100.0 \n",
            "Epoch 93 of 100, Loss tensor(0.0853, grad_fn=<NllLossBackward>), Accuracy 100.0 \n",
            "Epoch 94 of 100, Loss tensor(0.0833, grad_fn=<NllLossBackward>), Accuracy 100.0 \n",
            "Epoch 95 of 100, Loss tensor(0.0814, grad_fn=<NllLossBackward>), Accuracy 100.0 \n",
            "Epoch 96 of 100, Loss tensor(0.0795, grad_fn=<NllLossBackward>), Accuracy 100.0 \n",
            "Epoch 97 of 100, Loss tensor(0.0778, grad_fn=<NllLossBackward>), Accuracy 100.0 \n",
            "Epoch 98 of 100, Loss tensor(0.0761, grad_fn=<NllLossBackward>), Accuracy 100.0 \n",
            "Epoch 99 of 100, Loss tensor(0.0745, grad_fn=<NllLossBackward>), Accuracy 100.0 \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}