{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial8.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dgromann/SemComp_WS2018/blob/master/Tutorial8/Tutorial8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "NqBj76rSaQyU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Lesson 0.0.0: Store this notebook! \n",
        "\n",
        "Go to \"File\" and make sure you store this file as a local copy to either GitHub or your Google Drive. If you do not have a Google account and also do not want to create one, please check Option C below. "
      ]
    },
    {
      "metadata": {
        "id": "Ddv-6ADkjtQ-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Option A) Google Drive WITH collaboration\n",
        "\n",
        "If you want to work in a collaborative manner where each of you in the group can see each other's contributions, one of you needs to store the notebook in Google Drive and share it with the others. You share it by clicking on the SHARE button on the top right of this page and share the link with the \"everyone who receives this link can edit\" option with the other team members per e-mail, skype, or any other way you prefer.\n",
        "\n",
        "If you work with others, keep in mind to always copy the code before you edit it and always indicate your name as a comment (e.g. #Dagmar ) in the cell that it is clear who wrote which part. I also recommend creating a new code cell for your contributions."
      ]
    },
    {
      "metadata": {
        "id": "MWvHoYoAjwtm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Option B) Github without collaboration\n",
        "\n",
        "Collaborative functions are not available when storing the notebook in GitHub; you will see your own work but not that of others.\n"
      ]
    },
    {
      "metadata": {
        "id": "FK33HnCZjyot",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Option C) Download this notebook as ipynb (Jupyter notebook) or py (Python file)\n",
        "\n",
        "To run either of these on your local machine requires the installation of the required programs, which for the first tutorial are Python and NLTK. This will become more as we continue on to machine learning (requiring sklearn) and deep learning (requiring tensorflow and/or pytorch). In Google Codelab all of these are provided and do not need to be installed locally.\n"
      ]
    },
    {
      "metadata": {
        "id": "SRuBm8OE17Sb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lesson 1.0: Text generation with LSTMs\n",
        "\n",
        "Let's generate our first simple text generation model. "
      ]
    },
    {
      "metadata": {
        "id": "h4u0son7dC_O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data proprocessing\n",
        "\n",
        "We will load all required software libraries and data and then preprocess the data. This includes generating a batch generator."
      ]
    },
    {
      "metadata": {
        "id": "v9QbORoBIW72",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Get data and required software packages\n",
        "\n",
        "!wget https://raw.githubusercontent.com/dgromann/SemanticComputing/master/tutorial8/trump_tweets.txt\n",
        "!pip3 install torch\n",
        "\n",
        "import random\n",
        "import math\n",
        "import unidecode\n",
        "import string\n",
        "import re\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l4C4W6ikNP7G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load the data \n",
        "file = unidecode.unidecode(open('trump_tweets.txt').read())\n",
        "file_len = len(file)\n",
        "print('file_len =', file_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m04TxKvtcK5E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Separate data into shorter sequences \n",
        "sequence_length = 200\n",
        "\n",
        "def data_partitioning():\n",
        "    start_index = random.randint(0, file_len - sequence_length)\n",
        "    end_index = start_index + sequence_length + 1\n",
        "    return file[start_index:end_index]\n",
        "\n",
        "print(data_partitioning())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fGNrLu__ddht",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Building the model\n",
        "\n",
        "Next we need to build our model. We will start with a simple RNN."
      ]
    },
    {
      "metadata": {
        "id": "clizphm5ILXZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# EXERCISE: build some kind of RNN model (GRU or LSTM preferably)\n",
        "# Each step of this exercise is defined below\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "        super(RNN, self).__init__()\n",
        "        # Step 1: initialize all the parameters given to the function \n",
        "\n",
        "        # The input to hidden connection has already been provided for you\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        # Step 2: initialize a GRU or LSTM layer as a hidden layer\n",
        "\n",
        "        # Step3 : define a linear output layer \n",
        "\n",
        "        \n",
        "    def init_hidden(self):\n",
        "        # Step4: initialize the hidden state - be aware of the differences in \n",
        "        # type of network - for LSTM you need a hiddena and a cell state \n",
        "        # the variables are (num_lyers, minibatch_size, hidden_dim)\n",
        "\n",
        "        # For a GRU you only need one hidden layer \n",
        "\n",
        "    \n",
        "    def forward(self, input, hidden):\n",
        "        input = self.embedding(input.view(1, -1))\n",
        "        # Step 5: define the forward function for the lstm hidden layer (input.view(1, 1, -1))\n",
        "        \n",
        "        #Step 6: define the forward function for the linear output layer (output.view(1, -1))\n",
        "\n",
        "        \n",
        "        return output, hidden\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0wgsbb17VXKc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# EXERCISE: instantiate the above RNN model and print all model parameters that are \n",
        "# considered in backpropagation; you can address ```model.named_parameters()''' \n",
        "# that returns a kind of dictionary with names and values of the parameters (values.data) \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OH5hppv5eAfm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Preparing data for training\n",
        "\n",
        "We need to specify the input and the output of the network. "
      ]
    },
    {
      "metadata": {
        "id": "f7u6xRzPeG9H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "all_characters = string.printable\n",
        "n_characters = len(all_characters)\n",
        "\n",
        "# Turn string into list of longs\n",
        "def char_tensor(string):\n",
        "    tensor = torch.zeros(len(string)).long()\n",
        "    for c in range(len(string)):\n",
        "        tensor[c] = all_characters.index(string[c])\n",
        "    return Variable(tensor)\n",
        "\n",
        "# Generate individual batches matching input and target vectors\n",
        "def batch_generator():    \n",
        "    partition = data_partitioning()\n",
        "    inp = char_tensor(partition[:-1])\n",
        "    target = char_tensor(partition[1:])\n",
        "    return inp, target\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oXZ1bISOe11J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "Let's define everything needed for training."
      ]
    },
    {
      "metadata": {
        "id": "HPuSaby2e5D4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Sampling a predicted sequence from the network\n",
        "def sample_prediction(prime_str='a', predict_len=100, threshold=0.8):\n",
        "    hidden = model.init_hidden()\n",
        "    prime_input = char_tensor(prime_str)\n",
        "    predicted = prime_str\n",
        "\n",
        "    # Use priming string to \"build up\" hidden state\n",
        "    for p in range(len(prime_str) - 1):\n",
        "        _, hidden = model(prime_input[p], hidden)\n",
        "    inp = prime_input[-1]\n",
        "    \n",
        "    for p in range(predict_len):\n",
        "        output, hidden = model(inp, hidden)\n",
        "        \n",
        "        # Sample from the network as a multinomial distribution\n",
        "        output_dist = output.data.view(-1).div(threshold).exp()\n",
        "        top_i = torch.multinomial(output_dist, 1)[0]\n",
        "        \n",
        "        # Add predicted character to string and use as next input\n",
        "        predicted_char = all_characters[top_i]\n",
        "        predicted += predicted_char\n",
        "        inp = char_tensor(predicted_char)\n",
        "\n",
        "    return predicted\n",
        "\n",
        "print_every = 100\n",
        "plot_every = 10\n",
        "n_layers = 1\n",
        "\n",
        "# EXERCISE: set the following parameters \n",
        "n_epochs = 0\n",
        "hidden_size = 0\n",
        "lr = 0\n",
        "\n",
        "# EXERCISE: initialize the mode and specify an optimizer \n",
        "# because of the distribution used for the sample predictions \n",
        "# we need to use CrossEntropyloss() as loss function\n",
        "model = None\n",
        "model_optimizer = None\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "# Track losses for later visualization\n",
        "all_losses = []\n",
        "loss_avg = 0\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    # Get the batch for this epoch\n",
        "    inp, target = batch_generator()\n",
        "    \n",
        "    # Clear the accumulated gradients before we start training  \n",
        "    model.zero_grad()\n",
        "    \n",
        "    # Initialize the hidden state\n",
        "    hidden = model.init_hidden()\n",
        "    \n",
        "    temp_loss = 0\n",
        "\n",
        "    # Calculate and accummulate loss for each sequence in the batch\n",
        "    for c in range(sequence_length):\n",
        "        output, hidden = model(inp[c], hidden)\n",
        "        temp_loss += loss_function(output, target[c].unsqueeze(0))\n",
        "\n",
        "    # Backpropagation and optimization of parameters\n",
        "    temp_loss.backward()\n",
        "    model_optimizer.step()\n",
        "\n",
        "    # Calculate average loss over whole batch \n",
        "    loss = temp_loss.data.item() / sequence_length       \n",
        "    loss_avg += loss\n",
        "\n",
        "    if epoch % print_every == 0:\n",
        "        print('[Epoch %d, %d%% of total, current loss: %.4f]' % (epoch, epoch / n_epochs * 100, loss))\n",
        "        print(sample_prediction('Th', 100), '\\n')\n",
        "\n",
        "    if epoch % plot_every == 0:\n",
        "        all_losses.append(loss_avg / plot_every)\n",
        "        loss_avg = 0\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LSt3tqFw68PC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "%matplotlib inline\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(all_losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Iedgfzo5hl2g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lesson 1.1: Experiment with hyperparameters and optimization\n",
        "\n",
        "Experiment with the following attributes to improve on the performance of the above model (performance here can only \n",
        "be evaluated by looking at how sensible the produced tweets are): \n",
        "\n",
        "* Change the ```threshold``` number in the ```sample_prediction``` method above - the higher the less importance to high probabilities will be given, ergo the lower the more important high output probabilities will be\n",
        "* Experiment with different optimizers (see [PyTorch Optimizers](https://pytorch.org/docs/stable/optim.html))\n",
        "* Increase the depth of the network (what was the depth again?)\n",
        "* Experiment with the number of epochs\n",
        "\n",
        "Once you are done with optimizing the result and hopefully have obtained some nice automatically generated tweets, you can try the generation process on a different corpus. For instance, a Tweet collection of someone else or frequently these types of language models are applied to some Shakespeare corpus, e.g. [this one provided by Andrew Karpathy](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt).\n"
      ]
    }
  ]
}