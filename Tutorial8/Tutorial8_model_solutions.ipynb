{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial8_model_solutions.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dgromann/SemComp_WS2018/blob/master/Tutorial8_model_solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "NqBj76rSaQyU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Lesson 0.0.0: Store this notebook! \n",
        "\n",
        "Go to \"File\" and make sure you store this file as a local copy to either GitHub or your Google Drive. If you do not have a Google account and also do not want to create one, please check Option C below. "
      ]
    },
    {
      "metadata": {
        "id": "Ddv-6ADkjtQ-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Option A) Google Drive WITH collaboration\n",
        "\n",
        "If you want to work in a collaborative manner where each of you in the group can see each other's contributions, one of you needs to store the notebook in Google Drive and share it with the others. You share it by clicking on the SHARE button on the top right of this page and share the link with the \"everyone who receives this link can edit\" option with the other team members per e-mail, skype, or any other way you prefer.\n",
        "\n",
        "If you work with others, keep in mind to always copy the code before you edit it and always indicate your name as a comment (e.g. #Dagmar ) in the cell that it is clear who wrote which part. I also recommend creating a new code cell for your contributions."
      ]
    },
    {
      "metadata": {
        "id": "MWvHoYoAjwtm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Option B) Github without collaboration\n",
        "\n",
        "Collaborative functions are not available when storing the notebook in GitHub; you will see your own work but not that of others.\n"
      ]
    },
    {
      "metadata": {
        "id": "FK33HnCZjyot",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Option C) Download this notebook as ipynb (Jupyter notebook) or py (Python file)\n",
        "\n",
        "To run either of these on your local machine requires the installation of the required programs, which for the first tutorial are Python and NLTK. This will become more as we continue on to machine learning (requiring sklearn) and deep learning (requiring tensorflow and/or pytorch). In Google Codelab all of these are provided and do not need to be installed locally.\n"
      ]
    },
    {
      "metadata": {
        "id": "SRuBm8OE17Sb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lesson 1.0: Text generation with LSTMs\n",
        "\n",
        "Let's generate our first simple text generation model. "
      ]
    },
    {
      "metadata": {
        "id": "h4u0son7dC_O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data proprocessing\n",
        "\n",
        "We will load all required software libraries and data and then preprocess the data. This includes generating a batch generator."
      ]
    },
    {
      "metadata": {
        "id": "v9QbORoBIW72",
        "colab_type": "code",
        "outputId": "e73daf94-3b45-418e-bbf4-755d99d549d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "cell_type": "code",
      "source": [
        "# Get data and required software packages\n",
        "\n",
        "!wget https://raw.githubusercontent.com/dgromann/SemanticComputing/master/tutorial8/trump_tweets.txt\n",
        "!pip3 install torch\n",
        "\n",
        "import random\n",
        "import math\n",
        "import unidecode\n",
        "import string\n",
        "import re\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-01-31 06:53:00--  https://raw.githubusercontent.com/dgromann/SemanticComputing/master/tutorial8/trump_tweets.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2214449 (2.1M) [text/plain]\n",
            "Saving to: ‘trump_tweets.txt.1’\n",
            "\n",
            "\rtrump_tweets.txt.1    0%[                    ]       0  --.-KB/s               \rtrump_tweets.txt.1  100%[===================>]   2.11M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2019-01-31 06:53:00 (57.4 MB/s) - ‘trump_tweets.txt.1’ saved [2214449/2214449]\n",
            "\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "l4C4W6ikNP7G",
        "colab_type": "code",
        "outputId": "00c69756-edc8-4619-d509-2da151f80caa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Load the data \n",
        "file = unidecode.unidecode(open('trump_tweets.txt').read())\n",
        "file_len = len(file)\n",
        "print('file_len =', file_len)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "file_len = 2212910\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "m04TxKvtcK5E",
        "colab_type": "code",
        "outputId": "b0be1581-f9bf-4e61-b8fd-56779f0e5d1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "# Batch generator \n",
        "\n",
        "sequence_length = 200\n",
        "\n",
        "def data_partitioning():\n",
        "    start_index = random.randint(0, file_len - sequence_length)\n",
        "    end_index = start_index + sequence_length + 1\n",
        "    return file[start_index:end_index]\n",
        "\n",
        "print(data_partitioning())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the Women's Empowerment Panel this afternoon at the @WhiteHouse.... __HTTP__ _E_\n",
            "I'm looking forward to the Super Bowl but looking even more forward to Monday night at 8:00 best episode EVER of Celebri\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fGNrLu__ddht",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Building the model\n",
        "\n",
        "Next we need to build our model. We will start with a simple RNN."
      ]
    },
    {
      "metadata": {
        "id": "clizphm5ILXZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# EXERCISE: build some kind of RNN model (GRU or LSTM preferably)\n",
        "# Each step of this exercise is defined below\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "        super(RNN, self).__init__()\n",
        "        # Step 1: initialize all the parameters given to the function \n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        # The input to hidden connection has already been provided for you\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        # Step 2: initialize a GRU or LSTM layer as a hidden layer\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers)\n",
        "        #self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
        "        \n",
        "        # Step3 : define a linear output layer \n",
        "        self.output = nn.Linear(hidden_size, output_size)\n",
        "        \n",
        "    def init_hidden(self):\n",
        "        # Step4: initialize the hidden state - be aware of the differences in \n",
        "        # type of network - for LSTM you need a hiddena and a cell state \n",
        "        # the variables are (num_lyers, minibatch_size, hidden_dim)\n",
        "        h0 = torch.zeros(1, 1, self.hidden_size)\n",
        "        c0 = torch.zeros(1, 1, self.hidden_size)\n",
        "        return (h0, c0)\n",
        "      \n",
        "        # For a GRU you only need one hidden layer \n",
        "        # return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))\n",
        "    \n",
        "    def forward(self, input, hidden):\n",
        "        input = self.embedding(input.view(1, -1))\n",
        "        # Step 5: define the forward function for the lstm hidden layer (input.view(1, 1, -1))\n",
        "        output, hidden = self.lstm(input.view(1, 1, -1), hidden)\n",
        "        # for GRU \n",
        "        #output, hidden = self.gru(input.view(1, 1, -1), hidden)\n",
        "        \n",
        "        #Step 6: define the forward function for the linear output layer (output.view(1, -1))\n",
        "        output = self.output(output.view(1, -1))\n",
        "        return output, hidden\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0wgsbb17VXKc",
        "colab_type": "code",
        "outputId": "5ffd718d-fe1b-474c-96bd-c7a94e6c9d95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3629
        }
      },
      "cell_type": "code",
      "source": [
        "# EXERCISE: instantiate the above RNN model and print all model parameters that are \n",
        "# considered in backpropagation; you can address ```model.named_parameters()''' \n",
        "# that returns a kind of dictionary with names and values of the parameters (values.data) \n",
        "\n",
        "model = RNN(len(string.printable), 128, len(string.printable))\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "   if param.requires_grad:\n",
        "        print(name, param.data)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "embedding.weight tensor([[-2.0279, -0.0521,  1.8074,  ..., -0.7556, -1.5225, -0.1443],\n",
            "        [-0.0981, -0.0748, -0.4948,  ...,  0.4283,  0.9926,  1.6104],\n",
            "        [-0.0472, -0.0240,  0.3187,  ..., -0.0741,  1.6297,  0.8438],\n",
            "        ...,\n",
            "        [ 0.8263,  0.7753, -0.2259,  ..., -0.9165,  0.4363, -1.0122],\n",
            "        [-0.9328, -0.4002, -0.0673,  ...,  0.0890, -0.7901, -0.6187],\n",
            "        [ 0.5870, -0.5171, -0.4556,  ...,  0.3105, -0.6704, -1.1315]])\n",
            "lstm.weight_ih_l0 tensor([[ 0.0776, -0.0088, -0.0617,  ...,  0.0231, -0.0449,  0.0453],\n",
            "        [ 0.0356, -0.0840,  0.0717,  ..., -0.0489, -0.0638,  0.0173],\n",
            "        [ 0.0525,  0.0854,  0.0420,  ...,  0.0872, -0.0426, -0.0086],\n",
            "        ...,\n",
            "        [-0.0715, -0.0147, -0.0443,  ...,  0.0621,  0.0378,  0.0247],\n",
            "        [ 0.0460, -0.0193,  0.0036,  ...,  0.0816,  0.0513,  0.0782],\n",
            "        [-0.0112, -0.0688,  0.0223,  ...,  0.0840, -0.0453,  0.0484]])\n",
            "lstm.weight_hh_l0 tensor([[-0.0076,  0.0698,  0.0040,  ..., -0.0226, -0.0087, -0.0502],\n",
            "        [ 0.0134, -0.0512,  0.0534,  ...,  0.0151, -0.0502, -0.0354],\n",
            "        [ 0.0034, -0.0077, -0.0144,  ...,  0.0548,  0.0057, -0.0118],\n",
            "        ...,\n",
            "        [-0.0849,  0.0847, -0.0281,  ..., -0.0351, -0.0695,  0.0337],\n",
            "        [ 0.0839,  0.0098, -0.0586,  ..., -0.0312,  0.0670, -0.0430],\n",
            "        [-0.0701, -0.0765,  0.0356,  ...,  0.0283,  0.0770, -0.0433]])\n",
            "lstm.bias_ih_l0 tensor([-0.0245, -0.0704, -0.0383, -0.0323,  0.0774, -0.0416,  0.0280,  0.0749,\n",
            "        -0.0436, -0.0460, -0.0355,  0.0415,  0.0108,  0.0752, -0.0401,  0.0372,\n",
            "        -0.0151,  0.0303, -0.0082,  0.0655, -0.0571, -0.0374,  0.0124,  0.0397,\n",
            "         0.0270,  0.0349, -0.0672,  0.0718, -0.0684,  0.0442, -0.0342, -0.0286,\n",
            "         0.0426,  0.0435,  0.0834, -0.0651,  0.0756,  0.0833, -0.0178, -0.0501,\n",
            "         0.0684,  0.0126,  0.0291,  0.0710, -0.0216, -0.0528,  0.0612,  0.0702,\n",
            "        -0.0656, -0.0659, -0.0380,  0.0658, -0.0823, -0.0390, -0.0866, -0.0323,\n",
            "        -0.0800, -0.0491,  0.0251, -0.0040, -0.0188, -0.0080,  0.0834, -0.0564,\n",
            "         0.0360,  0.0353,  0.0069, -0.0202,  0.0096,  0.0024, -0.0538,  0.0281,\n",
            "        -0.0067, -0.0407, -0.0818, -0.0299, -0.0178,  0.0568, -0.0717, -0.0289,\n",
            "        -0.0570, -0.0606, -0.0636, -0.0159,  0.0344, -0.0821,  0.0339,  0.0324,\n",
            "         0.0634,  0.0300,  0.0261,  0.0862, -0.0196,  0.0188,  0.0055, -0.0742,\n",
            "         0.0405, -0.0682,  0.0482, -0.0678,  0.0769, -0.0460,  0.0045, -0.0444,\n",
            "         0.0387, -0.0480, -0.0058,  0.0016, -0.0117, -0.0042, -0.0766,  0.0583,\n",
            "        -0.0047,  0.0809, -0.0448,  0.0745, -0.0259, -0.0440, -0.0535, -0.0549,\n",
            "         0.0598, -0.0218, -0.0778, -0.0712, -0.0131,  0.0702, -0.0239,  0.0499,\n",
            "        -0.0269,  0.0326,  0.0387, -0.0377,  0.0528, -0.0707,  0.0620,  0.0331,\n",
            "        -0.0374,  0.0440,  0.0505, -0.0275,  0.0686,  0.0553, -0.0652,  0.0032,\n",
            "         0.0022, -0.0198, -0.0709, -0.0389, -0.0820,  0.0766,  0.0207,  0.0430,\n",
            "        -0.0035,  0.0228, -0.0868,  0.0805,  0.0488, -0.0438,  0.0619,  0.0405,\n",
            "         0.0752, -0.0813, -0.0369, -0.0080, -0.0854,  0.0862, -0.0211,  0.0595,\n",
            "        -0.0412,  0.0584,  0.0681, -0.0466, -0.0456, -0.0352,  0.0597, -0.0383,\n",
            "         0.0333,  0.0561, -0.0263,  0.0557,  0.0548,  0.0274,  0.0066,  0.0494,\n",
            "        -0.0638,  0.0691, -0.0614,  0.0157,  0.0401,  0.0065,  0.0609, -0.0495,\n",
            "        -0.0812, -0.0785,  0.0802,  0.0140, -0.0550,  0.0338, -0.0010, -0.0026,\n",
            "         0.0474,  0.0020, -0.0331, -0.0826, -0.0307,  0.0847, -0.0502, -0.0002,\n",
            "        -0.0867,  0.0680, -0.0246, -0.0467,  0.0529, -0.0042,  0.0392,  0.0683,\n",
            "         0.0767, -0.0677,  0.0690, -0.0186, -0.0010, -0.0091, -0.0180,  0.0770,\n",
            "         0.0539,  0.0247, -0.0273,  0.0405,  0.0048,  0.0751, -0.0555, -0.0119,\n",
            "         0.0684,  0.0378, -0.0847,  0.0719, -0.0368,  0.0208,  0.0624,  0.0154,\n",
            "        -0.0787, -0.0142,  0.0372, -0.0661,  0.0805,  0.0570,  0.0176, -0.0358,\n",
            "         0.0361, -0.0603, -0.0830, -0.0380, -0.0478,  0.0088, -0.0774, -0.0287,\n",
            "        -0.0419,  0.0220,  0.0112,  0.0536, -0.0064,  0.0359, -0.0357,  0.0870,\n",
            "         0.0761,  0.0562, -0.0059, -0.0520,  0.0308, -0.0124,  0.0467, -0.0278,\n",
            "        -0.0050,  0.0497,  0.0331,  0.0640, -0.0415,  0.0447, -0.0269,  0.0266,\n",
            "        -0.0554,  0.0748, -0.0214,  0.0394,  0.0745, -0.0272,  0.0197, -0.0140,\n",
            "        -0.0723, -0.0720,  0.0071, -0.0466, -0.0564, -0.0420,  0.0527, -0.0878,\n",
            "        -0.0150,  0.0206, -0.0439, -0.0271,  0.0715, -0.0305, -0.0360, -0.0747,\n",
            "        -0.0635, -0.0009, -0.0464,  0.0800,  0.0089, -0.0333, -0.0683,  0.0026,\n",
            "         0.0832, -0.0319,  0.0692,  0.0839,  0.0302, -0.0238,  0.0366, -0.0039,\n",
            "        -0.0220,  0.0620, -0.0823, -0.0605,  0.0268, -0.0383, -0.0609,  0.0394,\n",
            "        -0.0083,  0.0401, -0.0750,  0.0360,  0.0322, -0.0174,  0.0837, -0.0033,\n",
            "         0.0521,  0.0765,  0.0804, -0.0483, -0.0042,  0.0820,  0.0003, -0.0767,\n",
            "        -0.0688, -0.0818, -0.0186, -0.0076, -0.0483, -0.0709,  0.0223,  0.0501,\n",
            "         0.0432, -0.0210, -0.0607,  0.0098, -0.0675,  0.0194, -0.0858,  0.0084,\n",
            "         0.0500,  0.0842, -0.0666,  0.0343,  0.0121,  0.0666,  0.0794, -0.0336,\n",
            "        -0.0570,  0.0883, -0.0170, -0.0643,  0.0396,  0.0708, -0.0406,  0.0371,\n",
            "         0.0429,  0.0289, -0.0400,  0.0070,  0.0258,  0.0494, -0.0360, -0.0603,\n",
            "         0.0464, -0.0209, -0.0052,  0.0196,  0.0733, -0.0325, -0.0608,  0.0794,\n",
            "         0.0197, -0.0818, -0.0065, -0.0683, -0.0668, -0.0566,  0.0395, -0.0493,\n",
            "        -0.0665, -0.0634, -0.0067,  0.0720,  0.0510,  0.0832, -0.0701, -0.0352,\n",
            "        -0.0054,  0.0076,  0.0247, -0.0238, -0.0100,  0.0707, -0.0835, -0.0022,\n",
            "        -0.0450,  0.0475,  0.0146,  0.0444, -0.0634,  0.0543, -0.0702, -0.0025,\n",
            "         0.0325, -0.0866, -0.0431, -0.0884,  0.0863, -0.0115,  0.0598, -0.0362,\n",
            "        -0.0191, -0.0539, -0.0197, -0.0358, -0.0299,  0.0241,  0.0738,  0.0774,\n",
            "         0.0797,  0.0813, -0.0069, -0.0627,  0.0094,  0.0551, -0.0380, -0.0528,\n",
            "        -0.0787,  0.0159, -0.0326,  0.0090, -0.0680,  0.0529,  0.0233,  0.0378,\n",
            "         0.0324, -0.0711,  0.0671, -0.0836,  0.0532, -0.0030, -0.0515, -0.0559,\n",
            "         0.0246,  0.0834,  0.0049,  0.0061,  0.0379, -0.0652, -0.0563,  0.0451,\n",
            "        -0.0699, -0.0821,  0.0360,  0.0224,  0.0598,  0.0182, -0.0101, -0.0226,\n",
            "         0.0579,  0.0436,  0.0674, -0.0752, -0.0285, -0.0555,  0.0794, -0.0716,\n",
            "         0.0276, -0.0499,  0.0717,  0.0557, -0.0304,  0.0827,  0.0461,  0.0084,\n",
            "        -0.0831,  0.0057,  0.0541,  0.0551,  0.0257, -0.0057,  0.0152,  0.0284,\n",
            "        -0.0700,  0.0364,  0.0270,  0.0468,  0.0594,  0.0837, -0.0808, -0.0830])\n",
            "lstm.bias_hh_l0 tensor([ 2.0312e-02, -2.6475e-02,  4.0071e-02,  3.9001e-02,  7.3494e-02,\n",
            "         3.9846e-02, -7.0935e-02, -7.8102e-02, -5.2936e-02,  6.1141e-02,\n",
            "        -8.3363e-02, -5.4512e-02,  1.8801e-03,  5.8601e-02,  3.6291e-02,\n",
            "         8.5227e-02, -2.4928e-02, -1.8115e-02, -6.0478e-02, -2.9327e-02,\n",
            "         2.8513e-02, -3.7391e-02, -2.2380e-02,  7.8258e-02,  2.2127e-02,\n",
            "        -2.4776e-02, -8.8155e-02,  8.5888e-03,  2.8291e-02, -6.8923e-02,\n",
            "        -3.4401e-02,  4.0308e-03,  5.7269e-02,  7.7996e-02,  9.4832e-03,\n",
            "         4.2548e-02, -5.9218e-02, -1.8218e-02, -4.2638e-02, -1.4129e-02,\n",
            "         4.3710e-02,  4.4939e-02, -4.1671e-03, -3.8537e-02,  7.8622e-02,\n",
            "        -2.9613e-02,  4.9304e-02, -6.9937e-02, -7.9173e-02, -3.1880e-02,\n",
            "         1.2475e-02, -3.9797e-02, -5.3978e-02,  7.7103e-02,  3.2020e-02,\n",
            "        -1.1067e-02,  1.7580e-02,  6.6764e-02, -5.6827e-02,  7.6841e-02,\n",
            "        -1.8786e-02,  1.6915e-03,  3.5689e-02, -1.5357e-02,  5.9656e-02,\n",
            "         2.9916e-02,  4.9579e-02, -6.8552e-02,  7.9321e-02,  7.6036e-03,\n",
            "        -1.3821e-02, -6.5117e-02,  3.6158e-02, -3.5894e-02,  4.4602e-02,\n",
            "        -1.2197e-02, -5.7482e-02, -4.8724e-02, -2.8785e-02, -3.9972e-02,\n",
            "        -3.7760e-02, -1.0909e-02, -7.7208e-02, -4.0335e-02, -6.2148e-02,\n",
            "        -6.2024e-02,  2.9825e-02, -5.3575e-02, -1.5574e-02,  3.6940e-02,\n",
            "        -1.1702e-02, -2.5219e-02, -2.6252e-02,  4.7458e-02, -4.2932e-02,\n",
            "         1.2658e-02, -3.5015e-02, -6.5550e-02, -2.8049e-02,  6.4764e-02,\n",
            "        -7.7360e-02, -2.7222e-02,  7.8553e-02,  7.8878e-03, -3.3977e-02,\n",
            "        -6.0841e-02, -7.9723e-02,  6.8943e-02,  6.2899e-02,  5.9104e-02,\n",
            "        -2.4196e-02,  8.6679e-03,  7.1284e-02,  6.1971e-02, -3.4121e-02,\n",
            "         4.5899e-02,  1.7742e-02, -1.7897e-02, -5.0578e-02, -8.6030e-02,\n",
            "         1.5128e-02, -6.4317e-02, -4.3343e-03,  3.9893e-03, -7.9768e-03,\n",
            "        -1.8410e-02,  8.6185e-02,  7.3268e-02,  7.0994e-02, -1.0687e-02,\n",
            "        -2.4951e-02,  5.4107e-02, -4.7574e-02,  7.6038e-02, -5.7853e-02,\n",
            "         5.0151e-02, -5.7297e-02,  2.3342e-02, -2.9761e-02,  4.9045e-02,\n",
            "        -2.7834e-02,  4.7785e-03, -1.4722e-02,  3.3177e-02,  1.8221e-02,\n",
            "        -4.3218e-02,  7.1795e-02, -8.2847e-02,  5.4248e-02,  1.8416e-02,\n",
            "        -6.8564e-02, -6.0110e-02,  7.2046e-02, -7.1988e-02,  9.2101e-04,\n",
            "        -3.2634e-02, -5.5006e-02, -8.7325e-02,  2.0780e-02, -2.6820e-02,\n",
            "        -2.5575e-02,  2.3889e-02, -5.9191e-02,  8.7759e-02, -6.4993e-02,\n",
            "        -5.7109e-02,  5.0682e-02,  4.4512e-02,  2.4458e-02,  6.9350e-02,\n",
            "         1.4597e-02,  8.6331e-02, -1.5724e-02, -7.0515e-02, -2.4021e-02,\n",
            "         1.6184e-02, -4.0758e-02,  7.0987e-03,  8.6578e-02, -1.1017e-02,\n",
            "        -4.2786e-03, -8.3087e-02, -8.5203e-02,  2.3700e-02, -4.1113e-02,\n",
            "        -5.8292e-03, -5.1420e-03,  3.9230e-02,  8.2359e-02,  3.7691e-02,\n",
            "        -1.1966e-02, -8.6081e-04, -4.5883e-02,  7.3576e-02,  8.2953e-02,\n",
            "        -3.6909e-02,  2.9964e-02,  6.7601e-02,  4.3852e-02,  4.2020e-02,\n",
            "        -2.0724e-02,  5.9725e-02, -3.0715e-02, -3.9853e-02,  6.2048e-02,\n",
            "         3.7870e-02,  3.9558e-02,  3.1191e-02,  1.9419e-02,  3.0636e-02,\n",
            "        -2.4759e-02,  8.5334e-02,  2.1569e-02, -6.5736e-02,  4.5906e-02,\n",
            "         5.6113e-02, -1.2014e-02, -5.6199e-02, -4.7140e-02, -2.8338e-02,\n",
            "         1.5155e-02,  7.1839e-02, -3.2220e-02, -5.0677e-02,  1.4149e-02,\n",
            "        -3.4405e-02,  2.0703e-02,  4.7269e-02,  4.7789e-02,  8.0418e-02,\n",
            "        -6.7640e-02,  2.7131e-02,  3.9515e-02,  5.6584e-02, -5.6874e-02,\n",
            "        -3.6850e-02,  3.5280e-02, -5.9481e-02,  3.6202e-02,  7.4530e-02,\n",
            "        -8.0068e-03, -7.5215e-02, -6.5913e-02, -5.1381e-02, -2.3315e-03,\n",
            "        -2.8970e-02,  8.1282e-02, -4.9893e-02,  6.4403e-02,  4.5273e-02,\n",
            "        -1.1180e-02,  2.5864e-03, -4.3173e-02,  8.5089e-03,  6.4413e-02,\n",
            "        -4.0450e-02, -3.4459e-02, -4.6850e-02,  6.9318e-02,  1.0327e-03,\n",
            "        -6.3542e-02,  4.6195e-02,  2.9346e-02, -4.0737e-02,  4.1467e-02,\n",
            "         6.3792e-03,  4.5551e-02,  7.7906e-02,  1.8494e-02, -6.3298e-02,\n",
            "         3.3374e-02, -2.5391e-02, -6.0528e-02,  1.4937e-02, -6.5817e-02,\n",
            "         5.1534e-02,  6.3096e-02, -5.4596e-02, -2.8588e-02,  2.5523e-02,\n",
            "        -3.0761e-02, -6.6629e-02, -2.4637e-02,  2.0031e-02, -5.9934e-02,\n",
            "        -3.5823e-02, -1.6907e-02, -1.1243e-02,  5.8300e-02,  4.6243e-02,\n",
            "         2.7393e-03,  2.7335e-02,  4.0316e-02, -6.7305e-02,  6.2113e-03,\n",
            "        -7.8784e-02, -1.4132e-02,  7.1334e-02,  8.3419e-02, -5.2921e-02,\n",
            "         9.2765e-03, -2.2951e-02,  4.1139e-02,  1.1544e-02, -7.1146e-05,\n",
            "         7.6092e-02,  8.8072e-02, -7.6942e-02, -5.4394e-02,  3.8087e-02,\n",
            "         5.3871e-02,  4.3956e-02, -6.1081e-02,  3.9200e-02,  6.5550e-02,\n",
            "         1.4968e-02,  5.4817e-02,  8.5932e-02,  2.5022e-02,  2.1446e-03,\n",
            "        -5.8321e-02,  1.6226e-02, -2.6659e-03, -4.4045e-02,  1.6094e-02,\n",
            "         9.0262e-04,  3.9608e-02,  4.4911e-02, -5.2146e-02, -4.6506e-02,\n",
            "         3.0989e-02, -8.0452e-03, -1.2690e-02, -5.9829e-02,  8.3277e-02,\n",
            "         8.4105e-02,  8.6477e-02,  7.4971e-02,  2.1596e-02, -1.2392e-02,\n",
            "        -7.3797e-02, -4.2754e-02, -6.4261e-02, -2.5079e-02, -8.2721e-02,\n",
            "         5.3361e-02, -6.9154e-02,  2.1875e-02, -3.8158e-02,  5.9388e-02,\n",
            "         3.2495e-02, -9.9787e-03,  6.1157e-02,  7.8720e-02,  7.6255e-02,\n",
            "         4.2251e-02, -2.9560e-02,  6.7080e-02,  5.7904e-02, -5.8559e-02,\n",
            "         6.1187e-02, -3.5518e-02, -3.5611e-03,  1.7218e-02, -4.8619e-02,\n",
            "        -7.4807e-02,  5.6552e-02, -5.9880e-02, -5.0561e-02,  7.5325e-02,\n",
            "        -2.2460e-02,  5.4742e-02,  9.9336e-03, -4.2830e-02,  7.4996e-02,\n",
            "        -6.0867e-02,  4.6207e-02,  4.0676e-02,  1.3704e-02,  4.7482e-03,\n",
            "         6.2951e-03,  5.2548e-02,  2.3301e-02, -6.7051e-02, -4.6802e-02,\n",
            "         3.4526e-02,  8.0940e-02, -4.8205e-02, -8.2283e-02, -5.9207e-02,\n",
            "         4.2124e-02,  4.8501e-02, -2.7080e-02, -3.1084e-02, -8.8238e-02,\n",
            "        -2.6057e-03, -1.6951e-02,  1.7181e-02, -5.7366e-02, -8.6296e-02,\n",
            "         6.4111e-02, -4.6106e-02,  8.0784e-02, -6.9564e-02,  7.9916e-02,\n",
            "        -2.2610e-02,  1.9931e-02, -3.5251e-03, -8.2191e-02, -5.5215e-02,\n",
            "         4.2908e-02,  4.1344e-02,  1.8090e-02,  7.3440e-02, -5.9777e-02,\n",
            "         7.4925e-02, -7.9995e-02, -4.7683e-02,  5.7154e-02,  1.4278e-02,\n",
            "         6.0456e-02,  6.7677e-02,  2.5606e-02,  3.7762e-02,  8.7268e-02,\n",
            "        -4.1699e-02, -4.9259e-02,  6.0858e-02,  7.0497e-02,  7.5401e-02,\n",
            "         9.5980e-04, -4.8603e-02, -4.1424e-02, -4.8417e-02, -7.8327e-02,\n",
            "        -1.3981e-02,  5.6179e-04,  2.6247e-02, -7.6432e-02,  7.8416e-02,\n",
            "         3.6465e-02,  6.0454e-02,  4.1320e-02,  2.3052e-02, -2.7055e-02,\n",
            "        -3.4471e-02,  2.9448e-02, -1.9708e-02,  7.3073e-03,  4.3433e-02,\n",
            "         6.8875e-02, -6.3996e-02, -4.1773e-02,  6.7712e-03,  1.2453e-03,\n",
            "        -5.1237e-02, -8.5775e-02,  9.1807e-03,  1.6906e-02,  1.0017e-02,\n",
            "         1.2427e-02, -1.0636e-02, -6.8807e-02, -3.3811e-02,  1.9916e-02,\n",
            "        -2.8084e-02,  7.3287e-02,  9.7640e-03,  8.7497e-02, -4.7709e-03,\n",
            "         3.3895e-02,  8.3439e-02, -2.8099e-02, -5.8206e-02,  7.5911e-02,\n",
            "         1.4235e-02, -7.3219e-02, -4.0282e-02,  2.6925e-02, -8.5648e-02,\n",
            "         6.2696e-02,  1.1726e-02,  8.6246e-02, -5.1037e-03,  5.9430e-02,\n",
            "         3.3382e-02, -7.5781e-02, -3.8879e-02,  1.0664e-02,  3.6163e-02,\n",
            "        -8.8565e-03,  7.2327e-02, -2.7822e-03, -4.1638e-02,  3.8981e-02,\n",
            "         2.3801e-02, -4.2327e-03, -6.9033e-02,  4.0247e-02, -2.1360e-02,\n",
            "         5.0013e-02,  9.4054e-03,  3.5701e-02, -2.1016e-02,  1.4994e-02,\n",
            "        -4.5653e-02,  9.7976e-03,  8.0829e-02, -3.6973e-02, -8.4365e-02,\n",
            "         2.2815e-02,  3.2573e-02])\n",
            "output.weight tensor([[ 0.0246,  0.0835,  0.0334,  ..., -0.0579, -0.0717,  0.0259],\n",
            "        [ 0.0566, -0.0856, -0.0227,  ..., -0.0671, -0.0611,  0.0717],\n",
            "        [-0.0513,  0.0351,  0.0184,  ..., -0.0343,  0.0057, -0.0130],\n",
            "        ...,\n",
            "        [-0.0545, -0.0745,  0.0352,  ...,  0.0752, -0.0121,  0.0070],\n",
            "        [ 0.0714, -0.0377, -0.0666,  ...,  0.0215, -0.0163,  0.0629],\n",
            "        [ 0.0189,  0.0217,  0.0650,  ..., -0.0355,  0.0188, -0.0872]])\n",
            "output.bias tensor([-0.0741, -0.0094, -0.0436,  0.0873,  0.0191,  0.0716,  0.0690,  0.0625,\n",
            "         0.0512,  0.0131, -0.0630,  0.0373,  0.0739,  0.0866,  0.0151, -0.0585,\n",
            "         0.0010, -0.0776, -0.0010, -0.0357,  0.0164,  0.0441, -0.0304,  0.0082,\n",
            "        -0.0299,  0.0262,  0.0743, -0.0874, -0.0415, -0.0126, -0.0306,  0.0176,\n",
            "        -0.0021,  0.0754,  0.0558,  0.0442,  0.0756, -0.0665,  0.0080,  0.0118,\n",
            "         0.0712,  0.0637, -0.0081,  0.0426, -0.0198, -0.0734,  0.0264, -0.0482,\n",
            "        -0.0062, -0.0245,  0.0375,  0.0794, -0.0113, -0.0278,  0.0370,  0.0624,\n",
            "         0.0065,  0.0558, -0.0411,  0.0401,  0.0570,  0.0116, -0.0234, -0.0592,\n",
            "        -0.0574,  0.0646,  0.0837, -0.0465,  0.0756, -0.0274, -0.0619,  0.0678,\n",
            "        -0.0550,  0.0502,  0.0343,  0.0251,  0.0149, -0.0060, -0.0013, -0.0180,\n",
            "        -0.0215, -0.0371,  0.0808,  0.0840, -0.0373, -0.0077,  0.0544, -0.0005,\n",
            "         0.0728,  0.0826,  0.0192,  0.0108,  0.0200, -0.0884, -0.0026,  0.0559,\n",
            "        -0.0410, -0.0273, -0.0596,  0.0162])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OH5hppv5eAfm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Preparing data for training\n",
        "\n",
        "We need to specify the input and the output of the network. "
      ]
    },
    {
      "metadata": {
        "id": "f7u6xRzPeG9H",
        "colab_type": "code",
        "outputId": "5e114513-3b0e-4d86-ec86-02cb28077a2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "all_characters = string.printable\n",
        "n_characters = len(all_characters)\n",
        "\n",
        "\n",
        "# Turn string into list of longs\n",
        "def char_tensor(string):\n",
        "    tensor = torch.zeros(len(string)).long()\n",
        "    for c in range(len(string)):\n",
        "        tensor[c] = all_characters.index(string[c])\n",
        "    return Variable(tensor)\n",
        "\n",
        "print(char_tensor('abcDEF'))\n",
        "\n",
        "# We need to combine input and target into individual batches\n",
        "def batch_generator():    \n",
        "    partition = data_partitioning()\n",
        "    inp = char_tensor(partition[:-1])\n",
        "    target = char_tensor(partition[1:])\n",
        "    return inp, target\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([10, 11, 12, 39, 40, 41])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oXZ1bISOe11J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "Let's define everything needed for training."
      ]
    },
    {
      "metadata": {
        "id": "HPuSaby2e5D4",
        "colab_type": "code",
        "outputId": "dc3b4755-5f40-4d9e-8d22-4a068e835b3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2709
        }
      },
      "cell_type": "code",
      "source": [
        "# Sampling a predicted sequence from the network\n",
        "def sample_prediction(prime_str='a', predict_len=100, threshold=0.8):\n",
        "    hidden = model.init_hidden()\n",
        "    prime_input = char_tensor(prime_str)\n",
        "    predicted = prime_str\n",
        "\n",
        "    # Use priming string to \"build up\" hidden state\n",
        "    for p in range(len(prime_str) - 1):\n",
        "        _, hidden = model(prime_input[p], hidden)\n",
        "    inp = prime_input[-1]\n",
        "    \n",
        "    for p in range(predict_len):\n",
        "        output, hidden = model(inp, hidden)\n",
        "        \n",
        "        # Sample from the network as a multinomial distribution\n",
        "        output_dist = output.data.view(-1).div(threshold).exp()\n",
        "        top_i = torch.multinomial(output_dist, 1)[0]\n",
        "        \n",
        "        # Add predicted character to string and use as next input\n",
        "        predicted_char = all_characters[top_i]\n",
        "        predicted += predicted_char\n",
        "        inp = char_tensor(predicted_char)\n",
        "\n",
        "    return predicted\n",
        "\n",
        "print_every = 100\n",
        "plot_every = 10\n",
        "n_layers = 1\n",
        "\n",
        "# EXERCISE: set the following parameters \n",
        "n_epochs = 4000\n",
        "hidden_size = 128\n",
        "lr = 0.05\n",
        "\n",
        "# EXERCISE: initialize the model, specify an optimizer and\n",
        "# specify CrossEntropyLoss() as a loss function (because of distribution used to \n",
        "# generate sample predictions)\n",
        "model = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
        "model_optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "start = time.time()\n",
        "all_losses = []\n",
        "loss_avg = 0\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    # Get the batch for this epoch\n",
        "    inp, target = batch_generator()\n",
        "    \n",
        "    # Clear the accumulated gradients before we start training  \n",
        "    model.zero_grad()\n",
        "    \n",
        "    # Initialize the hidden state\n",
        "    hidden = model.init_hidden()\n",
        "    \n",
        "    temp_loss = 0\n",
        "\n",
        "    # Calculate and accummulate loss for each sequence in the batch\n",
        "    for c in range(sequence_length):\n",
        "        output, hidden = model(inp[c], hidden)\n",
        "        temp_loss += loss_function(output, target[c].unsqueeze(0))\n",
        "\n",
        "    # Backpropagation and optimization of parameters\n",
        "    temp_loss.backward()\n",
        "    model_optimizer.step()\n",
        "\n",
        "    # Calculate average loss over whole batch \n",
        "    loss = temp_loss.data.item() / sequence_length       \n",
        "    loss_avg += loss\n",
        "\n",
        "    if epoch % print_every == 0:\n",
        "        print('[Epoch %d, %d%% of total, current loss: %.4f]' % (epoch, epoch / n_epochs * 100, loss))\n",
        "        print(sample_prediction('Th', 100), '\\n')\n",
        "\n",
        "    if epoch % plot_every == 0:\n",
        "        all_losses.append(loss_avg / plot_every)\n",
        "        loss_avg = 0\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch 100, 2% of total, current loss: 2.4219]\n",
            "The eve arus onbampar aurpppec no rin the whay TR9E LE La pend sbes tad get # guy ary NO IN n fow mare \n",
            "\n",
            "[Epoch 200, 5% of total, current loss: 2.3871]\n",
            "Thiting ontfry ALASm __HTP__ _E_\n",
            "Wils son thity being itthe cor inate Ive ouncilld's Iasto. Statep Gre \n",
            "\n",
            "[Epoch 300, 7% of total, current loss: 2.2485]\n",
            "The everel the lear in with han faight and derecord ound nod exthat to wit. _E_\n",
            "Gereatt had in are ath \n",
            "\n",
            "[Epoch 400, 10% of total, current loss: 2.1190]\n",
            "This twhe pron es and wil ince say of hith __HTTP__ _E_\n",
            "TV fre for had sast fre At. _E_\n",
            "W stakely mo n \n",
            "\n",
            "[Epoch 500, 12% of total, current loss: 2.0621]\n",
            "This plebrimensiders are prower reat etweeer on MA Dengall the Stated _E_\n",
            "I Poll Obama shet es seccesi \n",
            "\n",
            "[Epoch 600, 15% of total, current loss: 1.9912]\n",
            "The on hours inting must ferione Saniter of ther for belects you'\" belocut there _E_\n",
            "Sectuse the mest  \n",
            "\n",
            "[Epoch 700, 17% of total, current loss: 1.7265]\n",
            "Thimen Sy ton on ginice ton the is moragijittuongred ould pare way to igtonald coments your Loosed Bar \n",
            "\n",
            "[Epoch 800, 20% of total, current loss: 2.1712]\n",
            "These srater Rep Sowneed minigerating the poost five your rome. There doctes caindfrientends! _E_\n",
            "Ante \n",
            "\n",
            "[Epoch 900, 22% of total, current loss: 2.0259]\n",
            "The @ran Can of whave Hampail bricas card bulbe no real ea to he was hoor Time Debe of the horge dy Ba \n",
            "\n",
            "[Epoch 1000, 25% of total, current loss: 2.4194]\n",
            "Thisater @FoxNews Ald __HTTP__ __HTTP__ _E_\n",
            "Why to in on ame.the sidernatacking of Stater fimesape. Ge \n",
            "\n",
            "[Epoch 1100, 27% of total, current loss: 1.9705]\n",
            "The Texplart the @tongration. int @MitResin in the be dow brepreation for the and @bortSank bere @Nany \n",
            "\n",
            "[Epoch 1200, 30% of total, current loss: 1.8122]\n",
            "The resstive ben _E_\n",
            "RT Ad bettthercort a make becortwer fuch hust vote for be be go much wotally grea \n",
            "\n",
            "[Epoch 1300, 32% of total, current loss: 2.3219]\n",
            "This FOR CN _E_\n",
            "RT @Deatter the Deather the defication totation tally thong of that the mishe the @Tac \n",
            "\n",
            "[Epoch 1400, 35% of total, current loss: 2.0017]\n",
            "The with beasshased bey annd a Popered & said ternmens Harss have have to spectustic is storted herang \n",
            "\n",
            "[Epoch 1500, 37% of total, current loss: 2.2092]\n",
            "The naten from I an coment contissiven they an and in expers. \"Delary meter is @FoxNews GOPVEE WAR3T P \n",
            "\n",
            "[Epoch 1600, 40% of total, current loss: 2.0260]\n",
            "The futer __HTTP__ _E_\n",
            ".@MittRomnite.....\"................. _E_\n",
            "Hillams toung America Hown Altonhing e \n",
            "\n",
            "[Epoch 1700, 42% of total, current loss: 1.8268]\n",
            "The xecits Ron't the Mar no shing lear! _E_\n",
            "\"Daive. I can for crowder now in in and for the rue sair t \n",
            "\n",
            "[Epoch 1800, 45% of total, current loss: 1.8673]\n",
            "Thanks: __HTTP__ _E_\n",
            "Thank the me candiba's beecsess. _E_\n",
            "Looking pun sen! _E_\n",
            "Amerion Scomp of seense \n",
            "\n",
            "[Epoch 1900, 47% of total, current loss: 1.7557]\n",
            "The Gast the seet the sderep the moward and the he wing f American the #SYPM:! _E_\n",
            ".@Pareth _E_\n",
            "\"Winit \n",
            "\n",
            "[Epoch 2000, 50% of total, current loss: 2.1845]\n",
            "The rall newsed havens the smart all of 201) __HTTP__ _E_\n",
            "The Whit The I prodimed at by the am is hisn \n",
            "\n",
            "[Epoch 2100, 52% of total, current loss: 2.2230]\n",
            "Thut __HTTP__ __HTTP__ __HTTP__ _E_\n",
            "I achelenson's President Parthingtund to Take Shoung courther turn \n",
            "\n",
            "[Epoch 2200, 55% of total, current loss: 1.9314]\n",
            "There a there A. I was a thet goned a with @$BACIM & ward of of of the debt the deal stod the is shoul \n",
            "\n",
            "[Epoch 2300, 57% of total, current loss: 2.0178]\n",
            "The now bee to the better. #Cregulable wonder be in be the works. _E_\n",
            "Jesturant Statefer they words be \n",
            "\n",
            "[Epoch 2400, 60% of total, current loss: 1.6818]\n",
            "They E#Trumpp20016 __HTTP__ _E_\n",
            "The election. Mence tonight on tope convillows are sepect! _E_\n",
            "#Revery \n",
            "\n",
            "[Epoch 2500, 62% of total, current loss: 1.6739]\n",
            "Thim This time to hell the Senought well got the >ould leading first this & it. The US.S. _E_\n",
            "Corompso \n",
            "\n",
            "[Epoch 2600, 65% of total, current loss: 1.9322]\n",
            "The'm havelt not supers other! _E_\n",
            "'xin a find to lovers @reitt reporters of my fut story. _E_\n",
            "We Russ \n",
            "\n",
            "[Epoch 2700, 67% of total, current loss: 1.7965]\n",
            "The America I revia that workers & that @O by @Suppired Mage forced way live @NBC __HTTP_Do Amending h \n",
            "\n",
            "[Epoch 2800, 70% of total, current loss: 1.7293]\n",
            "Thank you is on on and dection in geting on waster. This as and are debrations aid monight the my over \n",
            "\n",
            "[Epoch 2900, 72% of total, current loss: 1.5900]\n",
            "The New Golf to women @GAIGAgai __HTTP__ _E_\n",
            "I mediased times of men on and I ama saughed poodible to  \n",
            "\n",
            "[Epoch 3000, 75% of total, current loss: 2.2540]\n",
            "The even has for Americans lart prokes 200000 Hoody #WEND CNN:TO nowant of Susmsive sire septing Congr \n",
            "\n",
            "[Epoch 3100, 77% of total, current loss: 1.7470]\n",
            "The will be is icurating many happen W. Get will be his bordely for the beautinate world the members y \n",
            "\n",
            "[Epoch 3200, 80% of total, current loss: 1.9226]\n",
            "The PSW ACAGAMERIC AFIrShamer _E_\n",
            "Pew but should be must come for the lugh even weleadn't just are tha \n",
            "\n",
            "[Epoch 3300, 82% of total, current loss: 1.5502]\n",
            "The Champion noght! _E_\n",
            "I'm of the problemor the provess. United Tocessan't Touth Hillary of the newst \n",
            "\n",
            "[Epoch 3400, 85% of total, current loss: 1.8851]\n",
            "The 3verychonew premed  givet melf them __HTTP__ _E_\n",
            "Hepending these interview pelverriels. _E_\n",
            "ObamaC \n",
            "\n",
            "[Epoch 3500, 87% of total, current loss: 2.1219]\n",
            "The should a and in he is to best the Enjoy domersilublishucks the Stapperiolite hashurs this if our h \n",
            "\n",
            "[Epoch 3600, 90% of total, current loss: 1.6934]\n",
            "The Bump is sai numbers my gasted I am am out guy! _E_\n",
            "Our execordsible to go media @BarackOnKa are to \n",
            "\n",
            "[Epoch 3700, 92% of total, current loss: 1.7476]\n",
            "Thirt in __HTTP__ _E_\n",
            "I happending to herowite #NN @AKEGATIO him resport chancord ISIS to being the wa \n",
            "\n",
            "[Epoch 3800, 95% of total, current loss: 1.6571]\n",
            "The P. _E_\n",
            "Ternal State __HTTP__ _E_\n",
            "I deal missessed amazingtly dumband to disural no many and have s \n",
            "\n",
            "[Epoch 3900, 97% of total, current loss: 1.6582]\n",
            "The with @Menthaterson American the with the with $30T accribe elecpecting this great we will be to be \n",
            "\n",
            "[Epoch 4000, 100% of total, current loss: 1.7868]\n",
            "The are to fall uportan Demer pand and parth in to @MittRomney far of that has exastion to deficight U \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LSt3tqFw68PC",
        "colab_type": "code",
        "outputId": "5edfc4db-6cf0-43d8-8706-271cb9462d95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "%matplotlib inline\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(all_losses)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f95659cbf60>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nErkQQjjc/wds9lLpxKpkkgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Iedgfzo5hl2g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lesson 1.1: Experiment with hyperparameters and optimization\n",
        "\n",
        "Experiment with the following attributes to improve on the performance of the above model (performance here can only \n",
        "be evaluated by looking at how sensible the produced tweets are): \n",
        "\n",
        "* Change the ```threshold``` number in the ```sample_prediction``` method above - the higher the less importance to high probabilities will be given, ergo the lower the more important high output probabilities will be\n",
        "* Experiment with different optimizers (see [PyTorch Optimizers](https://pytorch.org/docs/stable/optim.html))\n",
        "* Increase the depth of the network (what was the depth again?)\n",
        "* Experiment with the number of epochs\n",
        "\n",
        "Once you are done with optimizing the result and hopefully have obtained some nice automatically generated tweets, you can try the generation process on a different corpus. For instance, a Tweet collection of someone else or frequently these types of language models are applied to some Shakespeare corpus, e.g. [this one provided by Andrew Karpathy](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt).\n"
      ]
    }
  ]
}
