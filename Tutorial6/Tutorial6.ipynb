{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial6",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dgromann/SemComp_WS2018/blob/master/Tutorial6/Tutorial6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ysAmgMvHjrp_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Lesson 0.0.0: Store this notebook! \n",
        "\n",
        "Go to \"File\" and make sure you store this file as a local copy to either GitHub or your Google Drive. If you do not have a Google account and also do not want to create one, please check Option C below. "
      ]
    },
    {
      "metadata": {
        "id": "Ddv-6ADkjtQ-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Option A) Google Drive WITH collaboration\n",
        "\n",
        "If you want to work in a collaborative manner where each of you in the group can see each other's contributions, one of you needs to store the notebook in Google Drive and share it with the others. You share it by clicking on the SHARE button on the top right of this page and share the link with the \"everyone who receives this link can edit\" option with the other team members per e-mail, skype, or any other way you prefer.\n",
        "\n",
        "If you work with others, keep in mind to always copy the code before you edit it and always indicate your name as a comment (e.g. #Dagmar ) in the cell that it is clear who wrote which part. I also recommend creating a new code cell for your contributions."
      ]
    },
    {
      "metadata": {
        "id": "MWvHoYoAjwtm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Option B) Github without collaboration\n",
        "\n",
        "Collaborative functions are not available when storing the notebook in GitHub; you will see your own work but not that of others.\n"
      ]
    },
    {
      "metadata": {
        "id": "FK33HnCZjyot",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Option C) Download this notebook as ipynb (Jupyter notebook) or py (Python file)\n",
        "\n",
        "To run either of these on your local machine requires the installation of the required programs, which for the first tutorial are Python and NLTK. This will become more as we continue on to machine learning (requiring sklearn) and deep learning (requiring tensorflow and/or pytorch). In Google Codelab all of these are provided and do not need to be installed locally.\n"
      ]
    },
    {
      "metadata": {
        "id": "llRJLqnF_4Hx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lesson 0.1: Pytorch tutorial - basics\n",
        "\n",
        "In order to get started with deep learning and practically code up neural networks, we need to familiarize ourselves with the packages that can be used to this end. There are two basic open source machine learning frameworks that can be used to this end: \n",
        "\n",
        "*   TensorFlow (Google)\n",
        "*   Torch (Facebook, Google DeepMind, Twitter)\n",
        "\n",
        "Since these are high level core libraries, it is easier to use a framework that builds on top of it and adds some usability and documentation. We are going to for once not use the Google solution, but will go with the Facebook solution of Pytorch. This first part of today's tutorial will introduce you to some core concepts of Pytorch before we start working with embeddings.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "4zHofgv7Ksp8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Let's first install pytorch \n",
        "!pip3 install torch torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zT6LYQjvJ3pm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The most basic and important concept in Pytorch is that of a **Tensor**, To speed up computation and offer more flexibility, pytorch replaces numpy arrays with tensors."
      ]
    },
    {
      "metadata": {
        "id": "aZtJy89bKGfk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy \n",
        "\n",
        "# Seed for random number generator to ensure reproducibility of\n",
        "# random initializations\n",
        "torch.manual_seed(1)\n",
        "\n",
        "#Difference between tensor and numpy array\n",
        "a = torch.ones(5)\n",
        "print(\"Tensor: \")\n",
        "print(a)\n",
        "print(\"Numpy array: \")\n",
        "print(a.numpy(), \"\\n\")\n",
        "\n",
        "\n",
        "# This creates a randomly initialized 5 x 3 matrix\n",
        "rand = torch.rand(5,3)\n",
        "print(\"Randomly initialized tensor: \", rand, \"\\n\")\n",
        "\n",
        "# This creates a 5 x 3 matriy filled with zeros and of dtype long\n",
        "# There are eight datatypes in tensor, this one is a datatype of 64-bit integer (signed)\n",
        "# Here are the others: https://pytorch.org/docs/stable/tensors.html\n",
        "zeros = torch.zeros(5,3, dtype=torch.long)\n",
        "print(\"Tensor initialized with zeros: \", zeros, \"\\n\")\n",
        "\n",
        "# Directly initialize a tensor with data \n",
        "data = torch.tensor([5.5, 3])\n",
        "print(\"Tensor initilialized with data: \", data, \"\\n\")\n",
        "\n",
        "# You can redefine an existing tensor \n",
        "redefined = rand.new_ones(5, 3, dtype=torch.double)\n",
        "print(\"Redefined randomly initialized tensor: \", redefined, \"\\n\")\n",
        "\n",
        "redefined_too = torch.rand_like(redefined, dtype=torch.float)\n",
        "print(\"Initializing randomly based on the size of redefined: \", redefined_too, \"\\n\")\n",
        "\n",
        "\n",
        "# Get the size - this is actually a tuple that supports tuple operations \n",
        "print(redefined_too.size(), \"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qFJM8p3JR3-v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Tensors in torch also support basic operations: "
      ]
    },
    {
      "metadata": {
        "id": "ETVukvJ4R3P_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Addition of tensors matching in size\n",
        "x = torch.rand(5, 3)\n",
        "y= torch.rand(5, 3)\n",
        "print(\"Addition: \", x + y, \"\\n\")\n",
        "print(\"Addition alternative syntax: \", torch.add(x, y), \"\\n\")\n",
        "\n",
        "# Addition with providing a tensor as argument\n",
        "result = torch.empty(5, 3)\n",
        "print(\"Addition with tensor as argument: \", torch.add(x, y, out=result), \"\\n\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mNEfziOWS3Wv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "All opreations that mutate a tensor are indicated with an underscore _ such as the example below:"
      ]
    },
    {
      "metadata": {
        "id": "l32jEjSBS2bk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# add x to y\n",
        "y.add_(x)\n",
        "print(\"Adding x to y: \", y)\n",
        "\n",
        "# you can also add a number \n",
        "print(\"Adding 1: \", y.add_(1))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v74kjaPZTINW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Indexing operations of tensors follow the numpy standard:"
      ]
    },
    {
      "metadata": {
        "id": "A7ZOTnMCTNDo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Indexing\n",
        "print(\"X: \", x, \"\\n\")\n",
        "print(\"Element at index one of each row of the matrix \", x[:, 1], \"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8Qu1om-xTyoI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Resizing: if you wish to change the shape of the tensor you can use torch.view:"
      ]
    },
    {
      "metadata": {
        "id": "ZF_3EW5-UG1Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Exercise: resizing: what effect do the following\n",
        "# resize operations have on the tensors\n",
        "x = torch.randn(4, 4)\n",
        "y = x.view(16)\n",
        "z = x.view(-1, 8)\n",
        "print(\"Resizing: \")\n",
        "print(\"Original\")\n",
        "print(x)\n",
        "print(\"Resized view(16)\", y, \"\\n\")\n",
        "print(\"Resized view(-1, 8)\", z, \"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lbyORAySgH4D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lesson 1:  Word Embeddings - first steps\n",
        "\n",
        "We will start looking at Pytorch and then play with existing embeddings. The training in pytorch we will do next week. "
      ]
    },
    {
      "metadata": {
        "id": "9bZEgkd6gYMC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Again random number generator to ensure reproducibility\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cODwB6TlGm1_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here is a mini-example of how to initialize the layer randomly and with only two words."
      ]
    },
    {
      "metadata": {
        "id": "9jvjHAr4sEdQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Map words to index to produce one-hot encodings \n",
        "word_to_ix = {\"hello\": 0, \"world\": 1}\n",
        "\n",
        "# Initialize the embedding layer (nn = neural network) with the number of the \n",
        "# vocabulary and the dimensionality of the vectors \n",
        "# here: two words, vectors of 5 dimensions as ouput\n",
        "embeds = nn.Embedding(2, 5) \n",
        "lookup_tensor = torch.tensor(list(word_to_ix.values()), dtype=torch.long)\n",
        "\n",
        "# Create a look up tensor for the random embeddings\n",
        "#for key, index in word_to_ix.items(): \n",
        "embeddings = embeds(lookup_tensor)\n",
        "print(embeddings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bW3IPEwgLCrO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lesson 2: Use existing embeddings\n",
        "\n",
        "The code below exemplifies how to load a trained embedding model in the gensim library. "
      ]
    },
    {
      "metadata": {
        "id": "aMaN6E36JzNI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Let's first load a small subset of word2vec embeddings that have been trained on a \n",
        "# large corpus of news documents  \n",
        "!wget https://github.com/dgromann/SemanticComputing/raw/master/tutorial6/word2vec_embeddings.bin\n",
        "!wget https://github.com/dgromann/SemanticComputing/raw/master/tutorial6/analogy.txt\n",
        "!pip3 install gensim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aZG8KQMILXz-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from sklearn.decomposition import PCA\n",
        "from matplotlib import pyplot\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "69aLE4-7LxlO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Let's load the model\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format(\"word2vec_embeddings.bin\", binary=True)\n",
        "\n",
        "# Print the length fo the whole vocabulary \n",
        "print(len(model.wv.vocab))\n",
        "\n",
        "# Print the embedding of a specific word \n",
        "print(model[\"good\"])\n",
        "\n",
        "# Get the 10 most similar words of \"good\"\n",
        "most_similar = model.most_similar(\"good\", topn=10)\n",
        "\n",
        "# Check whether our embeddings are good at the analogy task\n",
        "analogy = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5bQeIYOpPyVt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Exercise: get the length and datatype of the vector - \n",
        "# what does the length tell us and how would it influence our parameters when training? \n",
        "\n",
        "\n",
        "# Exercise: implement your own version of the analogy check without using a \n",
        "# built in function \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LZx7Tk7hRO8h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Evalute the loaded subset of the embeddings on the analogy.txt file from the Github repository. Each type of analogy is \n",
        "marked by a \": \" at the beginning of the subset. \n",
        "\n",
        "For instance, the analogy of capitals and countries is marked by \": capital-common-countries\" before the data start. Provide an accuracy measure for each type of analogy and compare on which it performs best."
      ]
    },
    {
      "metadata": {
        "id": "mucqVnmcQzrs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Exercise: Evaluate the embeddings on the analogy text file (already loaded for you)\n",
        "# To do so you need to load the file, and test the analogy task on each line apart from \n",
        "# the header lines marked with \": \"\n",
        "# If you want, it would also be interesting to get accuracies for the different types of \n",
        "# analogies that are seprated by a header line marked with \": \"\n",
        "\n",
        "analogy = open(\"analogy.txt\", \"r\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}