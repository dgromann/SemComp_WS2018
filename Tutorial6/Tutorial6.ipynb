{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial6",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dgromann/SemComp_WS2018/blob/master/Tutorial6/Tutorial6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ysAmgMvHjrp_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Lesson 0.0.0: Store this notebook! \n",
        "\n",
        "Go to \"File\" and make sure you store this file as a local copy to either GitHub or your Google Drive. If you do not have a Google account and also do not want to create one, please check Option C below. "
      ]
    },
    {
      "metadata": {
        "id": "Ddv-6ADkjtQ-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Option A) Google Drive WITH collaboration\n",
        "\n",
        "If you want to work in a collaborative manner where each of you in the group can see each other's contributions, one of you needs to store the notebook in Google Drive and share it with the others. You share it by clicking on the SHARE button on the top right of this page and share the link with the \"everyone who receives this link can edit\" option with the other team members per e-mail, skype, or any other way you prefer.\n",
        "\n",
        "If you work with others, keep in mind to always copy the code before you edit it and always indicate your name as a comment (e.g. #Dagmar ) in the cell that it is clear who wrote which part. I also recommend creating a new code cell for your contributions."
      ]
    },
    {
      "metadata": {
        "id": "MWvHoYoAjwtm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Option B) Github without collaboration\n",
        "\n",
        "Collaborative functions are not available when storing the notebook in GitHub; you will see your own work but not that of others.\n"
      ]
    },
    {
      "metadata": {
        "id": "FK33HnCZjyot",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Option C) Download this notebook as ipynb (Jupyter notebook) or py (Python file)\n",
        "\n",
        "To run either of these on your local machine requires the installation of the required programs, which for the first tutorial are Python and NLTK. This will become more as we continue on to machine learning (requiring sklearn) and deep learning (requiring tensorflow and/or pytorch). In Google Codelab all of these are provided and do not need to be installed locally.\n"
      ]
    },
    {
      "metadata": {
        "id": "llRJLqnF_4Hx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lesson 0.1: Pytorch tutorial - basics\n",
        "\n",
        "In order to get started with deep learning and practically code up neural networks, we need to familiarize ourselves with the packages that can be used to this end. There are two basic open source machine learning frameworks that can be used to this end: \n",
        "\n",
        "*   TensorFlow (Google)\n",
        "*   Torch (Facebook, Google DeepMind, Twitter)\n",
        "\n",
        "Since these are high level core libraries, it is easier to use a framework that builds on top of it and adds some usability and documentation. We are going to for once not use the Google solution, but will go with the Facebook solution of Pytorch. This first part of today's tutorial will introduce you to some core concepts of Pytorch before we start working with embeddings.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "4zHofgv7Ksp8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Let's first install pytorch \n",
        "!pip3 install torch torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zT6LYQjvJ3pm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The most basic and important concept in Pytorch is that of a **Tensor**, To speed up computation and offer more flexibility, pytorch replaces numpy arrays with tensors."
      ]
    },
    {
      "metadata": {
        "id": "aZtJy89bKGfk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy \n",
        "\n",
        "# Seed for random number generator to ensure reproducibility of\n",
        "# random initializations\n",
        "torch.manual_seed(1)\n",
        "\n",
        "#Difference between tensor and numpy array\n",
        "a = torch.ones(5)\n",
        "print(\"Tensor: \")\n",
        "print(a)\n",
        "print(\"Numpy array: \")\n",
        "print(a.numpy(), \"\\n\")\n",
        "\n",
        "\n",
        "# This creates a randomly initialized 5 x 3 matrix\n",
        "rand = torch.rand(5,3)\n",
        "print(\"Randomly initialized tensor: \", rand, \"\\n\")\n",
        "\n",
        "# This creates a 5 x 3 matriy filled with zeros and of dtype long\n",
        "# There are eight datatypes in tensor, this one is a datatype of 64-bit integer (signed)\n",
        "# Here are the others: https://pytorch.org/docs/stable/tensors.html\n",
        "zeros = torch.zeros(5,3, dtype=torch.long)\n",
        "print(\"Tensor initialized with zeros: \", zeros, \"\\n\")\n",
        "\n",
        "# Directly initialize a tensor with data \n",
        "data = torch.tensor([5.5, 3])\n",
        "print(\"Tensor initilialized with data: \", data, \"\\n\")\n",
        "\n",
        "# You can redefine an existing tensor \n",
        "redefined = rand.new_ones(5, 3, dtype=torch.double)\n",
        "print(\"Redefined randomly initialized tensor: \", redefined, \"\\n\")\n",
        "\n",
        "redefined_too = torch.rand_like(redefined, dtype=torch.float)\n",
        "print(\"Initializing randomly based on the size of redefined: \", redefined_too, \"\\n\")\n",
        "\n",
        "\n",
        "# Get the size - this is actually a tuple that supports tuple operations \n",
        "print(redefined_too.size(), \"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qFJM8p3JR3-v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Tensors in torch also support basic operations: "
      ]
    },
    {
      "metadata": {
        "id": "ETVukvJ4R3P_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Addition of tensors matching in size\n",
        "x = torch.rand(5, 3)\n",
        "y= torch.rand(5, 3)\n",
        "print(\"Addition: \", x + y, \"\\n\")\n",
        "print(\"Addition alternative syntax: \", torch.add(x, y), \"\\n\")\n",
        "\n",
        "# Addition with providing a tensor as argument\n",
        "result = torch.empty(5, 3)\n",
        "print(\"Addition with tensor as argument: \", torch.add(x, y, out=result), \"\\n\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mNEfziOWS3Wv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "All opreations that mutate a tensor are indicated with an underscore _ such as the example below:"
      ]
    },
    {
      "metadata": {
        "id": "l32jEjSBS2bk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# add x to y\n",
        "y.add_(x)\n",
        "print(\"Adding x to y: \", y)\n",
        "\n",
        "# you can also add a number \n",
        "print(\"Adding 1: \", y.add_(1))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v74kjaPZTINW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Indexing operations of tensors follow the numpy standard:"
      ]
    },
    {
      "metadata": {
        "id": "A7ZOTnMCTNDo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Indexing\n",
        "print(\"X: \", x, \"\\n\")\n",
        "print(\"Element at index one of each row of the matrix \", x[:, 1], \"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8Qu1om-xTyoI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Resizing: if you wish to change the shape of the tensor you can use torch.view:"
      ]
    },
    {
      "metadata": {
        "id": "ZF_3EW5-UG1Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Exercise: resizing: what effect do the following\n",
        "# resize operations have on the tensors\n",
        "x = torch.randn(4, 4)\n",
        "y = x.view(16)\n",
        "z = x.view(-1, 8)\n",
        "print(\"Resizing: \")\n",
        "print(\"Original\")\n",
        "print(x)\n",
        "print(\"Resized view(16)\", y, \"\\n\")\n",
        "print(\"Resized view(-1, 8)\", z, \"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZoMySrpVlUIJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Gradients and Backpropagation**\n",
        "\n",
        "If you set the flag  ```.requires_grad``` on a ```torch.Tensor``` to ```True``` the program will track all operations on it in order to enable later operations, such as backpropagation, which is very important to neural networks. \n",
        "\n",
        "When you finish all computations on your tensor, you can then simply call the function ```.backward()``` and have all the gradients computed automatically.. The gradient will then automatically be accumulated in the attribute ```.grad```. \n",
        "\n",
        "If you wish to disconnect a specific tensor from this process of tracking all operations, you can call the function ```.detach()```. This prevents future computations from being tracked. You can alternatively wrap the code block in a function ```with torch.no_grad()``` which does not track the operations on any variables included in the block. This is particularly helpful if you wish to evaluate a model that has trainable parameters with *required_grad=True* flags but for which we don't need the gradients in evaluation. \n",
        "\n",
        "There’s one more class which is very important for autograd implementation - a `Function`.\n",
        "\n",
        "`Tensor` and `Function` are interconnected and build up an acyclic graph, that encodes a complete history of computation. Each tensor has a `.grad_fn` attribute that references a Function that has created the `Tensor` (except for Tensors created by the user - their grad_fn is None).\n",
        "\n",
        "If you want to compute the derivatives, you can call `.backward() `on a `Tensor`. If `Tensor` is a scalar (i.e. it holds a one element data), you don’t need to specify any arguments to `backward()`, however if it has more elements, you need to specify a `gradient` argument that is a tensor of matching shape.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "yYXem-00m_rw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Tensor that requires gradien = operations are being tracked \n",
        "x = torch.ones(2, 2, requires_grad=True)\n",
        "print(x)\n",
        "\n",
        "# Let's do some operation \n",
        "y = x + 2 \n",
        "print(y)\n",
        "\n",
        "# y was created that has a grad_fn \n",
        "print(y.grad_fn)\n",
        "\n",
        "# Some more operations \n",
        "z = y * y * 3\n",
        "out = z.mean()\n",
        "\n",
        "print(z, out)\n",
        "\n",
        "\n",
        "# Gradients\n",
        "# Let's calculate and print the gradietn (d(out)/dx) and print it\n",
        "out.backward()\n",
        "print(x.grad)\n",
        "\n",
        "# Stop autograd from tracking \n",
        "print(x.requires_grad)\n",
        "print((x ** 2).requires_grad)\n",
        "\n",
        "with torch.no_grad():\n",
        "    print((x ** 2).requires_grad)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lbyORAySgH4D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lesson 1:  Word Embeddings - first steps\n",
        "\n",
        "We will start looking at Pytorch and then play with existing embeddings. The training in pytorch we will do next week. "
      ]
    },
    {
      "metadata": {
        "id": "9bZEgkd6gYMC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Again random number generator to ensure reproducibility\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cODwB6TlGm1_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here is a mini-example of how to initialize the layer randomly and with only two words."
      ]
    },
    {
      "metadata": {
        "id": "9jvjHAr4sEdQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Map words to index to produce one-hot encodings \n",
        "word_to_ix = {\"hello\": 0, \"world\": 1}\n",
        "\n",
        "# Initialize the embedding layer (nn = neural network) with the number of the \n",
        "# vocabulary and the dimensionality of the vectors \n",
        "# here: two words, vectors of 5 dimensions as ouput\n",
        "embeds = nn.Embedding(2, 5) \n",
        "lookup_tensor = torch.tensor(list(word_to_ix.values()), dtype=torch.long)\n",
        "\n",
        "# Create a look up tensor for the random embeddings\n",
        "#for key, index in word_to_ix.items(): \n",
        "embeddings = embeds(lookup_tensor)\n",
        "print(embeddings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VviNW1RYouRq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's train our first embeddings. What do SGD and lr mean in the code below? What happens if you increase lr and \n",
        "increase the number of iterations?\n",
        "\n",
        "The below implementation is just a toy implementation. For a better version, see [this word2vec implementation in Pytorch](https://adoni.github.io/2017/11/08/word2vec-pytorch/)"
      ]
    },
    {
      "metadata": {
        "id": "Ncju5K1zow6h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "CONTEXT_SIZE = 2\n",
        "EMBEDDING_DIM = 10\n",
        "# We will use Shakespeare Sonnet 2\n",
        "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
        "And dig deep trenches in thy beauty's field,\n",
        "Thy youth's proud livery so gazed on now,\n",
        "Will be a totter'd weed of small worth held:\n",
        "Then being asked, where all thy beauty lies,\n",
        "Where all the treasure of thy lusty days;\n",
        "To say, within thine own deep sunken eyes,\n",
        "Were an all-eating shame, and thriftless praise.\n",
        "How much more praise deserv'd thy beauty's use,\n",
        "If thou couldst answer 'This fair child of mine\n",
        "Shall sum my count, and make my old excuse,'\n",
        "Proving his beauty by succession thine!\n",
        "This were to be new made when thou art old,\n",
        "And see thy blood warm when thou feel'st it cold.\"\"\".split()\n",
        "# we should tokenize the input, but we will ignore that for now\n",
        "# build a list of tuples.  Each tuple is ([ word_i-2, word_i-1 ], target word)\n",
        "trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n",
        "            for i in range(len(test_sentence) - 2)]\n",
        "# print the first 3, just so you can see what they look like\n",
        "print(trigrams[:3])\n",
        "\n",
        "# deduplicate \n",
        "vocab = set(test_sentence)\n",
        "\n",
        "# generate the word index\n",
        "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
        "\n",
        "\n",
        "class NGramLanguageModeler(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
        "        super(NGramLanguageModeler, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
        "        self.linear2 = nn.Linear(128, vocab_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs).view((1, -1))\n",
        "        out = F.relu(self.linear1(embeds))\n",
        "        out = self.linear2(out)\n",
        "        log_probs = F.log_softmax(out, dim=1)\n",
        "        return log_probs\n",
        "\n",
        "\n",
        "losses = []\n",
        "loss_function = nn.NLLLoss()\n",
        "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
        "\n",
        "# Exercise: What do SGD and lr mean? What happenes if you change them?\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(10):\n",
        "    total_loss = 0\n",
        "    for context, target in trigrams:\n",
        "\n",
        "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
        "        # into integer indices and wrap them in tensors)\n",
        "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
        "\n",
        "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
        "        # new instance, you need to zero out the gradients from the old\n",
        "        # instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 3. Run the forward pass, getting log probabilities over next\n",
        "        # words\n",
        "        log_probs = model(context_idxs)\n",
        "\n",
        "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
        "        # word wrapped in a tensor)\n",
        "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
        "\n",
        "        # Step 5. Do the backward pass and update the gradient\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
        "        total_loss += loss.item()\n",
        "    losses.append(total_loss)\n",
        "print(losses) # The loss decreased every iteration over the training data!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bW3IPEwgLCrO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lesson 2: Use existing embeddings\n",
        "\n",
        "The code below exemplifies how to load a trained embedding model in the gensim library. "
      ]
    },
    {
      "metadata": {
        "id": "aMaN6E36JzNI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Let's first load a small subset of word2vec embeddings that have been trained on a \n",
        "# large corpus of news documents  \n",
        "!wget https://github.com/dgromann/SemanticComputing/raw/master/tutorial6/word2vec_embeddings.bin\n",
        "!wget https://raw.githubusercontent.com/dgromann/SemComp_WS2018/master/Tutorial6/analogy.txt\n",
        "!pip3 install gensim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aZG8KQMILXz-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from sklearn.decomposition import PCA\n",
        "from matplotlib import pyplot\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "69aLE4-7LxlO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Let's load the model\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format(\"word2vec_embeddings.bin\", binary=True)\n",
        "\n",
        "# Print the length fo the whole vocabulary \n",
        "print(len(model.wv.vocab))\n",
        "\n",
        "# Print the embedding of a specific word \n",
        "print(model[\"good\"])\n",
        "\n",
        "# Get the 10 most similar words of \"good\"\n",
        "most_similar = model.most_similar(\"good\", topn=10)\n",
        "\n",
        "# Check whether our embeddings are good at the analogy task\n",
        "analogy = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5bQeIYOpPyVt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Exercise: get the length and datatype of the vector - \n",
        "# what does the length tell us and how would it influence our parameters when training? \n",
        "\n",
        "\n",
        "# Exercise: implement your own version of the analogy check without using a \n",
        "# built in function \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LZx7Tk7hRO8h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Evalute the loaded subset of the embeddings on the analogy.txt file from the Github repository. Each type of analogy is \n",
        "marked by a \": \" at the beginning of the subset. \n",
        "\n",
        "For instance, the analogy of capitals and countries is marked by \": capital-common-countries\" before the data start. Provide an accuracy measure for each type of analogy and compare on which it performs best."
      ]
    },
    {
      "metadata": {
        "id": "mucqVnmcQzrs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Exercise: Evaluate the embeddings on the analogy text file (already loaded for you)\n",
        "# To do so you need to load the file, and test the analogy task on each line apart from \n",
        "# the header lines marked with \": \"\n",
        "# If you want, it would also be interesting to get accuracies for the different types of \n",
        "# analogies that are seprated by a header line marked with \": \"\n",
        "\n",
        "analogy = open(\"analogy.txt\", \"r\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}