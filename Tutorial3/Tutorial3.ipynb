{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dgromann/SemComp_WS2018/blob/master/Tutorial3/Tutorial3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "cS4SXWk62o6B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Lesson 0.0.0: Store this notebook! \n",
        "\n",
        "Go to \"File\" and make sure you store this file as a local copy to either GitHub or your Google Drive. If you do not have a Google account and also do not want to create one, please check Option C below. "
      ]
    },
    {
      "metadata": {
        "id": "0rTqeojW2tHC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Option A) Google Drive WITH collaboration\n",
        "\n",
        "If you want to work in a collaborative manner where each of you in the group can see each other's contributions, one of you needs to store the notebook in Google Drive and share it with the others. You share it by clicking on the SHARE button on the top right of this page and share the link with the \"everyone who receives this link can edit\" option with the other team members per e-mail, skype, or any other way you prefer.\n",
        "\n",
        "If you work with others, keep in mind to always copy the code before you edit it and always indicate your name as a comment (e.g. #Dagmar ) in the cell that it is clear who wrote which part. I also recommend creating a new code cell for your contributions.\n"
      ]
    },
    {
      "metadata": {
        "id": "SmAV-fDY2yit",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Option B) Github without collaboration\n",
        "\n",
        "Collaborative functions are not available when storing the notebook in GitHub; you will see your own work but not that of others.\n"
      ]
    },
    {
      "metadata": {
        "id": "Jsy9d_2B2zQJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Option C) Download this notebook as ipynb (Jupyter notebook) or py (Python file)\n",
        "\n",
        "To run either of these on your local machine requires the installation of the required programs, which for the first tutorial are Python and NLTK. This will become more as we continue on to machine learning (requiring sklearn) and deep learning (requiring tensorflow and/or pytorch). In Google Codelab all of these are provided and do not need to be installed locally.\n"
      ]
    },
    {
      "metadata": {
        "id": "qD56Lp8A23uV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lesson 1: Let's go over a Naive Bayes example"
      ]
    },
    {
      "metadata": {
        "id": "tNLDuv7Lfzzj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Different types of Naive Bayes in sklearn: \n",
        "\n",
        "*   Bernoulli Naive Bayes : for binary classification tasks; assigns 0 (word does not occur in document) or 1 (word does occur in document)\n",
        "* Multinomial Naive Bayes: for discrete variables (e.g. movie ratings or several topics) where we have the count of each word to predict the class or label \n",
        "* Gaussian Naive Bayes: assumes a normal distribution and used for continuous variables, that is our features are continuous (e.g. sepal width, petal width, etc. of a flower) \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "GXrq-2KTbakx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Most imports for Lesson 1, 2 and 3 - some additional ones below\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NQtfT5OIlyHc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The following cell provides all processing steps from preprocessing the text (in this case no special preprocessing is required)\n",
        "to training a Multinomial Naive Bayes classifier. "
      ]
    },
    {
      "metadata": {
        "id": "IKEtkhOpiocm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "categories = ['alt.atheism', 'soc.religion.christian','comp.graphics', 'sci.med']\n",
        "\n",
        "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
        "twenty_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\n",
        "\n",
        "print(\"The categories of this classification dataset are: \", twenty_train.target_names)\n",
        "print(\"The number of documents in this repository is: \", len(twenty_train.filenames), \"which is equivalent to \", len(twenty_train.data))\n",
        "\n",
        "# Tokenizing the text \n",
        "count_vect = CountVectorizer()\n",
        "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
        "print(\"Training data after CountVectorizer: \", X_train_counts.shape)\n",
        "\n",
        "# Convert occurrences to frequencies\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
        "print(\"Training data after TF-IDF: \", X_train_tfidf.shape)\n",
        "\n",
        "# Let's initialize the classifier and fit the model to the training data\n",
        "clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)\n",
        "\n",
        "# Prepare test data\n",
        "X_test_counts = count_vect.transform(twenty_test.data)\n",
        "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
        "print(\"Test data after TF-IDF: \", X_test_tfidf.shape)\n",
        "\n",
        "# Get prediction over test data\n",
        "predicted = clf.predict(X_test_tfidf)\n",
        "print(\"Accuracy is: \", np.mean(predicted == twenty_test.target))\n",
        "print(\"Alternative way to calculate accuracy: \", accuracy_score(twenty_test.target, predicted))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CsZJpt06PFj-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This cell allows you to make predictions on your own sentences using the trained model ."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "8rmqpqPro6Rg",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Exercise: classify your own sentences with the trained model \n",
        "# Study the code above in detail to understand what you need to do \n",
        "# before you can ask the model to predict a label for your sentence(s).\n",
        "# Two example sentences are provided below.\n",
        "docs_new = ['Test this', 'OpenGL on the GPU is fast']\n",
        "\n",
        "predicted = None \n",
        "\n",
        "for doc, category in zip(docs_new, predicted):\n",
        "  print('%r => %s' % (doc, twenty_train.target_names[category]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Dz_LvZX5s5_6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "All of the above processing steps can be simplified to a pipeline that applies all steps to each input."
      ]
    },
    {
      "metadata": {
        "id": "f7lAFxrxtBV1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Exercise: Apply the following pipeline to classifying\n",
        "\n",
        "text_clf = Pipeline([('vect', CountVectorizer()),\n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', MultinomialNB()),])\n",
        "\n",
        "text_clf.fit(twenty_train.data, twenty_train.target) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zxa1Rb2-svYe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lesson 2: Sentiment analysis with Naive Bayes on tweets including  preprocessing"
      ]
    },
    {
      "metadata": {
        "id": "Mxu9-FlEPRI0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This task first asks you to clean the provided tweets. The set is a subset of a 2017 challenge on sentiment analysis \n",
        "at SemEval. After preprocessing, see what kind of accuracy you can get with your algorithm and whether more preprocessing\n",
        "might be useful. You can also first just run the algorithm without preprocessing and then see whether and how it improves when certain items are removed.. "
      ]
    },
    {
      "metadata": {
        "id": "mgXxx_UQtNTv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install wordcloud\n",
        "# Some additional imports for Lesson 2\n",
        "from google.colab import files\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        " \n",
        "import string\n",
        "\n",
        "# load the text file from the github Tutorial 3 folder\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tSYjTFRKPoK2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The wordcloud method below is only for visualizing the content of the tweets in a cloud of words to see which ones are predominant in this dataset. "
      ]
    },
    {
      "metadata": {
        "id": "50DwsltU3BAS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Exercise: clean the tweets, that is, take a look at the tweets and \n",
        "# think of reasonable preprocessing steps for sentiment classification.\n",
        "# After you have preprocessed the tweets, use append() to add the cleaned \n",
        "# data to the variable data[] and append all the sentiment labels to the \n",
        "# variable labels[]\n",
        "\n",
        "# Method for visualizing tweets in a word cloud if you want\n",
        "def wordcloud(data):\n",
        "  stopwords = set(STOPWORDS)\n",
        "  wordcloud = WordCloud(background_color=\"white\",stopwords=stopwords,random_state = 2016).generate(\" \".join([i for i in data]))\n",
        "  plt.figure( figsize=(20,10), facecolor='k')\n",
        "  plt.imshow(wordcloud)\n",
        "  plt.axis(\"off\")\n",
        "  plt.title(\"My Tweet Cloud\")\n",
        "\n",
        "file = open(\"SemEval2017_task4_subset.txt\", \"r\")\n",
        "\n",
        "# For punctuation removal\n",
        "punctuation = list(string.punctuation)\n",
        "\n",
        "data = []\n",
        "labels = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vhFMC70_G1Zr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Exercise: Once you are done with preprocessing and your data and labels \n",
        "# are stored, run an NB classifier on the tweets \n",
        "\n",
        "# This is how the data is split into training and test set \n",
        "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2)\n",
        "\n",
        "\n",
        "# If you want to print some of the predictions to have a look \n",
        "for i in range(0,10):  \n",
        "  print(y_test[i], predicted[i], X_test[i])\n",
        "\n",
        "  \n",
        "# Exercie: How could we obtain the split into training, test, and \n",
        "# development set?\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YItVhpqiIsH1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lesson 3: Cross-validation"
      ]
    },
    {
      "metadata": {
        "id": "ocpVdGBiTtkg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "K-fold cross-validation iteratively tests on different partitions of the data to avoid overfitting."
      ]
    },
    {
      "metadata": {
        "id": "yVLVu30xTcPH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Exercise: Try to get the following k-fold cross validation to run on \n",
        "# our trainng data for practising.\n",
        "# how does this change the accuracy?\n",
        "kf = KFold(n_splits=2) # number of splits you would like to obtain \n",
        "\n",
        "for train_index, test_index in kf.split(X_train):\n",
        "  X_train, X_test = X_train[train_index], X_train[test_index]\n",
        "  y_train, y_test = y_train[train_index], y_train[test_index]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oSoGzPiycE-Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Bonus: Evaluate the effect of hashtags on tweets "
      ]
    },
    {
      "metadata": {
        "id": "9SrcNNe5cKH2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Exercise: collect all the hashtags from tweets with the label of the respective \n",
        "# tweet, that is, hashtags coming from a positive tweet have a positive label\n",
        "# and then get the ten most frequent ones for each category. Which one is \n",
        "# predominant? Which one is the most frequent hashtag across all categories?\n",
        "# Does this observation take effect when training with or without hashtags?\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}