{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial5.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dgromann/SemComp_WS2018/blob/master/Tutorial5/Tutorial5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ysAmgMvHjrp_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Lesson 0.0.0: Store this notebook! \n",
        "\n",
        "Go to \"File\" and make sure you store this file as a local copy to either GitHub or your Google Drive. If you do not have a Google account and also do not want to create one, please check Option C below. "
      ]
    },
    {
      "metadata": {
        "id": "Ddv-6ADkjtQ-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Option A) Google Drive WITH collaboration\n",
        "\n",
        "If you want to work in a collaborative manner where each of you in the group can see each other's contributions, one of you needs to store the notebook in Google Drive and share it with the others. You share it by clicking on the SHARE button on the top right of this page and share the link with the \"everyone who receives this link can edit\" option with the other team members per e-mail, skype, or any other way you prefer.\n",
        "\n",
        "If you work with others, keep in mind to always copy the code before you edit it and always indicate your name as a comment (e.g. #Dagmar ) in the cell that it is clear who wrote which part. I also recommend creating a new code cell for your contributions."
      ]
    },
    {
      "metadata": {
        "id": "MWvHoYoAjwtm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Option B) Github without collaboration\n",
        "\n",
        "Collaborative functions are not available when storing the notebook in GitHub; you will see your own work but not that of others.\n"
      ]
    },
    {
      "metadata": {
        "id": "FK33HnCZjyot",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Option C) Download this notebook as ipynb (Jupyter notebook) or py (Python file)\n",
        "\n",
        "To run either of these on your local machine requires the installation of the required programs, which for the first tutorial are Python and NLTK. This will become more as we continue on to machine learning (requiring sklearn) and deep learning (requiring tensorflow and/or pytorch). In Google Codelab all of these are provided and do not need to be installed locally.\n"
      ]
    },
    {
      "metadata": {
        "id": "3PWXTHKBj0ZJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lesson 1: Data cleaning and analysis \n",
        "\n",
        "Today we will work with the real-world data set of company e- mails called [Enron](http://www.cs.cmu.edu/~enron/). The Enron Corporation, an energy, commodity and service corporation, went bankrupt in 2001 as a result of fraudulent business practices. In total 0.5 million e-mails send by Enron executives between 2000 and 2002 were recorded and published along with financial information. The goal of processing this dataset with machine learning algorithms is to detect emails of interest which might be involved in fraudulent activities."
      ]
    },
    {
      "metadata": {
        "id": "YWcIM5E-O5TR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We do not have any labels for this dataset, this is, we do not know which messages are of interest and which are not. Thus, we need to cluster the messages. First, however, we need to upload the data and structure them in a format to enable clustering. To this end, we will upload the \"enron.txt\" file that contains 100 sample messages from the dataset. We need to separate the header and the body of the e-mails and for now, only store the Message-ID, the sender of each e-mail (\"From: \"), the receiver (\"To: \"), and the body.  Complete the provided function to this end.."
      ]
    },
    {
      "metadata": {
        "id": "vvr3YTMskDwe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.cluster.hierarchy import ward\n",
        "from random import randint\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DDJchdwUPrBr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercise 1.1: \n",
        "Use the method below to complete the processing and cleaning of the data. Some **important additional hints**: \n",
        "\n",
        "*   messages are separated by lines with this string: `\"--------Separator------------\" ` \n",
        "*   the `header_re` variable provides a pattern that identifies header lines \n",
        "* filter the body by using `re.sub(r'[^a-zA-Z]', ' ', line)` to make sure your cleaned body dictionary only contains interesting  and useful information\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "uniO5O_skL24",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Some Regular Expresssions for processing e-mails\n",
        "# Finding different types of linebreaks\n",
        "linebreak = [\"\\n\", \"\\r\", \"\\r\\n\"]  # unix, mac, and DOS newlines\n",
        "\n",
        "# Finding header lines\n",
        "header_re = re.compile('([^:]+):\\s(.*)')\n",
        "\n",
        "\n",
        "#One possible version of how to parse e-mails\n",
        "def parse_email(input_file):\n",
        "    from_dict = {}\n",
        "    to_dict = {}\n",
        "    emails_body = {}\n",
        "       \n",
        "    # Exercise: Get the \"From\", \"To\", and body information from each message\n",
        "    # you can use the regular expression above to find out which lines are \n",
        "    # part of the header and which ones are not \n",
        "    # Each message is separated from the others by containing \n",
        "    # \"--------Separator------------\" in the line \n",
        "\n",
        "    return emails_body, from_dict, to_dict\n",
        "  \n",
        "enron = open(\"enron.txt\", \"r\")\n",
        "body, from_dict, to_dict = parse_email(enron)\n",
        "print(\"Length of from_dict: \", len(from_dict), \"Length of to_dict: \", len(to_dict), \"Length of body_dict: \", len(body))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eZWVlgyfRvSV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Principal Component Analysis for Data Visualization\n",
        "\n",
        "It is important to understand the data. A good starting point after having looked at the actual messages and after data cleaning, is to visualize the data. We cannot easily visualize the many features (words, header information, etc.) of each messages. We need to reduce the dimension to two in order to be able to visualize them easily. \n",
        "\n",
        "**Principal Components Analysis (PCA) ** is a dimensionality reduction algorithm that can be used to significantly speed up your unsupervised feature learning algorithm. It finds a sequence of linear combination of the variables called the principal components that explain the maximum variance and summarize the most information in the data and are mutually uncorrelated.\n",
        "\n",
        "The goal of PCA is to find the most relevant set of variables and linearly combine this set to a single variable called the principal component.\n",
        "\n",
        "\n",
        "1.   The first principal component has the highest variance across data\n",
        "2.   The second principal component is uncorrelated with the first and also has high variance \n",
        "\n",
        "PCA will find a lower-dimensional subspace onto which to project our data. Let's look at an example dataset (Source: http://ufldl.stanford.edu/wiki/index.php/PCA):\n",
        "\n",
        "![alt text](http://ufldl.stanford.edu/wiki/images/thumb/b/ba/PCA-rawdata.png/800px-PCA-rawdata.png)\n",
        "\n",
        "From this visualization it appears that the data have two main directions of variation: u1 being the primary direction of variation  and u2 being the secondary:\n",
        "\n",
        "![alt text](http://ufldl.stanford.edu/wiki/images/thumb/b/b4/PCA-u1.png/800px-PCA-u1.png)\n",
        "\n",
        "In terms of calculation, we need to generate the following matrix in order to find the directions u1 and u2:\n",
        "\n",
        "![alt text](http://ufldl.stanford.edu/wiki/images/math/d/a/9/da9b50ec05dbe4ae513e4f52093b8342.png)\n",
        "\n",
        "The principal direction of variation of the data turns out to be the principal eigenvector of this matrix \"Sigma\" and the second top direction of variation is the second eigenvector of this matrix. Depending on the number of dimensions you wish to reduce to, you can use the corresponding numbers of eigenvectors from this matrix to transform your data. Generally the eigenvectors of Sigma are represented as a matrix on their one called U. \n",
        "\n",
        "If you wish to reduce to only one dimension, your reduction would consist in:\n",
        "![alt text](http://ufldl.stanford.edu/wiki/images/math/c/d/0/cd047246fd68f6d52b2fd068e063c0ef.png)\n"
      ]
    },
    {
      "metadata": {
        "id": "XQ32zNmaRf0X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_clustering_results(X, clf, n_clusters):\n",
        "    # Let's plot this with matplotlib to visualize it.\n",
        "    # First we need to make 2D coordinates from the sparse matrix.\n",
        "    X_dense = X.todense()\n",
        "    pca = PCA(n_components=2).fit(X_dense)\n",
        "    coords = pca.transform(X_dense)\n",
        "\n",
        "    # Lets plot it again, but this time we add some color to it.\n",
        "    # This array needs to be at least the length of the n_clusters.\n",
        "    label_colors = []\n",
        "    for i in range(n_clusters):\n",
        "        label_colors.append('#%06X' % randint(0, 0xFFFFFF))\n",
        "    colors = [label_colors[i] for i in labels]\n",
        "\n",
        "    plt.scatter(coords[:, 0], coords[:, 1], c=colors)\n",
        "    # Plot the cluster centers\n",
        "    centroids = clf.cluster_centers_\n",
        "    centroid_coords = pca.transform(centroids)\n",
        "    plt.scatter(centroid_coords[:, 0], centroid_coords[:, 1], marker='X', s=200, linewidths=2, c='#444d60')\n",
        "    plt.title(\"Visualization of k-means clustering results \")\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cCpw_ziHZBmS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercise 1.2: Applied PCA for Visualization\n",
        "\n",
        "Use the provided \"plot_clustering_results\" in order to obtain a first visualization of the enron dataset. To to this, we first need to turn our words into features and then run PCA on them."
      ]
    },
    {
      "metadata": {
        "id": "uqs7-wQuATrg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Exercise: Convert body to matrix of TF-IDF features and remove stop words\n",
        "# Using a parameter of the TF-IDF Vectorizer\n",
        "\n",
        "# Exercise: change the following variable \"X\" to the name of your matrix and plot the results\n",
        "X_dense = X.todense()\n",
        "coords = PCA(n_components=2).fit_transform(X_dense)\n",
        "plt.title(\"Visualization data using PCA\")\n",
        "plt.scatter(coords[:, 0], coords[:, 1], c='m')\n",
        "plt.show()\n",
        "\n",
        "# Exercise: use your TF-IDF feature matrix to get the 20 top keywords of all e-mails (function \"name of your TfidfVectorizer\".get_feature_names() and then\n",
        "#argsort only supports small to large sorting; to change this order we use -1 in the following line\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IIAJmWVgZ4Po",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lesson 2: Clustering\n",
        "\n",
        "This part will practically apply the k-means cluster to the above enron dataset."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ZauQ0Fisi1XA",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Exercise : Perform clustering on the data using k-means\n",
        "n_clusters = None\n",
        "\n",
        "# Exercise: Plot the resulting clusters using the function plot_clustering_results (see above)\n",
        "plot_clustering_results(X, kmeans, n_clusters)\n",
        "\n",
        "# Exercise: get the top TF-IDF features for each cluster\n",
        "print(\"Top terms per cluster:\")\n",
        "order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}